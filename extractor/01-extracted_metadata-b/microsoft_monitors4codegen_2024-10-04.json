{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 00:28:27"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/microsoft/monitors4codegen",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "microsoft",
                "type": "Organization"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2023-11-04T21:49:04Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-10-01T05:58:31Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "license": [
        {
            "result": {
                "value": "https://api.github.com/licenses/mit",
                "type": "License",
                "name": "MIT License",
                "url": "https://api.github.com/licenses/mit",
                "spdx_id": "MIT"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "    MIT License\n\n    Copyright (c) Microsoft Corporation.\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE\n",
                "type": "File_dump"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/LICENSE"
        }
    ],
    "description": [
        {
            "result": {
                "value": "Code and Data artifact for NeurIPS 2023 paper - \"Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context\". `multispy` is a lsp client library in Python intended to be used to build applications around language servers.",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "This repository hosts the official code and data artifact for the paper [\"Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context\"](https://neurips.cc/virtual/2023/poster/70362) appearing at NeurIPS 2023 ([\"Guiding Language Models of Code with Global Context using Monitors\"](https://arxiv.org/abs/2306.10763) on Arxiv). The work introduces Monitor-Guided Decoding (MGD) for code generation using Language Models, where a monitor uses static analysis to guide the decoding.\n",
                "type": "Text_excerpt",
                "original_header": "Introduction",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "value": "Description of expected columns in the inference results csv input to the evaluation script:\n* `repo`: Name of the repository from which the testcase was sourced\n* `classFileName`: relative path to file containing the testcase prompt location\n* `methodStartIdx`: String index of starting `'{'` of the method\n* `methodStopIdx`: String index of closing `'}'` of the method\n* `dot_idx`: String index of `'.'` that is the dereference prompt point\n* `configuration`: Identifies the configuration used to generate the given code sample. Values from: `['SC-classExprTypes', 'CG-6B', 'SC-FIM-classExprTypes', 'SC-RLPG-MGD', 'SC-MGD', 'SC-FIM-classExprTypes-MGD', 'CG-2B', 'SC', 'CG-2B-MGD', 'CG-350M-classExprTypes-MGD', 'SC-FIM', 'TD-3', 'CG-350M-MGD', 'SC-FIM-MGD', 'SC-RLPG', 'CG-350M', 'CG-350M-classExprTypes', 'SC-classExprTypes-MGD', 'CG-6B-MGD', 'TD-3-MGD']`\n* `temperature`: Temperature used for sampling. Values from: `[0.8, 0.6, 0.4, 0.2]`\n* `model`: Name of the model used for sampling. Values from: `['Salesforce/codegen-6B-multi', 'bigcode/santacoder', 'Salesforce/codegen-2B-multi', 'Salesforce/codegen-350M-multi', 'text-davinci-003']`\n* `context`: Decoding strategy used. Values from: `['autoregressive', 'fim']`\n* `prefix`: Prompt strategy used. Values from: `['classExprTypes', 'none', 'rlpg']`\n* `rlpg_best_rule_name`: Name of the rule used for creating RLPG prompt (if used for the corresponding testcase). Values from: `[nan, 'in_file#lines#0.25', 'in_file#lines#0.5', 'in_file#lines#0.75', 'import_file#method_names#0.5']`\n* `output`: Generated output by the model\n* `compilationSucceeded`: Result of compiling the generated method in the context of the full repository. 1 if success, 0 otherwise. Values from: `[1, 0]`\n",
                "type": "Text_excerpt",
                "original_header": "Description of `inference results csv` file format",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context",
                    "2. Evaluation Scripts"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "1. [Datasets](#1-datasets): PragmaticCode and DotPrompts\n2. [Evaluation scripts](#2-evaluation-scripts): Scripts to evaluate LMs by taking as input inferences (code generated by the model) for examples in DotPrompts and producing score@k scores for the metrics reported in the paper: Compilation Rate (CR), Next-Identifier Match (NIM), Identifier-Sequence Match (ISM) and Prefix Match (PM).\n3. [Inference Results over DotPrompts](#3-inference-results-over-dotprompts): Generated code for examples in DotPrompts with various model configurations reported in the paper. The graphs and tables reported in the paper can be reproduced by running the evaluation scripts on the provided inference results.\n4. [`multilspy`](#4-multilspy): A cross-platform library designed to simplify the process of creating language server clients to query and obtain results of various static analyses from a wide variety of language servers that communicate over the [Language Server Protocol](https://microsoft.github.io/language-server-protocol/). `multilspy` is intended to be used as a library to easily query various language servers, without having to worry about setting up their configurations and implementing the client-side of language server protocol. `multilspy` currently supports running language servers for Java, Rust, C# and Python, and we aim to expand this list with the help of the community.\n5. [Monitor-Guided Decoding](#5-monitor-guided-decoding): Implementation of various monitors monitoring for different properties reported in the paper (for example: monitoring for type-valid identifier dereferences, monitoring for correct number of arguments to method calls, monitoring for typestate validity of method call sequences, etc.), spanning 3 programming languages. \n",
                "original_header": "Repository Contents"
            },
            "confidence": 0.9954759787566144,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "The full dataset, along with repository zip files is available in our Zenodo dataset release at [https://zenodo.org/records/10072088](https://zenodo.org/records/10072088). The list of repositories along with their respective licenses consisting PragmaticCode is available in [datasets/PragmaticCode/repos.csv](datasets/PragmaticCode/repos.csv). The contents of the files required for inference for each of the repositories is available in [datasets/PragmaticCode/fileContentsByRepo.json](datasets/PragmaticCode/fileContentsByRepo.json).\n \n",
                "original_header": "PragmaticCode"
            },
            "confidence": 0.9561921213917982,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "DotPrompts is a set of examples derived from PragmaticCode, such that each example consists of a prompt to a dereference location (a code location having the \".\" operator in Java). DotPrompts can be used to benchmark Language Models of Code on their ability to utilize repository level context to generate code for method-level completion tasks. The task for the models is to complete a partially written Java method, utilizing the full repository available from PragmaticCode. Since all the repositories in PragmaticCode are buildable, DotPrompts (derived from PragmaticCode) supports Compilation Rate as a metric of evaluation for generated code, apart from standard metrics of ground truth match like Next-Identifier Match, Identifier Sequence Match and Prefix Match.  \n",
                "original_header": "DotPrompts"
            },
            "confidence": 0.9956312462218537,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "We provide all inferences (generated code) generated by all model configurations reported in the paper, for every example in DotPrompts. This consists of 6 independently sampled inferences for 18 different model configurations (spanning parameter scale, prompt templates, use of FIM context, etc.) for every example in DotPrompts. \n",
                "original_header": "3. Inference Results over DotPrompts"
            },
            "confidence": 0.9503767676199298,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "`multilspy` is a cross-platform library that we have built to set up and interact with various language servers in a unified and easy way. [Language servers]((https://microsoft.github.io/language-server-protocol/overviews/lsp/overview/)) are tools that perform a variety of static analyses on source code and provide useful information such as type-directed code completion suggestions, symbol definition locations, symbol references, etc., over the [Language Server Protocol (LSP)](https://microsoft.github.io/language-server-protocol/overviews/lsp/overview/). `multilspy` intends to ease the process of using language servers, by abstracting the setting up of the language servers, performing language-specific configuration and handling communication with the server over the json-rpc based protocol, while exposing a simple interface to the user. \nSince LSP is language-agnostic, `multilspy` can provide the results for static analyses of code in different languages over a common interface. `multilspy` is easily extensible to any language that has a Language Server and currently supports Java, Rust, C# and Python and we aim to support more language servers from the [list of language server implementations](https://microsoft.github.io/language-server-protocol/implementors/servers/). \n",
                "original_header": "4. `multilspy`"
            },
            "confidence": 0.984114265568666,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "[src/monitors4codegen/monitor_guided_decoding/openai_gen.py](src/monitors4codegen/monitor_guided_decoding/openai_gen.py) provides the method `openai_mgd` which takes the prompt and a `Monitor` as input, and returns the MGD guided generation using an OpenAI model.\n \n",
                "original_header": "MGD with OpenAI models"
            },
            "confidence": 0.9193868621899434,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n \n",
                "original_header": "Trademarks"
            },
            "confidence": 0.9725227067402532,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ],
    "name": [
        {
            "result": {
                "value": "monitors4codegen",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "microsoft/monitors4codegen",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/microsoft/monitors4codegen/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/microsoft/monitors4codegen/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 193,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "ai, ai4code, artificial-intelligence, code-analysis, code-completion, code-generation, codegen, dataset, huggingface-transformers, language-server-client, language-server-protocol, large-language-models, llm, lsp, lsp-client, neurips, neurips-2023, program-synthesis, transformer",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 25,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/microsoft/monitors4codegen/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 562484
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "code_of_conduct": [
        {
            "result": {
                "value": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n",
                "type": "File_dump"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/CODE_OF_CONDUCT.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "# This CITATION.cff file was generated with cffinit.\n# Visit https://bit.ly/cffinit to generate yours today!\n\ncff-version: 1.2.0\ntitle: >-\n  Monitor-Guided Decoding of Code LMs with Static Analysis\n  of Repository Context\nmessage: >-\n  If you use this repository, please cite it using the metadata\n  from this file.\ntype: dataset\nauthors:\n  - given-names: Lakshya A\n    family-names: Agrawal\n    email: t-lakagrawal@microsoft.com\n    affiliation: Microsoft Research\n    orcid: 'https://orcid.org/0000-0003-0409-8212'\n  - given-names: Aditya\n    family-names: Kanade\n    email: kanadeaditya@microsoft.com\n    affiliation: Microsoft Research\n  - given-names: Navin\n    family-names: Goyal\n    email: navingo@microsoft.com\n    affiliation: Microsoft Research\n  - given-names: Shuvendu K.\n    family-names: Lahiri\n    email: shuvendu.lahiri@microsoft.com\n    affiliation: Microsoft Research\n  - given-names: Sriram K.\n    family-names: Rajamani\n    email: sriram@microsoft.com\n    affiliation: Microsoft Research\nidentifiers:\n  - type: doi\n    value: 10.48550/arXiv.2306.10763\n  - type: url\n    value: >-\n      https://openreview.net/forum?id=qPUbKxKvXq&noteId=98Ukj82fSP\nabstract: >-\n  Language models of code (LMs) work well when the\n  surrounding code provides sufficient context. This is not\n  true when it becomes necessary to use types, functionality\n  or APIs defined elsewhere in the repository or a linked\n  library, especially those not seen during training. LMs\n  suffer from limited awareness of such global context and\n  end up hallucinating.\n\n\n  Integrated development environments (IDEs) assist\n  developers in understanding repository context using\n  static analysis. We extend this assistance, enjoyed by\n  developers, to LMs. We propose monitor-guided decoding\n  (MGD) where a monitor uses static analysis to guide the\n  decoding. We construct a repository-level dataset\n  PragmaticCode for method-completion in Java and evaluate\n  MGD on it. On models of varying parameter scale, by\n  monitoring for type-consistent object dereferences, MGD\n  consistently improves compilation rates and agreement with\n  ground truth. Further, LMs with fewer parameters, when\n  augmented with MGD, can outperform larger LMs. With MGD,\n  SantaCoder-1.1B achieves better compilation rate and\n  next-identifier match than the much larger\n  text-davinci-003 model.\n\n\n  We also conduct a generalizability study to evaluate the\n  ability of MGD to generalize to multiple programming\n  languages (Java, C# and Rust), coding scenarios (e.g.,\n  correct number of arguments to method calls), and to\n  enforce richer semantic constraints (e.g., stateful API\n  protocols). Our data and implementation are available at\n  https://github.com/microsoft/monitors4codegen.\nkeywords:\n  - program analysis\n  - correctness\n  - code generation\n  - Language models\n",
                "type": "File_dump",
                "format": "cff"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/CITATION.cff"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "usage": [
        {
            "result": {
                "value": "For example, consider the partial code to be completed in the figure below. To complete this code, an LM has to generate identifiers consistent with the type of the object returned by `ServerNode.Builder.newServerNode()`. The method `newServerNode` and its return type, class `ServerNode.Builder`, are defined in another file. If an LM does not have information about the `ServerNode.Builder type`, it ends up hallucinating, as can be seen in the example generations with the text-davinci-003 and SantaCoder models. The completion uses identifiers `host` and `port`, which do not exist in the type `ServerNode.Builder`. The generated code therefore results in \u201csymbol not found\u201d compilation errors. \n\nMGD uses static analysis to guide the decoding of LMs, to generate code following certain properties. In the example, MGD is used to monitor for generating code with type-correct dereferences, and the SantaCoder model with the same prompt is able to generate the correct code completion, which compiles and matches the ground truth as well.\n\nAs reported in the paper, we observe that **MGD can improve the compilation rate of code generated by LMs at all scales (350M-175B) by 19-25%**, without any training/fine-tuning required. Further, it boosts the ground-truth match at all granularities from token-level to method-level code completion.\n\n![](figures/motivating_example.png)\n",
                "type": "Text_excerpt",
                "original_header": "Monitor-Guided Decoding: Motivating Example",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "value": "Example usage:\n```python\nfrom multilspy import SyncLanguageServer\nfrom multilspy.multilspy_config import MultilspyConfig\nfrom multilspy.multilspy_logger import MultilspyLogger\n...\nconfig = MultilspyConfig.from_dict({\"code_language\": \"java\"}) # Also supports \"python\", \"rust\", \"csharp\"\nlogger = MultilspyLogger()\nlsp = SyncLanguageServer.create(config, logger, \"/abs/path/to/project/root/\")\nwith lsp.start_server():\n    result = lsp.request_definition(\n        \"relative/path/to/code_file.java\", # Filename of location where request is being made\n        163, # line number of symbol for which request is being made\n        4 # column number of symbol for which request is being made\n    )\n    result2 = lsp.request_completions(\n        ...\n    )\n    result3 = lsp.request_references(\n        ...\n    )\n    result4 = lsp.request_document_symbols(\n        ...\n    )\n    result5 = lsp.request_hover(\n        ...\n    )\n    ...\n```\n\n`multilspy` also provides an asyncio based API which can be used in async contexts. Example usage (asyncio):\n```python\nfrom multilspy import LanguageServer\n...\nlsp = LanguageServer.create(...)\nasync with lsp.start_server():\n    result = await lsp.request_definition(\n        ...\n    )\n    ...\n```\n\nThe file [microsoft/multilspy - src/multilspy/language_server.py](https://github.com/microsoft/multilspy/blob/main/src/multilspy/language_server.py) provides the `multilspy` API. Several tests for `multilspy` present under [microsoft/multilspy - tests/multilspy/](https://github.com/microsoft/multilspy/tree/main/tests/multilspy) provide detailed usage examples for `multilspy`. The tests can be executed by running:\n```bash\npytest tests/multilspy\n```\n",
                "type": "Text_excerpt",
                "original_header": "Usage",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context",
                    "4. `multilspy`"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "We use the Python packages listed in [requirements.txt](requirements.txt). Our experiments used python 3.10. It is recommended to install the same with dependencies in an isolated virtual environment. To create a virtual environment using `venv`:\n```setup\npython3 -m venv venv_monitors4codegen\nsource venv_monitors4codegen/bin/activate\n```\nor using conda:\n```\nconda create -n monitors4codegen python=3.10\nconda activate monitors4codegen\n```\nFurther details and instructions on creation of python virtual environments can be found in the [official documentation](https://docs.python.org/3/library/venv.html). Further, we also refer users to [Miniconda](https://docs.conda.io/en/latest/miniconda.html), as an alternative to the above steps for creation of the virtual environment.\n\nTo install the requirements for running evaluations as described [below](#2-evaluation-scripts):\n\n```setup\npip3 install -r requirements.txt\n```\n",
                "type": "Text_excerpt",
                "original_header": "Environment Setup",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context",
                    "2. Evaluation Scripts"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "value": "To install `multilspy` using pip, execute the following command:\n```\npip install https://github.com/microsoft/multilspy/archive/main.zip\n```\n",
                "type": "Text_excerpt",
                "original_header": "Installation",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context",
                    "4. `multilspy`"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "The generated samples along with their compilation status, following the format described [above](#description-of-inference-results-csv-file-format), is available at [inference_results/dotprompts_results.csv](inference_results/dotprompts_results.csv). The file is stored using [git lfs](https://git-lfs.com/). If the file is not available locally after cloning this repository, please check the [git lfs website](https://git-lfs.com/) for instructions on setup, and clone the repository again after git lfs setup. \n",
                "original_header": "3. Inference Results over DotPrompts"
            },
            "confidence": 0.9999856822685839,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n \n",
                "original_header": "Trademarks"
            },
            "confidence": 0.9988852287322683,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ],
    "run": [
        {
            "result": {
                "value": "The evaluation script can be run as follows:\n```\npython3 eval_results.py <path to inference results - csv> <path to PragmaticCode filecontents - json> <path to output directory>\n```\n\nThe above command will create a directory `<path to output directory>`, containing all the graphs and tables reported in the paper along with extra details. The command also generates a report in the output directory, named `Report.md` which relates the generated figures to sections in the paper. \n\nTo ensure that the environment setup has been done correctly, please run the below command, which runs the evaluation script over dummy data (included in [inference_results/dotprompts_results_sample.csv](inference_results/dotprompts_results_sample.csv)). If the command fails, that indicates an error in the environment setup and the authors request you to kindly report the same.\n```\npython3 evaluation_scripts/eval_results.py inference_results/dotprompts_results_sample.csv datasets/PragmaticCode/fileContentsByRepo.json results_sample/\n```\n",
                "type": "Text_excerpt",
                "original_header": "Running the evaluation script",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context",
                    "2. Evaluation Scripts"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        },
        {
            "result": {
                "value": "# Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context\nAlternative title: Guiding Language Models of Code with Global Context using Monitors\n\n## Introduction\nThis repository hosts the official code and data artifact for the paper [\"Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context\"](https://neurips.cc/virtual/2023/poster/70362) appearing at NeurIPS 2023 ([\"Guiding Language Models of Code with Global Context using Monitors\"](https://arxiv.org/abs/2306.10763) on Arxiv). The work introduces Monitor-Guided Decoding (MGD) for code generation using Language Models, where a monitor uses static analysis to guide the decoding.\n\n## Repository Contents\n1. [Datasets](#1-datasets): PragmaticCode and DotPrompts\n2. [Evaluation scripts](#2-evaluation-scripts): Scripts to evaluate LMs by taking as input inferences (code generated by the model) for examples in DotPrompts and producing score@k scores for the metrics reported in the paper: Compilation Rate (CR), Next-Identifier Match (NIM), Identifier-Sequence Match (ISM) and Prefix Match (PM).\n3. [Inference Results over DotPrompts](#3-inference-results-over-dotprompts): Generated code for examples in DotPrompts with various model configurations reported in the paper. The graphs and tables reported in the paper can be reproduced by running the evaluation scripts on the provided inference results.\n4. [`multilspy`](#4-multilspy): A cross-platform library designed to simplify the process of creating language server clients to query and obtain results of various static analyses from a wide variety of language servers that communicate over the [Language Server Protocol](https://microsoft.github.io/language-server-protocol/). `multilspy` is intended to be used as a library to easily query various language servers, without having to worry about setting up their configurations and implementing the client-side of language server protocol. `multilspy` currently supports running language servers for Java, Rust, C# and Python, and we aim to expand this list with the help of the community.\n5. [Monitor-Guided Decoding](#5-monitor-guided-decoding): Implementation of various monitors monitoring for different properties reported in the paper (for example: monitoring for type-valid identifier dereferences, monitoring for correct number of arguments to method calls, monitoring for typestate validity of method call sequences, etc.), spanning 3 programming languages.\n\n**The `multilspy` library has now been migrated to [microsoft/multilspy](https://github.com/microsoft/multilspy).**\n\n## Monitor-Guided Decoding: Motivating Example\nFor example, consider the partial code to be completed in the figure below. To complete this code, an LM has to generate identifiers consistent with the type of the object returned by `ServerNode.Builder.newServerNode()`. The method `newServerNode` and its return type, class `ServerNode.Builder`, are defined in another file. If an LM does not have information about the `ServerNode.Builder type`, it ends up hallucinating, as can be seen in the example generations with the text-davinci-003 and SantaCoder models. The completion uses identifiers `host` and `port`, which do not exist in the type `ServerNode.Builder`. The generated code therefore results in \u201csymbol not found\u201d compilation errors. \n\nMGD uses static analysis to guide the decoding of LMs, to generate code following certain properties. In the example, MGD is used to monitor for generating code with type-correct dereferences, and the SantaCoder model with the same prompt is able to generate the correct code completion, which compiles and matches the ground truth as well.\n\nAs reported in the paper, we observe that **MGD can improve the compilation rate of code generated by LMs at all scales (350M-175B) by 19-25%**, without any training/fine-tuning required. Further, it boosts the ground-truth match at all granularities from token-level to method-level code completion.\n\n![](figures/motivating_example.png)\n\n## 1. Datasets\n\n### Dataset Statistics\n|||\n|--------------|:-----:|\n| Number of repositories in PragmaticCode |  100 |\n| Number of methods in DotPrompts | 1420 |\n| Number of examples in DotPrompts | 10538 |\n\n### PragmaticCode\nPragmaticCode is a dataset of real-world open-source Java projects complete with their development environments and dependencies (through their respective build systems). The authors tried to ensure that all the repositories in PragmaticCode were released publicly only after the determined training dataset cutoff date (31 March 2022) for the CodeGen, SantaCoder and text-davinci-003 family of models, which were used to evaluate MGD.\n\nThe full dataset, along with repository zip files is available in our Zenodo dataset release at [https://zenodo.org/records/10072088](https://zenodo.org/records/10072088). The list of repositories along with their respective licenses consisting PragmaticCode is available in [datasets/PragmaticCode/repos.csv](datasets/PragmaticCode/repos.csv). The contents of the files required for inference for each of the repositories is available in [datasets/PragmaticCode/fileContentsByRepo.json](datasets/PragmaticCode/fileContentsByRepo.json).\n\n### DotPrompts\nDotPrompts is a set of examples derived from PragmaticCode, such that each example consists of a prompt to a dereference location (a code location having the \".\" operator in Java). DotPrompts can be used to benchmark Language Models of Code on their ability to utilize repository level context to generate code for method-level completion tasks. The task for the models is to complete a partially written Java method, utilizing the full repository available from PragmaticCode. Since all the repositories in PragmaticCode are buildable, DotPrompts (derived from PragmaticCode) supports Compilation Rate as a metric of evaluation for generated code, apart from standard metrics of ground truth match like Next-Identifier Match, Identifier Sequence Match and Prefix Match. \n\nThe scenario described in [motivating example above](#monitor-guided-decoding-motivating-example) is an example in DotPrompts.\n\nThe complete description of an example in DotPrompts is a tuple - `(repo, classFileName, methodStartIdx, methodStopIdx, dot_idx)`. The dataset is available at [datasets/DotPrompts/dataset.csv](datasets/DotPrompts/dataset.csv).\n\n## 2. Evaluation Scripts\n### Environment Setup\nWe use the Python packages listed in [requirements.txt](requirements.txt). Our experiments used python 3.10. It is recommended to install the same with dependencies in an isolated virtual environment. To create a virtual environment using `venv`:\n```setup\npython3 -m venv venv_monitors4codegen\nsource venv_monitors4codegen/bin/activate\n```\nor using conda:\n```\nconda create -n monitors4codegen python=3.10\nconda activate monitors4codegen\n```\nFurther details and instructions on creation of python virtual environments can be found in the [official documentation](https://docs.python.org/3/library/venv.html). Further, we also refer users to [Miniconda](https://docs.conda.io/en/latest/miniconda.html), as an alternative to the above steps for creation of the virtual environment.\n\nTo install the requirements for running evaluations as described [below](#2-evaluation-scripts):\n\n```setup\npip3 install -r requirements.txt\n```\n\n### Running the evaluation script\nThe evaluation script can be run as follows:\n```\npython3 eval_results.py <path to inference results - csv> <path to PragmaticCode filecontents - json> <path to output directory>\n```\n\nThe above command will create a directory `<path to output directory>`, containing all the graphs and tables reported in the paper along with extra details. The command also generates a report in the output directory, named `Report.md` which relates the generated figures to sections in the paper. \n\nTo ensure that the environment setup has been done correctly, please run the below command, which runs the evaluation script over dummy data (included in [inference_results/dotprompts_results_sample.csv](inference_results/dotprompts_results_sample.csv)). If the command fails, that indicates an error in the environment setup and the authors request you to kindly report the same.\n```\npython3 evaluation_scripts/eval_results.py inference_results/dotprompts_results_sample.csv datasets/PragmaticCode/fileContentsByRepo.json results_sample/\n```\n\n### Description of `inference results csv` file format\nDescription of expected columns in the inference results csv input to the evaluation script:\n* `repo`: Name of the repository from which the testcase was sourced\n* `classFileName`: relative path to file containing the testcase prompt location\n* `methodStartIdx`: String index of starting `'{'` of the method\n* `methodStopIdx`: String index of closing `'}'` of the method\n* `dot_idx`: String index of `'.'` that is the dereference prompt point\n* `configuration`: Identifies the configuration used to generate the given code sample. Values from: `['SC-classExprTypes', 'CG-6B', 'SC-FIM-classExprTypes', 'SC-RLPG-MGD', 'SC-MGD', 'SC-FIM-classExprTypes-MGD', 'CG-2B', 'SC', 'CG-2B-MGD', 'CG-350M-classExprTypes-MGD', 'SC-FIM', 'TD-3', 'CG-350M-MGD', 'SC-FIM-MGD', 'SC-RLPG', 'CG-350M', 'CG-350M-classExprTypes', 'SC-classExprTypes-MGD', 'CG-6B-MGD', 'TD-3-MGD']`\n* `temperature`: Temperature used for sampling. Values from: `[0.8, 0.6, 0.4, 0.2]`\n* `model`: Name of the model used for sampling. Values from: `['Salesforce/codegen-6B-multi', 'bigcode/santacoder', 'Salesforce/codegen-2B-multi', 'Salesforce/codegen-350M-multi', 'text-davinci-003']`\n* `context`: Decoding strategy used. Values from: `['autoregressive', 'fim']`\n* `prefix`: Prompt strategy used. Values from: `['classExprTypes', 'none', 'rlpg']`\n* `rlpg_best_rule_name`: Name of the rule used for creating RLPG prompt (if used for the corresponding testcase). Values from: `[nan, 'in_file#lines#0.25', 'in_file#lines#0.5', 'in_file#lines#0.75', 'import_file#method_names#0.5']`\n* `output`: Generated output by the model\n* `compilationSucceeded`: Result of compiling the generated method in the context of the full repository. 1 if success, 0 otherwise. Values from: `[1, 0]`\n\n## 3. Inference Results over DotPrompts\nWe provide all inferences (generated code) generated by all model configurations reported in the paper, for every example in DotPrompts. This consists of 6 independently sampled inferences for 18 different model configurations (spanning parameter scale, prompt templates, use of FIM context, etc.) for every example in DotPrompts.\n\nThe generated samples along with their compilation status, following the format described [above](#description-of-inference-results-csv-file-format), is available at [inference_results/dotprompts_results.csv](inference_results/dotprompts_results.csv). The file is stored using [git lfs](https://git-lfs.com/). If the file is not available locally after cloning this repository, please check the [git lfs website](https://git-lfs.com/) for instructions on setup, and clone the repository again after git lfs setup.\n\nEach row in the file contains several multi-line string cells, and therefore, while viewing them in tools like Microsoft Office Excel, kindly enable \"Word Wrap\" to be able to view the full contents.\n\nTo run the [evaluation scripts](#2-evaluation-scripts) over the inferences, in order to reproduce the graphs and tables reported in the paper, run:\n```\npython3 evaluation_scripts/eval_results.py inference_results/dotprompts_results.csv datasets/PragmaticCode/fileContentsByRepo.json results/\n```\n\nThe above command creates a directory [results](results/) (already included in the repository), containing all the figures and tables provided in the paper along with extra details. The command also generates a report in the output directory which relates the generated figures to sections in the paper. In case of above command, the report is generated at [results/Report.md](results/Report.md).\n\n## 4. `multilspy`\n**The `multilspy` library has now been migrated to [microsoft/multilspy](https://github.com/microsoft/multilspy).**\n\n`multilspy` is a cross-platform library that we have built to set up and interact with various language servers in a unified and easy way. [Language servers]((https://microsoft.github.io/language-server-protocol/overviews/lsp/overview/)) are tools that perform a variety of static analyses on source code and provide useful information such as type-directed code completion suggestions, symbol definition locations, symbol references, etc., over the [Language Server Protocol (LSP)](https://microsoft.github.io/language-server-protocol/overviews/lsp/overview/). `multilspy` intends to ease the process of using language servers, by abstracting the setting up of the language servers, performing language-specific configuration and handling communication with the server over the json-rpc based protocol, while exposing a simple interface to the user.\n\nSince LSP is language-agnostic, `multilspy` can provide the results for static analyses of code in different languages over a common interface. `multilspy` is easily extensible to any language that has a Language Server and currently supports Java, Rust, C# and Python and we aim to support more language servers from the [list of language server implementations](https://microsoft.github.io/language-server-protocol/implementors/servers/).\n\nSome of the analyses results that `multilspy` can provide are:\n- Finding the definition of a function or a class ([textDocument/definition](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_definition))\n- Finding the callers of a function or the instantiations of a class ([textDocument/references](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_references))\n- Providing type-based dereference completions ([textDocument/completion](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_completion))\n- Getting information displayed when hovering over symbols, like method signature ([textDocument/hover](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_hover))\n- Getting list/tree of all symbols defined in a given file, along with symbol type like class, method, etc. ([textDocument/documentSymbol](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_documentSymbol))\n- Please create an issue/PR to add any other LSP request not listed above\n\n### Installation\nTo install `multilspy` using pip, execute the following command:\n```\npip install https://github.com/microsoft/multilspy/archive/main.zip\n```\n\n### Usage\nExample usage:\n```python\nfrom multilspy import SyncLanguageServer\nfrom multilspy.multilspy_config import MultilspyConfig\nfrom multilspy.multilspy_logger import MultilspyLogger\n...\nconfig = MultilspyConfig.from_dict({\"code_language\": \"java\"}) # Also supports \"python\", \"rust\", \"csharp\"\nlogger = MultilspyLogger()\nlsp = SyncLanguageServer.create(config, logger, \"/abs/path/to/project/root/\")\nwith lsp.start_server():\n    result = lsp.request_definition(\n        \"relative/path/to/code_file.java\", # Filename of location where request is being made\n        163, # line number of symbol for which request is being made\n        4 # column number of symbol for which request is being made\n    )\n    result2 = lsp.request_completions(\n        ...\n    )\n    result3 = lsp.request_references(\n        ...\n    )\n    result4 = lsp.request_document_symbols(\n        ...\n    )\n    result5 = lsp.request_hover(\n        ...\n    )\n    ...\n```\n\n`multilspy` also provides an asyncio based API which can be used in async contexts. Example usage (asyncio):\n```python\nfrom multilspy import LanguageServer\n...\nlsp = LanguageServer.create(...)\nasync with lsp.start_server():\n    result = await lsp.request_definition(\n        ...\n    )\n    ...\n```\n\nThe file [microsoft/multilspy - src/multilspy/language_server.py](https://github.com/microsoft/multilspy/blob/main/src/multilspy/language_server.py) provides the `multilspy` API. Several tests for `multilspy` present under [microsoft/multilspy - tests/multilspy/](https://github.com/microsoft/multilspy/tree/main/tests/multilspy) provide detailed usage examples for `multilspy`. The tests can be executed by running:\n```bash\npytest tests/multilspy\n```\n\n## 5. Monitor-Guided Decoding\n\nA monitor under the Monitor-Guided Decoding framework, is instantiated using `multilspy` as the LSP client, and provides maskgen to guide the LM decoding. The monitor interface is defined as class `Monitor` in file [src/monitors4codegen/monitor_guided_decoding/monitor.py](src/monitors4codegen/monitor_guided_decoding/monitor.py). The interface is implemented by various monitors supporting different properties like valid identifier dereferences, valid number of arguments, valid typestate method calls, etc.\n\n### MGD with HuggingFace models\n[src/monitors4codegen/monitor_guided_decoding/hf_gen.py](src/monitors4codegen/monitor_guided_decoding/hf_gen.py) provides the class `MGDLogitsProcessor` which can be used with any HuggingFace Language Model, as a [`LogitsProcessor`](https://huggingface.co/docs/transformers/internal/generation_utils#logitsprocessor) to guide the LM using MGD. Example uses with [SantaCoder](https://huggingface.co/bigcode/santacoder) model are available in [tests/monitor_guided_decoding/test_dereferences_monitor_java.py](tests/monitor_guided_decoding/test_dereferences_monitor_java.py).\n\n### MGD with OpenAI models\n[src/monitors4codegen/monitor_guided_decoding/openai_gen.py](src/monitors4codegen/monitor_guided_decoding/openai_gen.py) provides the method `openai_mgd` which takes the prompt and a `Monitor` as input, and returns the MGD guided generation using an OpenAI model.\n\n### Monitors\n#### Dereferences Monitor\n[src/monitors4codegen/monitor_guided_decoding/monitors/dereferences_monitor.py](src/monitors4codegen/monitor_guided_decoding/monitors/dereferences_monitor.py) provides the instantiation of `Monitor` class for dereferences monitor. It can be used to guide LMs to generate valid identifier dereferences. Unit tests for the dereferences monitor are present in [tests/monitor_guided_decoding/test_dereferences_monitor_java.py](tests/monitor_guided_decoding/test_dereferences_monitor_java.py), which also provide usage examples for the dereferences monitor.\n\n#### Monitor for valid number of arguments to function calls\n[src/monitors4codegen/monitor_guided_decoding/monitors/numargs_monitor.py](src/monitors4codegen/monitor_guided_decoding/monitors/numargs_monitor.py) provides the instantiation of `Monitor` class for numargs_monitor. It can be used to guide LMs to generate correct number of arguments to function calls. Unit tests, which also provide usage examples are present in [tests/monitor_guided_decoding/test_numargs_monitor_java.py](tests/monitor_guided_decoding/test_numargs_monitor_java.py).\n\n#### Monitor for typestate specifications\nThe typestate analysis is used to enforce that methods on an object are called in a certain order, consistent with the ordering constraints provided by the API contracts. Example usage of the typestate monitor for Rust is available in the unit test file [tests/monitor_guided_decoding/test_typestate_monitor_rust.py](tests/monitor_guided_decoding/test_typestate_monitor_rust.py).\n\n#### Switch-Enum Monitor\n[src/monitors4codegen/monitor_guided_decoding/monitors/switch_enum_monitor.py](src/monitors4codegen/monitor_guided_decoding/monitors/switch_enum_monitor.py) provides the instantiation of `Monitor` for generating valid named enum constants in C#. Unit tests for the switch-enum monitor are present in [tests/monitor_guided_decoding/test_switchenum_monitor_csharp.py](tests/monitor_guided_decoding/test_switchenum_monitor_csharp.py), which also provide usage examples for the switch-enum monitor.\n\n#### Class Instantiation Monitor\n[src/monitors4codegen/monitor_guided_decoding/monitors/class_instantiation_monitor.py](src/monitors4codegen/monitor_guided_decoding/monitors/class_instantiation_monitor.py) provides the instantiation of `Monitor` for generating valid class instantiations following `'new '` in a Java code base. Unit tests for the class-instantiation monitor, which provide examples usages are present in [tests/monitor_guided_decoding/test_classinstantiation_monitor_java.py](tests/monitor_guided_decoding/test_classinstantiation_monitor_java.py).\n\n### Joint Monitoring\nMultiple monitors can be used simultaneously to guide LMs to adhere to multiple properties. Example demonstration with 2 monitors used jointly are present in [tests/monitor_guided_decoding/test_joint_monitors.py](tests/monitor_guided_decoding/test_joint_monitors.py).\n\n## Frequently Asked Questions (FAQ)\n### ```asyncio``` related Runtime error when executing the tests for MGD\nIf you get the following error:\n```\nRuntimeError: Task <Task pending name='Task-2' coro=<_AsyncGeneratorContextManager.__aenter__() running at\n    python3.8/contextlib.py:171> cb=[_chain_future.<locals>._call_set_state() at\n    python3.8/asyncio/futures.py:367]> got Future <Future pending> attached to a different loop python3.8/asyncio/locks.py:309: RuntimeError\n```\n\nPlease ensure that you create a new environment with Python ```>=3.10```. For further details, please have a look at the [StackOverflow Discussion](https://stackoverflow.com/questions/73599594/asyncio-works-in-python-3-10-but-not-in-python-3-8).\n",
                "type": "Text_excerpt",
                "original_header": "BASH11* related Runtime error when executing the tests for MGD",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context",
                    "Frequently Asked Questions (FAQ)"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ],
    "faq": [
        {
            "result": {
                "value": "# Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context\nAlternative title: Guiding Language Models of Code with Global Context using Monitors\n\n## Introduction\nThis repository hosts the official code and data artifact for the paper [\"Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context\"](https://neurips.cc/virtual/2023/poster/70362) appearing at NeurIPS 2023 ([\"Guiding Language Models of Code with Global Context using Monitors\"](https://arxiv.org/abs/2306.10763) on Arxiv). The work introduces Monitor-Guided Decoding (MGD) for code generation using Language Models, where a monitor uses static analysis to guide the decoding.\n\n## Repository Contents\n1. [Datasets](#1-datasets): PragmaticCode and DotPrompts\n2. [Evaluation scripts](#2-evaluation-scripts): Scripts to evaluate LMs by taking as input inferences (code generated by the model) for examples in DotPrompts and producing score@k scores for the metrics reported in the paper: Compilation Rate (CR), Next-Identifier Match (NIM), Identifier-Sequence Match (ISM) and Prefix Match (PM).\n3. [Inference Results over DotPrompts](#3-inference-results-over-dotprompts): Generated code for examples in DotPrompts with various model configurations reported in the paper. The graphs and tables reported in the paper can be reproduced by running the evaluation scripts on the provided inference results.\n4. [`multilspy`](#4-multilspy): A cross-platform library designed to simplify the process of creating language server clients to query and obtain results of various static analyses from a wide variety of language servers that communicate over the [Language Server Protocol](https://microsoft.github.io/language-server-protocol/). `multilspy` is intended to be used as a library to easily query various language servers, without having to worry about setting up their configurations and implementing the client-side of language server protocol. `multilspy` currently supports running language servers for Java, Rust, C# and Python, and we aim to expand this list with the help of the community.\n5. [Monitor-Guided Decoding](#5-monitor-guided-decoding): Implementation of various monitors monitoring for different properties reported in the paper (for example: monitoring for type-valid identifier dereferences, monitoring for correct number of arguments to method calls, monitoring for typestate validity of method call sequences, etc.), spanning 3 programming languages.\n\n**The `multilspy` library has now been migrated to [microsoft/multilspy](https://github.com/microsoft/multilspy).**\n\n## Monitor-Guided Decoding: Motivating Example\nFor example, consider the partial code to be completed in the figure below. To complete this code, an LM has to generate identifiers consistent with the type of the object returned by `ServerNode.Builder.newServerNode()`. The method `newServerNode` and its return type, class `ServerNode.Builder`, are defined in another file. If an LM does not have information about the `ServerNode.Builder type`, it ends up hallucinating, as can be seen in the example generations with the text-davinci-003 and SantaCoder models. The completion uses identifiers `host` and `port`, which do not exist in the type `ServerNode.Builder`. The generated code therefore results in \u201csymbol not found\u201d compilation errors. \n\nMGD uses static analysis to guide the decoding of LMs, to generate code following certain properties. In the example, MGD is used to monitor for generating code with type-correct dereferences, and the SantaCoder model with the same prompt is able to generate the correct code completion, which compiles and matches the ground truth as well.\n\nAs reported in the paper, we observe that **MGD can improve the compilation rate of code generated by LMs at all scales (350M-175B) by 19-25%**, without any training/fine-tuning required. Further, it boosts the ground-truth match at all granularities from token-level to method-level code completion.\n\n![](figures/motivating_example.png)\n\n## 1. Datasets\n\n### Dataset Statistics\n|||\n|--------------|:-----:|\n| Number of repositories in PragmaticCode |  100 |\n| Number of methods in DotPrompts | 1420 |\n| Number of examples in DotPrompts | 10538 |\n\n### PragmaticCode\nPragmaticCode is a dataset of real-world open-source Java projects complete with their development environments and dependencies (through their respective build systems). The authors tried to ensure that all the repositories in PragmaticCode were released publicly only after the determined training dataset cutoff date (31 March 2022) for the CodeGen, SantaCoder and text-davinci-003 family of models, which were used to evaluate MGD.\n\nThe full dataset, along with repository zip files is available in our Zenodo dataset release at [https://zenodo.org/records/10072088](https://zenodo.org/records/10072088). The list of repositories along with their respective licenses consisting PragmaticCode is available in [datasets/PragmaticCode/repos.csv](datasets/PragmaticCode/repos.csv). The contents of the files required for inference for each of the repositories is available in [datasets/PragmaticCode/fileContentsByRepo.json](datasets/PragmaticCode/fileContentsByRepo.json).\n\n### DotPrompts\nDotPrompts is a set of examples derived from PragmaticCode, such that each example consists of a prompt to a dereference location (a code location having the \".\" operator in Java). DotPrompts can be used to benchmark Language Models of Code on their ability to utilize repository level context to generate code for method-level completion tasks. The task for the models is to complete a partially written Java method, utilizing the full repository available from PragmaticCode. Since all the repositories in PragmaticCode are buildable, DotPrompts (derived from PragmaticCode) supports Compilation Rate as a metric of evaluation for generated code, apart from standard metrics of ground truth match like Next-Identifier Match, Identifier Sequence Match and Prefix Match. \n\nThe scenario described in [motivating example above](#monitor-guided-decoding-motivating-example) is an example in DotPrompts.\n\nThe complete description of an example in DotPrompts is a tuple - `(repo, classFileName, methodStartIdx, methodStopIdx, dot_idx)`. The dataset is available at [datasets/DotPrompts/dataset.csv](datasets/DotPrompts/dataset.csv).\n\n## 2. Evaluation Scripts\n### Environment Setup\nWe use the Python packages listed in [requirements.txt](requirements.txt). Our experiments used python 3.10. It is recommended to install the same with dependencies in an isolated virtual environment. To create a virtual environment using `venv`:\n```setup\npython3 -m venv venv_monitors4codegen\nsource venv_monitors4codegen/bin/activate\n```\nor using conda:\n```\nconda create -n monitors4codegen python=3.10\nconda activate monitors4codegen\n```\nFurther details and instructions on creation of python virtual environments can be found in the [official documentation](https://docs.python.org/3/library/venv.html). Further, we also refer users to [Miniconda](https://docs.conda.io/en/latest/miniconda.html), as an alternative to the above steps for creation of the virtual environment.\n\nTo install the requirements for running evaluations as described [below](#2-evaluation-scripts):\n\n```setup\npip3 install -r requirements.txt\n```\n\n### Running the evaluation script\nThe evaluation script can be run as follows:\n```\npython3 eval_results.py <path to inference results - csv> <path to PragmaticCode filecontents - json> <path to output directory>\n```\n\nThe above command will create a directory `<path to output directory>`, containing all the graphs and tables reported in the paper along with extra details. The command also generates a report in the output directory, named `Report.md` which relates the generated figures to sections in the paper. \n\nTo ensure that the environment setup has been done correctly, please run the below command, which runs the evaluation script over dummy data (included in [inference_results/dotprompts_results_sample.csv](inference_results/dotprompts_results_sample.csv)). If the command fails, that indicates an error in the environment setup and the authors request you to kindly report the same.\n```\npython3 evaluation_scripts/eval_results.py inference_results/dotprompts_results_sample.csv datasets/PragmaticCode/fileContentsByRepo.json results_sample/\n```\n\n### Description of `inference results csv` file format\nDescription of expected columns in the inference results csv input to the evaluation script:\n* `repo`: Name of the repository from which the testcase was sourced\n* `classFileName`: relative path to file containing the testcase prompt location\n* `methodStartIdx`: String index of starting `'{'` of the method\n* `methodStopIdx`: String index of closing `'}'` of the method\n* `dot_idx`: String index of `'.'` that is the dereference prompt point\n* `configuration`: Identifies the configuration used to generate the given code sample. Values from: `['SC-classExprTypes', 'CG-6B', 'SC-FIM-classExprTypes', 'SC-RLPG-MGD', 'SC-MGD', 'SC-FIM-classExprTypes-MGD', 'CG-2B', 'SC', 'CG-2B-MGD', 'CG-350M-classExprTypes-MGD', 'SC-FIM', 'TD-3', 'CG-350M-MGD', 'SC-FIM-MGD', 'SC-RLPG', 'CG-350M', 'CG-350M-classExprTypes', 'SC-classExprTypes-MGD', 'CG-6B-MGD', 'TD-3-MGD']`\n* `temperature`: Temperature used for sampling. Values from: `[0.8, 0.6, 0.4, 0.2]`\n* `model`: Name of the model used for sampling. Values from: `['Salesforce/codegen-6B-multi', 'bigcode/santacoder', 'Salesforce/codegen-2B-multi', 'Salesforce/codegen-350M-multi', 'text-davinci-003']`\n* `context`: Decoding strategy used. Values from: `['autoregressive', 'fim']`\n* `prefix`: Prompt strategy used. Values from: `['classExprTypes', 'none', 'rlpg']`\n* `rlpg_best_rule_name`: Name of the rule used for creating RLPG prompt (if used for the corresponding testcase). Values from: `[nan, 'in_file#lines#0.25', 'in_file#lines#0.5', 'in_file#lines#0.75', 'import_file#method_names#0.5']`\n* `output`: Generated output by the model\n* `compilationSucceeded`: Result of compiling the generated method in the context of the full repository. 1 if success, 0 otherwise. Values from: `[1, 0]`\n\n## 3. Inference Results over DotPrompts\nWe provide all inferences (generated code) generated by all model configurations reported in the paper, for every example in DotPrompts. This consists of 6 independently sampled inferences for 18 different model configurations (spanning parameter scale, prompt templates, use of FIM context, etc.) for every example in DotPrompts.\n\nThe generated samples along with their compilation status, following the format described [above](#description-of-inference-results-csv-file-format), is available at [inference_results/dotprompts_results.csv](inference_results/dotprompts_results.csv). The file is stored using [git lfs](https://git-lfs.com/). If the file is not available locally after cloning this repository, please check the [git lfs website](https://git-lfs.com/) for instructions on setup, and clone the repository again after git lfs setup.\n\nEach row in the file contains several multi-line string cells, and therefore, while viewing them in tools like Microsoft Office Excel, kindly enable \"Word Wrap\" to be able to view the full contents.\n\nTo run the [evaluation scripts](#2-evaluation-scripts) over the inferences, in order to reproduce the graphs and tables reported in the paper, run:\n```\npython3 evaluation_scripts/eval_results.py inference_results/dotprompts_results.csv datasets/PragmaticCode/fileContentsByRepo.json results/\n```\n\nThe above command creates a directory [results](results/) (already included in the repository), containing all the figures and tables provided in the paper along with extra details. The command also generates a report in the output directory which relates the generated figures to sections in the paper. In case of above command, the report is generated at [results/Report.md](results/Report.md).\n\n## 4. `multilspy`\n**The `multilspy` library has now been migrated to [microsoft/multilspy](https://github.com/microsoft/multilspy).**\n\n`multilspy` is a cross-platform library that we have built to set up and interact with various language servers in a unified and easy way. [Language servers]((https://microsoft.github.io/language-server-protocol/overviews/lsp/overview/)) are tools that perform a variety of static analyses on source code and provide useful information such as type-directed code completion suggestions, symbol definition locations, symbol references, etc., over the [Language Server Protocol (LSP)](https://microsoft.github.io/language-server-protocol/overviews/lsp/overview/). `multilspy` intends to ease the process of using language servers, by abstracting the setting up of the language servers, performing language-specific configuration and handling communication with the server over the json-rpc based protocol, while exposing a simple interface to the user.\n\nSince LSP is language-agnostic, `multilspy` can provide the results for static analyses of code in different languages over a common interface. `multilspy` is easily extensible to any language that has a Language Server and currently supports Java, Rust, C# and Python and we aim to support more language servers from the [list of language server implementations](https://microsoft.github.io/language-server-protocol/implementors/servers/).\n\nSome of the analyses results that `multilspy` can provide are:\n- Finding the definition of a function or a class ([textDocument/definition](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_definition))\n- Finding the callers of a function or the instantiations of a class ([textDocument/references](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_references))\n- Providing type-based dereference completions ([textDocument/completion](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_completion))\n- Getting information displayed when hovering over symbols, like method signature ([textDocument/hover](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_hover))\n- Getting list/tree of all symbols defined in a given file, along with symbol type like class, method, etc. ([textDocument/documentSymbol](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_documentSymbol))\n- Please create an issue/PR to add any other LSP request not listed above\n\n### Installation\nTo install `multilspy` using pip, execute the following command:\n```\npip install https://github.com/microsoft/multilspy/archive/main.zip\n```\n\n### Usage\nExample usage:\n```python\nfrom multilspy import SyncLanguageServer\nfrom multilspy.multilspy_config import MultilspyConfig\nfrom multilspy.multilspy_logger import MultilspyLogger\n...\nconfig = MultilspyConfig.from_dict({\"code_language\": \"java\"}) # Also supports \"python\", \"rust\", \"csharp\"\nlogger = MultilspyLogger()\nlsp = SyncLanguageServer.create(config, logger, \"/abs/path/to/project/root/\")\nwith lsp.start_server():\n    result = lsp.request_definition(\n        \"relative/path/to/code_file.java\", # Filename of location where request is being made\n        163, # line number of symbol for which request is being made\n        4 # column number of symbol for which request is being made\n    )\n    result2 = lsp.request_completions(\n        ...\n    )\n    result3 = lsp.request_references(\n        ...\n    )\n    result4 = lsp.request_document_symbols(\n        ...\n    )\n    result5 = lsp.request_hover(\n        ...\n    )\n    ...\n```\n\n`multilspy` also provides an asyncio based API which can be used in async contexts. Example usage (asyncio):\n```python\nfrom multilspy import LanguageServer\n...\nlsp = LanguageServer.create(...)\nasync with lsp.start_server():\n    result = await lsp.request_definition(\n        ...\n    )\n    ...\n```\n\nThe file [microsoft/multilspy - src/multilspy/language_server.py](https://github.com/microsoft/multilspy/blob/main/src/multilspy/language_server.py) provides the `multilspy` API. Several tests for `multilspy` present under [microsoft/multilspy - tests/multilspy/](https://github.com/microsoft/multilspy/tree/main/tests/multilspy) provide detailed usage examples for `multilspy`. The tests can be executed by running:\n```bash\npytest tests/multilspy\n```\n\n## 5. Monitor-Guided Decoding\n\nA monitor under the Monitor-Guided Decoding framework, is instantiated using `multilspy` as the LSP client, and provides maskgen to guide the LM decoding. The monitor interface is defined as class `Monitor` in file [src/monitors4codegen/monitor_guided_decoding/monitor.py](src/monitors4codegen/monitor_guided_decoding/monitor.py). The interface is implemented by various monitors supporting different properties like valid identifier dereferences, valid number of arguments, valid typestate method calls, etc.\n\n### MGD with HuggingFace models\n[src/monitors4codegen/monitor_guided_decoding/hf_gen.py](src/monitors4codegen/monitor_guided_decoding/hf_gen.py) provides the class `MGDLogitsProcessor` which can be used with any HuggingFace Language Model, as a [`LogitsProcessor`](https://huggingface.co/docs/transformers/internal/generation_utils#logitsprocessor) to guide the LM using MGD. Example uses with [SantaCoder](https://huggingface.co/bigcode/santacoder) model are available in [tests/monitor_guided_decoding/test_dereferences_monitor_java.py](tests/monitor_guided_decoding/test_dereferences_monitor_java.py).\n\n### MGD with OpenAI models\n[src/monitors4codegen/monitor_guided_decoding/openai_gen.py](src/monitors4codegen/monitor_guided_decoding/openai_gen.py) provides the method `openai_mgd` which takes the prompt and a `Monitor` as input, and returns the MGD guided generation using an OpenAI model.\n\n### Monitors\n#### Dereferences Monitor\n[src/monitors4codegen/monitor_guided_decoding/monitors/dereferences_monitor.py](src/monitors4codegen/monitor_guided_decoding/monitors/dereferences_monitor.py) provides the instantiation of `Monitor` class for dereferences monitor. It can be used to guide LMs to generate valid identifier dereferences. Unit tests for the dereferences monitor are present in [tests/monitor_guided_decoding/test_dereferences_monitor_java.py](tests/monitor_guided_decoding/test_dereferences_monitor_java.py), which also provide usage examples for the dereferences monitor.\n\n#### Monitor for valid number of arguments to function calls\n[src/monitors4codegen/monitor_guided_decoding/monitors/numargs_monitor.py](src/monitors4codegen/monitor_guided_decoding/monitors/numargs_monitor.py) provides the instantiation of `Monitor` class for numargs_monitor. It can be used to guide LMs to generate correct number of arguments to function calls. Unit tests, which also provide usage examples are present in [tests/monitor_guided_decoding/test_numargs_monitor_java.py](tests/monitor_guided_decoding/test_numargs_monitor_java.py).\n\n#### Monitor for typestate specifications\nThe typestate analysis is used to enforce that methods on an object are called in a certain order, consistent with the ordering constraints provided by the API contracts. Example usage of the typestate monitor for Rust is available in the unit test file [tests/monitor_guided_decoding/test_typestate_monitor_rust.py](tests/monitor_guided_decoding/test_typestate_monitor_rust.py).\n\n#### Switch-Enum Monitor\n[src/monitors4codegen/monitor_guided_decoding/monitors/switch_enum_monitor.py](src/monitors4codegen/monitor_guided_decoding/monitors/switch_enum_monitor.py) provides the instantiation of `Monitor` for generating valid named enum constants in C#. Unit tests for the switch-enum monitor are present in [tests/monitor_guided_decoding/test_switchenum_monitor_csharp.py](tests/monitor_guided_decoding/test_switchenum_monitor_csharp.py), which also provide usage examples for the switch-enum monitor.\n\n#### Class Instantiation Monitor\n[src/monitors4codegen/monitor_guided_decoding/monitors/class_instantiation_monitor.py](src/monitors4codegen/monitor_guided_decoding/monitors/class_instantiation_monitor.py) provides the instantiation of `Monitor` for generating valid class instantiations following `'new '` in a Java code base. Unit tests for the class-instantiation monitor, which provide examples usages are present in [tests/monitor_guided_decoding/test_classinstantiation_monitor_java.py](tests/monitor_guided_decoding/test_classinstantiation_monitor_java.py).\n\n### Joint Monitoring\nMultiple monitors can be used simultaneously to guide LMs to adhere to multiple properties. Example demonstration with 2 monitors used jointly are present in [tests/monitor_guided_decoding/test_joint_monitors.py](tests/monitor_guided_decoding/test_joint_monitors.py).\n\n## Frequently Asked Questions (FAQ)\n### ```asyncio``` related Runtime error when executing the tests for MGD\nIf you get the following error:\n```\nRuntimeError: Task <Task pending name='Task-2' coro=<_AsyncGeneratorContextManager.__aenter__() running at\n    python3.8/contextlib.py:171> cb=[_chain_future.<locals>._call_set_state() at\n    python3.8/asyncio/futures.py:367]> got Future <Future pending> attached to a different loop python3.8/asyncio/locks.py:309: RuntimeError\n```\n\nPlease ensure that you create a new environment with Python ```>=3.10```. For further details, please have a look at the [StackOverflow Discussion](https://stackoverflow.com/questions/73599594/asyncio-works-in-python-3-10-but-not-in-python-3-8).\n",
                "type": "Text_excerpt",
                "original_header": "BASH11* related Runtime error when executing the tests for MGD",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context",
                    "Frequently Asked Questions (FAQ)"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ],
    "contributing_guidelines": [
        {
            "result": {
                "value": "This project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n",
                "type": "Text_excerpt",
                "original_header": "Contributing",
                "parent_header": [
                    "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ],
    "images": [
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/figures/motivating_example.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2306.10763"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/microsoft/monitors4codegen/main/README.md"
        }
    ]
}