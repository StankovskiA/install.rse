{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-03 18:51:19"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/similarityMetrics/similarityMetrics",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "similarityMetrics",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2021-04-14T17:19:25Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-05-27T14:18:08Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "name": [
        {
            "result": {
                "value": "similarityMetrics",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "similarityMetrics/similarityMetrics",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/similarityMetrics/similarityMetrics/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/similarityMetrics/similarityMetrics/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 5,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 0,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/similarityMetrics/similarityMetrics/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 38319
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "type": [
        {
            "result": {
                "value": "commandline-application",
                "type": "String"
            },
            "confidence": 0.82,
            "technique": "software_type_heuristics"
        }
    ],
    "requirements": [
        {
            "result": {
                "value": "We assume Ubuntu 18.04, Python 3.6.7, Keras 2.4.3, numpy 1.19.5, Tensorflow 2.4.1, javalang 0.13.0, nltk 3.6.1, pandas 1.1.5, py-rouge 1.1, pytorch 1.8.1, sentence-transformers 1.1.0, matplotlib 3.3.4\n",
                "type": "Text_excerpt",
                "original_header": "Dependencies",
                "parent_header": [
                    "Semantic Similarity Metrics for Evaluating Source Code Summarization"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md"
        }
    ],
    "run": [
        {
            "result": {
                "value": "```console\nresearch@server:~/dev/similarityMetrics$ time python3 -W ignore simmetrics.py --help\n```\nThis will output the list of input arguments that can be passed via the command line to figure out what information needs to be included to run the simmetrics.py file.\n\nTo run the similarity using attendgru embedding, the trained model needs to be downloaded in the similarityMetrics directory.\nThe model can be downloaded from the following link:\nhttps://drive.google.com/file/d/1tDiv6kRRwydhYi8wY3Wgv_cL7Jkur-Pd/view?usp=sharing\n\nTo run the similarity using inferSent encoding, follow the instructions in their official repository to download the Glove embedding and the pretrained model. The instructions can be found in the following repository:\nhttps://github.com/facebookresearch/InferSent\n",
                "type": "Text_excerpt",
                "original_header": "Step 3: Run Similarity Metrics",
                "parent_header": [
                    "Semantic Similarity Metrics for Evaluating Source Code Summarization",
                    "Similarity Metrics"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md"
        },
        {
            "result": {
                "value": "```console\nresearch@server:~/dev/similarityMetrics$ time python3 -W ignore use_score_v.py --help\n```\nThis will output the list of input arguments that can be passed via the command line to figure out what information needs to be included to run the use_score_v.py file.\n\nThe use_score_v.py file can be used to get the cosine similarity of the embedding obtained using the universal sentence encoder(large) model (This is refered to as the USE+c score).\nThe following command can be run to obtain the USE+c score on the 210 comment set used in the human study for our paper:\n```console\nresearch@server:~/dev/similarityMetrics$ time python -W ignore use_score_v.py comsdata/attendgru_coms.txt --coms-filename=comsdata/refcoms.txt --batchsize=20000 --gpu=0\n```\nYou can adjust the batch-size depending on the gpu memory and change the generated-input-filename and the coms-filename to compute the USE+c score on your own reference and generated output.\n",
                "type": "Text_excerpt",
                "original_header": "Run Universal Sentence Encoder on your own reference and baseline generated comments",
                "parent_header": [
                    "Semantic Similarity Metrics for Evaluating Source Code Summarization"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md"
        }
    ],
    "support": [
        {
            "result": {
                "value": "We also include the 210 function dataset that we used in the human study in the comsdata/ directory in both pickle format and txt format.\nThe raw data obtained from the human study is also made available in this repository in the final_megafile.csv file.\nThe Spearman Rho and Kendall Tau correlations we compute from the raw data is also made available in this repository.\n",
                "type": "Text_excerpt",
                "original_header": "Human Study Data",
                "parent_header": [
                    "Semantic Similarity Metrics for Evaluating Source Code Summarization"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "```console\nresearch@server:~/dev/similarityMetrics$ time python3 -W ignore use_score_v.py --help\n```\nThis will output the list of input arguments that can be passed via the command line to figure out what information needs to be included to run the use_score_v.py file.\n\nThe use_score_v.py file can be used to get the cosine similarity of the embedding obtained using the universal sentence encoder(large) model (This is refered to as the USE+c score).\nThe following command can be run to obtain the USE+c score on the 210 comment set used in the human study for our paper:\n```console\nresearch@server:~/dev/similarityMetrics$ time python -W ignore use_score_v.py comsdata/attendgru_coms.txt --coms-filename=comsdata/refcoms.txt --batchsize=20000 --gpu=0\n```\nYou can adjust the batch-size depending on the gpu memory and change the generated-input-filename and the coms-filename to compute the USE+c score on your own reference and generated output.\n",
                "type": "Text_excerpt",
                "original_header": "Run Universal Sentence Encoder on your own reference and baseline generated comments",
                "parent_header": [
                    "Semantic Similarity Metrics for Evaluating Source Code Summarization"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md"
        }
    ],
    "description": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This repository contains code and data for calculating semantic similarity metrics for evaluating source code summarization. This project is aimed to demonstrate the empirical and statistical evidence that bleu score does not correlate between similarity between reference and autmatically generated comments. It also provides a script for computing cosine similarity score using universal sentence encoder on the comment pairs.\n \n",
                "original_header": "Semantic Similarity Metrics for Evaluating Source Code Summarization"
            },
            "confidence": 0.9971092724329788,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "(Their code was downloaded from: https://github.com/mcmillco/funcom)\n \n",
                "original_header": "Step 2: Train Attendgru"
            },
            "confidence": 0.9974932917791103,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "Semantic Similarity Metrics for Evaluating Source Code Summarization"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/similarityMetrics/similarityMetrics/master/README.md"
        }
    ]
}