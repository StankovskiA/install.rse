{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 00:25:00"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/FlowSs/RLMutation",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "FlowSs",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2023-01-12T23:13:05Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-01-17T18:45:41Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "license": [
        {
            "result": {
                "value": "https://api.github.com/licenses/mit",
                "type": "License",
                "name": "MIT License",
                "url": "https://api.github.com/licenses/mit",
                "spdx_id": "MIT"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "MIT License\n\nCopyright (c) 2022 rlmuticst2023\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
                "type": "File_dump"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/LICENSE"
        }
    ],
    "name": [
        {
            "result": {
                "value": "RLMutation",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "FlowSs/RLMutation",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/FlowSs/RLMutation/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/FlowSs/RLMutation/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 4,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 1,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/FlowSs/RLMutation/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 166693
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "Shell",
                "name": "Shell",
                "type": "Programming_language",
                "size": 621
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "has_script_file": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/RLMT/algo_loop.sh",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "requirements": [
        {
            "result": {
                "value": "Python 3.8 was used alongside the following libraries:\n\n<details>\n  <summary>Click to see</summary>\n  \n```\nabsl-py==1.2.0\nBox2D==2.3.10\ncachetools==5.2.0\ncharset-normalizer==2.1.1\ncloudpickle==2.2.0\ncontourpy==1.0.5\ncycler==0.11.0\nfonttools==4.37.4\ngoogle-auth==2.12.0\ngoogle-auth-oauthlib==0.4.6\ngrpcio==1.49.1\ngym==0.21.0\nidna==3.4\nimportlib-metadata==4.13.0\nkiwisolver==1.4.4\nMarkdown==3.4.1\nMarkupSafe==2.1.1\nmatplotlib==3.6.1\nnumpy==1.23.1\noauthlib==3.2.1\npackaging==21.3\npandas==1.5.0\npatsy==0.5.3\nPillow==9.2.0\nprotobuf==3.19.6\npyasn1==0.4.8\npyasn1-modules==0.2.8\npyparsing==3.0.9\npython-dateutil==2.8.2\npytz==2022.4\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.9\nscipy==1.9.2\nsix==1.16.0\nstable-baselines3==1.6.2\nstatsmodels==0.13.2\ntensorboard==2.10.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntorch==1.11.0\ntqdm==4.64.1\ntyping_extensions==4.4.0\nurllib3==1.26.12\nWerkzeug==2.2.2\nzipp==3.9.0\n```\n</details>\n",
                "type": "Text_excerpt",
                "original_header": "Requirements",
                "parent_header": [
                    "Mutation Testing for Deep Reinforcement Learning"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "07/02/2023: Error in the 'no_discount_factor' mutation applied to DQN: it should be `(1 - replay_data.dones)` and not `(replay_data.dones)`. Updated the code as well as provided trained agents with the correct mutation in `results/corrected_NDF_DQN/`.\n\nWhen testing on the initial environment, the following results were obtained with the new agents:\n\n```\nEnvironement: CartPole-v1, Model dqn, Mutation NDF\nAverage Mutated/Healthy Ratio Test : 0.55\nReward Distribution Test : Killed\nDistance Distribution Test: Killed\n```\n\n```\nEnvironement: LunarLander-v2, Model dqn, Mutation NDF\nAverage Mutated/Healthy Ratio Test : 0.95\nReward Distribution Test : Killed\nDistance Distribution Test: Killed\n```\n\ni.e. only a change on the AVG method when the environment is CartPole-v1 (from 1.0 to 0.55), rest is unchanged.\n",
                "type": "Text_excerpt",
                "original_header": "Hot Fixes",
                "parent_header": [
                    "Mutation Testing for Deep Reinforcement Learning"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "If you wish to retrain the agents as we did, you can run the following script:\n```bash\npython train.py -a [algorithm] -na [number_of_agents] -e [environment] -t [total_steps] -s [start_from] -m [mutation] \n``` \nFor instance:\n```bash\npython train.py -a ppo -na 20 -e CartPole-v1 -t 200000 -s 0 -m '{\"no_reverse\": \"None\"}' \n``` \n",
                "original_header": "Training of agents"
            },
            "confidence": 0.9941636592998463,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "The next step is to evaluate the agents on the environment and to get their reward. This can\nbe done with the following scripts:\n```bash\npython test_agent.py -a [algorithm] -na [number_of_agents] -e [environment] -m [mutation] \n``` \n",
                "original_header": "Evaluating agents"
            },
            "confidence": 0.9967068342874938,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Thus,\n```bash\npython eval_mut.py -a ppo -e CartPole-v1 -m NR\n``` \n",
                "original_header": "Mutation test on the initial environment"
            },
            "confidence": 0.96310266498796,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "To search for the boundary test environments as well as to evaluate said test environments on\nthe FOM, the following script should be run:\n```bash\npython algo.py -a [algorithm] -e [environment] -i [init_val] -l [limits] -b [bounds] -t [test_mode] -n [number_of_cpus]\n``` \n",
                "original_header": "Generating test environments"
            },
            "confidence": 0.9978940482423809,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Models trained need to have the `logs/` directory set in the correct sub-directory. Once again, if you uncompress\nthe `.zip` from Zenodo, everything will be set accordingly. \nFinally, the following script should be run:\n```bash\npython hom_prop.py -a [algorithm] -e [environment] -t [test_mode] -n [number_of_cpus]\n``` \n",
                "original_header": "Evaluating Higher Order Mutation properties"
            },
            "confidence": 0.9991564584740544,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "[1] N. Humbatova, G. Jahangirova, and P. Tonella, \u201cDeepcrime: mutation\ntesting of deep learning systems based on real faults,\u201d in Proceedings of\nthe 30th ACM SIGSOFT International Symposium on Software Testing\nand Analysis, pp. 67\u201378, 2021.\n\n[2] Y. Lu, W. Sun, and M. Sun, \u201cTowards mutation testing of reinforcement\nlearning systems,\u201d Journal of Systems Architecture, vol. 131, p. 102701, 2022.\n\n[3] A. Nikanjam, M. M. Morovati, F. Khomh, and H. Ben Braiek, \u201cFaults\nin deep reinforcement learning programs: a taxonomy and a detection\napproach,\u201d Automated Software Engineering, vol. 29, no. 1, pp. 1\u201332, 2022\n\n[4] L. Ma, F. Zhang, J. Sun, M. Xue, B. Li, F. Juefei-Xu, C. Xie, L. Li,\nY. Liu, J. Zhao, et al., \u201cDeepmutation: Mutation testing of deep learning\nsystems,\u201d in 2018 IEEE 29th International Symposium on Software\nReliability Engineering (ISSRE), pp. 100\u2013111, IEEE, 2018.\n\n[5] Y. Jia and M. Harman, \u201cHigher order mutation testing,\u201d Information\nand Software Technology, vol. 51, no. 10, pp. 1379\u20131393, 2009. Source\nCode Analysis and Manipulation, SCAM 2008.\n\n[6] T. Everitt, V. Krakovna, L. Orseau, M. Hutter, and S. Legg, \u201cReinforce-\nment learning with a corrupted reward channel,\u201d 2017.\n",
                "type": "Text_excerpt",
                "original_header": "References",
                "parent_header": [
                    "Mutation Testing for Deep Reinforcement Learning"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        }
    ],
    "application_domain": [
        {
            "result": {
                "type": "String",
                "value": "Reinforcement Learning"
            },
            "confidence": 0.9341666666666667,
            "technique": "supervised_classification"
        }
    ],
    "description": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This replication package is intended for the paper \"_Mutation Testing of Deep Reinforcement Learning\nBased on Real Faults_\" accepted to the International Conference on Software Testing (ICST) 2023. A preprint version is available\non [arxiv](https://arxiv.org/abs/2301.05651) and the published version is available on the publisher's website [IEEE](https://ieeexplore.ieee.org/abstract/document/10132198).\n \n",
                "original_header": "Mutation Testing for Deep Reinforcement Learning"
            },
            "confidence": 0.9515269267060235,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "We give an extended version of the descriptions of our mutation operators as well as to which paper it references to.\n<details>\n  <summary>Click to see the table</summary> \n| Our Mutation | Description    | Reason for inclusion     | \n| :---        |    :----:   |  :--- |\n| _Reward Noise (RN)_      | In RL, one of the most important signals that the agent receives for evaluating its performance is the reward. The agent uses the reward signal to assess the quality of the actions it has taken in the states that it has visited. The _RN_ mutation operator adds a noise to the true reward that the agent was meant to receive and returns it to the agent. We denote the noisy reward $r_t^\\*$. The new reward is calculated as $$r_t^\\* = r_t + \\mathcal{N}(0,~0.1 \\times r_t)$$      | The agent may observe incorrect rewards from the environment due to sensory faults, bugs in environment implementation, or nefarious attacks [6]|  \n| _Mangled (M)_      | As explained in Background section, the agent collects its interactions with the environment in the form of $(s_t, a_t, r_t, s_{t+1})$ tuple. The correlation (like order, the resultant state/reward from previous actions) between these collected interactions, allows the agent to learn from its experiences. The _M_ mutation operator damages the correlation between collected experiences. This operator returns a random $s_{t+1}$ and $r_t$ which are not the state and reward the agent should receive according to its current state and the taken action. We denote the new randomly selected $s_{t+1}$ and $r_t$ as $s'_{t+1}$ and $r'\\_t$, respectively. Therefore, instead of receiving $(s_t, a_t, r_t, s\\_{t+1})$, the agent receives $(s\\_t, a\\_t, r'\\_t, s'\\_{t+1})$ | The agent may observe incorrect (state, reward) tuple from the environment due to sensory faults, bugs in environment implementation, or nefarious attacks [6]|  \n| _Random (Ra)_      | Similar to the mangled mutation operator, the \\textit{Ra} mutation returns a $(s_t, a_t, r'\\_t, s'\\_{t+1})$ tuple to the agent. However, unlike the _M_ operator where $s'\\_{t+1}$ and $r'\\_t$ are selected randomly and are not associated with each other, the random operator returns a $s'_{t+1}$ and $r'_t$ to the agent which were sampled from the same experience tuple but have no association with $s_t$ and $a_t$. | Same as above|  \n| _Repeat (R)_      | Returns the previous observation to the agent. If the agent has two consecutive experiences in the form of $(s_t, a_t, r_t, s_{t+1})$ and $(s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2})$, this operator returns $(s\\_{t+1}, a\\_{t+1}, r\\_{t+1}, s\\_{t+2})$ with $r\\_{t+1} = r\\_t, s\\_{t+2} = s\\_{t+1}$  | Same as above|  \n| _No Discount Factor (NDF)_      | In order for the agent to learn a balance between short-term and long-term received rewards, the discount factor is used. If the developer does not implement this concept for calculating the rewards during agent's training, the resulting agent will put equal importance on all the actions it has taken and the rewards it has received. As a result, such an agent will either be unable to correctly learn the mechanics of the environment or have difficulty in doing so.  | _Wrong network update_ category [3]|  \n| _Missing Terminal State (MTS)_      | In RL environments, the terminal state is defined as the state which was the last state the agent transitioned to before the environment was terminated. The termination criteria for an environment can be reaching the goal state, stepping into a trap state, or reaching a time limit. Normally in designing an RL environment, the terminal state contains a different reward signal than the rest of the transitions, e.g., if the agent falls in a trap, it should receive a negative reward. This mutation operator simulates the cases where the developer incorrectly implements identifying the termination criteria. As a result, the agent will not receive the termination signal and will be unable to correctly determine the correlation between the actions taken and the results achieved.  | _Missing terminal state_ category [3]|  \n| _No Reverse (NR)_      | For calculating the returns of an episode, recent rewards are discounted less than the earlier ones. During implementing this concept, it is imperative that the practitioner pays attention to how this concept is applied. Wrong network update, a common mistake that developers make is forgetting to reverse the order of the received rewards during calculating the returns. The mutation operator simulates such cases where the resulting agents, do not reverse the order of the received rewards and therefore learn an incorrect association between the experiences.  | _Wrong update rule_ category [3]|  \n| _Missing State Update (MSU)_      | The agent receives its interaction with the environment as a $(s\\_t, a\\_t, r\\_t, s\\_{t+1})$ tuple. Therefore, the developer needs to pay attention to correctly updating the state that agent is in, after taking an action. This means that after the agent takes another step and transitions from $s\\_{t+1}$ to $s\\_{t+2}$, the developer needs to update the experience tuple from $(s\\_t, a\\_t, r\\_t, s\\_{t+1})$ to $(s\\_{t+1}, a\\_{t+1}, r\\_{t+1}, s\\_{t+2})$. This mutation operator simulates the condition in which the developer makes a mistake in doing so. As a result, the agent always sees the same state in its experience tuple (e.g. $(s\\_{t}, a\\_{t+1}, r\\_{t+1}, s\\_{t+2})$ ) during training and is unable to learn the mechanics of the environment  | _Missing stepping the environment_ category [3] |  \n| _Incorrect Loss Function (ILF)_      | During implementation of a deep RL algorithm, one of the most important steps is the correct implementation of the loss function. The loss function calculates the error of the agent's estimations and updates its Neural Network (NN) accordingly. This mutation operator, simulates the cases where a developer implements the loss function incorrectly. As a result, the error for agent's NN outputs is incorrectly calculated and the agent will be unable to learn the mechanics of the environment.  | _Wrong update rule_ category [3] |  \n| _Policy Activation Change (PAC)_      | NNs generally reliy on non-linear activation functions such as ReLU and TanhH in order to introduce non-linearity which helps in modeling complex behaviors like object detection or image classification. Choosing the activation function correctly is important as it can have a great impact on how the model learns the policy. This mutation changes the default activation used in the policy network, i.e., NN that learns the agent\u2019s policy.  | DeepCrime [1] _Change Activation Function (ACH)_ |  \n| _Policy Optimizer Change (POC)_      | To search for the best set of weights for a given task, NNs use optimizers. An optimizer is an algorithm leveraged to minimize a loss function, with Gradient Descent methods being the most popular. As with activation, the choice of the optimizer used by an NN is crucial for the proper learning of the policy. This mutation modifies the default optimizer used for each algorithm, while keeping the learning rate the same. | DeepCrime [1] _Change Optimisation Function (OCH)_ |  \nWe list down the mutations used in Lu et al. [2].  We describe if a mutation is included to which of our mutations it refers to.\nIf not included we give a reason why it was not included, generally there is a lack of\nevidence that such operator is based on a real fault, that is they did not provide a reference and\nwe did not find evidence of such fault in the taxonomy we refer to. \n| Their Mutation | Description    | Included or not?     | \n| :---        |    :----:   |  :--- |\n| _Reward Reduction_      | Reduce High Reward       | Similar to our _Reward Noise_ except we add a noise instead which mimic a sensor failure better. Manually setting is also more complex and prone to bias. |  \n| _Reward Increase_      | Increase Low Reward       | Same as above   |  \n| _Reward Instability_      | Make rewards unstable       | Same as above   |  \n| _State Record Crash_      | Hide or Duplicate state-action pairs       | Similar to our _Repeat_ for the duplicate part. It's not clear how a state could *hide*, since something has to be returned to the agent. Nonetheless, we did not find a fault which would cause the program to _hide_ as described.   |  \n| _State Delay_      | Improperly associate a state with a subsequent action       | Similar to our _Mangled_  |  \n| _State Repetition_      | Associate a fixed state with a series of continuous actions       | Similar to our _Repeat_, but seems to be more general.  |  \n| _State Error_      | Associate a fixed state with a series of continuous actions       | Similar to our _Random_  |  \n| _Q-Table Fuzzing_      | Fuzz Q-Table       | Too specific to simple Q-learning. Not observed in the faults we had.  |  \n| _Input-Layer Neuron Removal_      | Remove input neurons       | While removed neurons can happen as they are describe in DeepCrime [1], they note that their implementation is \"tricky as they trigger a cascade of changes\" and \"The usage of such operators might be limited depending on the structure of the network under test as some of the generated mutants might be causing crashes, which is not particularly useful for the mutation testing\". In particular, Input/Output layers are sensitive ones as any change in the shape expected can cause problem since it won't match with the input fed to the network or expected by the labels which limits the relevance of generated mutants. Thus, we chose not to implement them.  |  \n| _Output-Layer Neuron Removal_      | Remove output neurons       | Same as above  |  \n| _Output-Layer Neuron Addition_      | Add neuron(s) in output layer       | Same as above  |  \n| _Changing Exploration_      | Switch exploration approach       | Specific to Q-Learning. Not observed in the faults we had.  |  \n| _Mutating Epsilon_      | Mutate \u03b5 value       | Specific to Q-Learning. Not observed in the faults we had.  |  \n| _Changing Noise_      | Change noise injected into the parameters of Q-function       | For DQN, this would be akin to fuzzing the weights. Something that was done in for instance DeepMutation [4] in supervised learning. Yet, this mutation seems not to be based on a real fault as DeepCrime's taxonomy does not mention it. Similarly, we did not observe such fault in RL. Not implemented.  |  \n| _Switching Buffer_      | Switch replay buffer mechanism       | We did not observed a fault where the switching off the replay buffer. The closest we have is implemented with the _No Reverse_ which affect the order in which rewards are discounted.  |  \n| _Fuzzing Buffer_      | Lose, repeat or modify experiences      | This mutation seems similar to some of their state level ones in their effect, expect it intervenes at the buffer level. Since it affects the experiences of the agents (i.e. tuple (State, action, reward)), our operators such as _Repeat_ or _Reward Noise_ act similarly. Nonetheless, we did not observe mutations that affected the buffer in that way (i.e. losing/modifying experiences in the buffer once they have been observed). |  \n| _Shuffling Replay Priority_      | Change the priority of replays       | A random version of our _No Reverse_. Is also covered partly in the _No Discount Factor_ which is a special case of their mutation (i.e. all replays have the same priority). We did not observe a _shuffling_ of the replay buffer. |  \n| _Precision Mutation_      | Mutate the precision of discretizing continuous spaces       | We did not observe such mutation. Moreover, in our experiments, we do not have environment with continuous spaces.  |  \n| _Approximation Mutation_      | Change approximation methods       | Same as above |  \n</details>\n \n",
                "original_header": "Mutation operators comparison with Lu et al. paper"
            },
            "confidence": 0.9767542328836811,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Hyper-parameters used to train the models are given in `settings.py` in the `HYPER_PARAMS` variable.\nThey are the same as the tuned ones presented in the [RL-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)\nrepositories. The only difference being that, since we could not use the vectorized option of \nstable baselines 3 as it conflicts with our multiprocess training/mutation injection, the number of vectorized environments\nfor `PPO` and `A2C` was set to 1 and we increased the number of total steps to compensate. We checked\nthat obtained value were inline were benchmark ones. The total steps for each environment/algorithm is\nas follow: \n",
                "original_header": "Training of agents"
            },
            "confidence": 0.936898201256165,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "`init_val` is a list of the initial value of the environment, `limits` are the search limits for the\nbinary search for each parameter given as `lower_param_1 upper_param_1 lower_param_2...`, `bounds` are\nthe test environments parameters if already computed (avoid the search). `test_mode` is the mutation test\nmethod being applied. Both are distribution based following DeepCrime's approach, only the distribution metric\nchanges with `r` for reward based (default) and `dtr` for inter/intra distance based (see RQ1 results). `number_of_cpus` is simply\nthe number of cpus used (by default, maximum available - 1). \n",
                "original_header": "Generating test environments"
            },
            "confidence": 0.9239170256342686,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "Mutation Testing for Deep Reinforcement Learning"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2301.05651"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/FlowSs/RLMutation/main/README.md"
        }
    ]
}