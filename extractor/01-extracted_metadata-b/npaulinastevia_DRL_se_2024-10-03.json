{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-03 20:45:17"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/npaulinastevia/drl_se",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "npaulinastevia",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2022-08-14T08:21:46Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-03-03T11:17:02Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "name": [
        {
            "result": {
                "value": "drl_se",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "npaulinastevia/drl_se",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/npaulinastevia/drl_se/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/npaulinastevia/drl_se/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 3,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 0,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/npaulinastevia/DRL_se/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 122843
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "requirements": [
        {
            "result": {
                "value": "Python 3.8+\n",
                "type": "Text_excerpt",
                "original_header": "Requirements",
                "parent_header": [
                    "A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "```\npip install -r requirements.txt\n```",
                "type": "Text_excerpt",
                "original_header": "Set up",
                "parent_header": [
                    "A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        }
    ],
    "run": [
        {
            "result": {
                "value": "\n\nEach directory contains the files we use to run the experiments.\n\nGame_testing_problem directory contains game_Keras-rl.py, game_Stable=baselines.py and game_Tensorforce.py to run experiments involving the game testing problem\n\nTest_case_prioritization_problem directory contains se_Keras.py, se_Tensorforce.py to run experiments involving the test case prioritization problem\n\nFor each file the options (run dqn or a2c) can be change directly on the file.\n",
                "type": "Text_excerpt",
                "original_header": "Run the experiments.",
                "parent_header": [
                    "A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        }
    ],
    "usage": [
        {
            "result": {
                "value": "```\npython game_Keras-rl.py\n```\n\nto collect bugs on the blockmaze game by using the DQN algorithm from keras-rl framework.\n",
                "type": "Text_excerpt",
                "original_header": "Example.",
                "parent_header": [
                    "A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks",
                    "Run the experiments."
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        }
    ],
    "faq": [
        {
            "result": {
                "value": "stable_baselines3.dqn.DQN(\n\"MlpPolicy\", env, learning_rate=0.00025, policy_kwargs = dict(net_arch=[256, 128, 128,4]), gamma=0.99, verbose=1, batch_size=128,\nbuffer_size=1000000, learning_starts=50000, tau=1.0,  train_freq=4, gradient_steps=1, \nreplay_buffer_class=None, replay_buffer_kwargs=None, optimize_memory_usage=False, target_update_interval=10000, \nexploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.05, max_grad_norm=10, \ntensorboard_log=None, policy_kwargs=None, seed=None, device='auto', _init_setup_model=True)\n\nstable_baselines3.a2c.A2C(\n\"MlpPolicy\", env, learning_rate=0.00025, n_steps=128, use_rms_prop=False, policy_kwargs=dict(net_arch=[256, 128, 128,4]), gamma=0.99, rms_prop_eps=1e-08\n n_steps=5,  gae_lambda=1.0, ent_coef=0.0, vf_coef=0.5, \nmax_grad_norm=0.5,  use_rms_prop=True, use_sde=False, sde_sample_freq=-1, normalize_advantage=False, \ntensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True)\n\nstable_baselines3.ppo.PPO(\n\"MlpPolicy\", env, learning_rate=0.00025, n_steps=128, policy_kwargs= dict(net_arch=[256, 128, 128,4]), gamma=0.99, verbose=1,\nbatch_size=128, n_epochs=10,  gae_lambda=0.95, \nclip_range=0.2, clip_range_vf=None, normalize_advantage=True, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, use_sde=False, \nsde_sample_freq=-1, target_kl=None, tensorboard_log=None, policy_kwargs=None, seed=None, device='auto', _init_setup_model=True)\n\ntensorforce.agents.DeepQNetwork(\nstates, actions, memory,network=[\n                      dict(type='flatten'),\n                      dict(type='dense', size=256, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='linear', size=4),\n                  ],\n                  discount=0.99, memory=10000, learning_rate=0.00025, batch_size=128,\n max_episode_timesteps=None,  update_frequency=0.25, \nstart_updating=None,  huber_loss=None, horizon=1,  reward_processing=None, return_processing=None, \npredict_terminal_values=False, target_update_weight=1.0, target_sync_frequency=1, state_preprocessing='linear_normalization', \nexploration=0.0, variable_noise=0.0, l2_regularization=0.0, entropy_regularization=0.0, config=None, saver=None, \nsummarizer=None, tracking=None, recorder=None, **kwargs)\n\ntensorforce.agents.AdvantageActorCritic(states, actions, \nparallel_interactions=4, network=[\n                      dict(type='flatten'),\n                      dict(type='dense', size=256, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='linear', size=4),\n                  ],\n                  discount=0.99, memory=10000, learning_rate=0.00025, batch_size=128\nmax_episode_timesteps=None,  use_beta_distribution=False, update_frequency=1.0,  horizon=1,  reward_processing=None, return_processing=None, \nadvantage_processing=None, predict_terminal_values=False, critic='auto', critic_optimizer=1.0, state_preprocessing='linear_normalization', \nexploration=0.0, variable_noise=0.0, l2_regularization=0.0, entropy_regularization=0.0, config=None, saver=None,\nsummarizer=None, tracking=None, recorder=None, **kwargs)\n\ntensorforce.agents.ProximalPolicyOptimization(states, actions, max_episode_timesteps,  \nparallel_interactions=4, network=[\n                      dict(type='flatten'),\n                      dict(type='dense', size=256, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='linear', size=4),\n                  ],\n                  discount=0.99, memory=10000, learning_rate=0.00025, batch_size=128,\n use_beta_distribution=False,\n update_frequency=1.0,  multi_step=10, subsampling_fraction=0.33, likelihood_ratio_clipping=0.25, \nreward_processing=None, return_processing=None, advantage_processing=None, predict_terminal_values=False, baseline=None, \nbaseline_optimizer=None, state_preprocessing='linear_normalization', exploration=0.0, variable_noise=0.0, l2_regularization=0.0, \nentropy_regularization=0.0,  config=None, saver=None, summarizer=None, tracking=None, recorder=None, **kwargs)\n\nrl.agents.dqn.DQNAgent(model==[\n                      dict(type='flatten'),\n                      dict(type='dense', size=256, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='linear', size=4),\n                  ], policy=LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n                                      value_max=1, value_min=0.05, value_test=.2, nb_steps=10000), \nmemory=SequentialMemory(limit=10000, window_length=1),\n                       gamma=.99, batch_size=128, enable_double_dqn=False\ntest_policy=None, enable_dueling_network=False, dueling_type='avg')\n",
                "type": "Text_excerpt",
                "original_header": "Game Testing problem",
                "parent_header": [
                    "A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks",
                    "Hyperparameters lists"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        },
        {
            "result": {
                "value": "stable_baselines.a2c.A2C(policy, env, gamma=0.99, n_steps=5, vf_coef=0.25, ent_coef=0.01, max_grad_norm=0.5, learning_rate=0.0007, \nalpha=0.99, momentum=0.0, epsilon=1e-05, lr_schedule='constant', verbose=0, tensorboard_log=None, _init_setup_model=True, \npolicy_kwargs=None, full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None)\n\nstable_baselines.ddpg.DDPG(policy, env, gamma=0.99, memory_policy=None, eval_env=None, nb_train_steps=50, nb_rollout_steps=100,\nnb_eval_steps=100, param_noise=None, action_noise=None, normalize_observations=False, tau=0.001, batch_size=128, \nparam_noise_adaption_interval=50, normalize_returns=False, enable_popart=False, observation_range=(-5.0, 5.0), critic_l2_reg=0.0, \nreturn_range=(-inf, inf), actor_lr=0.0001, critic_lr=0.001, clip_norm=None, reward_scale=1.0, render=False, render_eval=False, \nmemory_limit=None, buffer_size=50000, random_exploration=0.0, verbose=0, tensorboard_log=None, _init_setup_model=True,\npolicy_kwargs=None, full_tensorboard_log=False, seed=None, n_cpu_tf_sess=1)\n\nstable_baselines.deepq.DQN(policy, env, gamma=0.99, learning_rate=0.0005, buffer_size=50000, exploration_fraction=0.1, \nexploration_final_eps=0.02, exploration_initial_eps=1.0, train_freq=1, batch_size=32, double_q=True, learning_starts=1000, \ntarget_network_update_freq=500, prioritized_replay=False, prioritized_replay_alpha=0.6, prioritized_replay_beta0=0.4, \nprioritized_replay_beta_iters=None, prioritized_replay_eps=1e-06, param_noise=False, n_cpu_tf_sess=None, verbose=0, \ntensorboard_log=None, _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=None)\n\ntensorforce.agents.AdvantageActorCritic(states, actions, \nparallel_interactions=1, network=[\n                            dict(type='flatten'),\n                            dict(type='dense', size=64, activation='relu'),\n                            dict(type='dense', size=64, activation='relu'),\n                            dict(type='linear', size=2),\n                        ],\n                        discount=0.99, memory=10000, learning_rate=0.0007, batch_size=32\nmax_episode_timesteps=None,  use_beta_distribution=False, update_frequency=1.0,  horizon=1,  reward_processing=None, return_processing=None, \nadvantage_processing=None, predict_terminal_values=False, critic='auto', critic_optimizer=1.0, state_preprocessing='linear_normalization', \nexploration=0.0, variable_noise=0.0, l2_regularization=0.0, entropy_regularization=0.0, config=None, saver=None,\nsummarizer=None, tracking=None, recorder=None, **kwargs)\n\ntensorforce.agents.DeepQNetwork(\nstates, actions, memory,network=[\n                        dict(type='flatten'),\n                        dict(type='dense', size=64, activation='relu'),\n                        dict(type='dense', size=64, activation='relu'),\n                        dict(type='linear', size=2),\n                    ],\n                    discount=0.99, memory=10000, learning_rate=0.0005, batch_size=32\n max_episode_timesteps=None,  update_frequency=0.25, \nstart_updating=None,  huber_loss=None, horizon=1,  reward_processing=None, return_processing=None, \npredict_terminal_values=False, target_update_weight=1.0, target_sync_frequency=1, state_preprocessing='linear_normalization', \nexploration=0.0, variable_noise=0.0, l2_regularization=0.0, entropy_regularization=0.0, config=None, saver=None, \nsummarizer=None, tracking=None, recorder=None, **kwargs)\n\ntensorforce.agents.DeterministicPolicyGradient(states, actions, \nnetwork=[\n                        dict(type='flatten'),\n                        dict(type='dense', size=64, activation='relu'),\n                        dict(type='dense', size=64, activation='relu'),\n                        dict(type='linear', size=1),\n                    ],\n                    discount=0.99, memory=10000, learning_rate=0.0005, batch_size=32\nmax_episode_timesteps=None,  \nuse_beta_distribution=True, update_frequency=1.0, start_updating=None,  horizon=1, \nreward_processing=None, return_processing=None, predict_terminal_values=False, critic='auto', critic_optimizer=1.0, \nstate_preprocessing='linear_normalization', exploration=0.1, variable_noise=0.0, l2_regularization=0.0, entropy_regularization=0.0, \nparallel_interactions=1, config=None, saver=None, summarizer=None, tracking=None, recorder=None, **kwargs)\n\nrl.agents.dqn.DQNAgent(model==[\n                      dict(type='flatten'),\n                      dict(type='dense', size=256, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='dense', size=128, activation='relu'),\n                      dict(type='linear', size=4),\n                  ], policy=LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_test=.05,value_min=0.02), \nmemory=SequentialMemory(limit=10000, window_length=1),\n                       gamma=.99, batch_size=32, enable_double_dqn=False\ntest_policy=None, enable_dueling_network=False, dueling_type='avg')\n\nactor = Sequential()\nactor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nactor.add(Input(shape=env.observation_space.shape))\nactor.add(Dense(64))\nactor.add(Activation('relu'))\nactor.add(Dense(64))\nactor.add(Activation('relu'))\nactor.add(Dense(number_actions))\nactor.add(Activation('sigmoid'))\naction_input = Input(shape=(nb_actions,), name='action_input')\nobservation_input =  Input(shape=(1,) + env.observation_space.shape, name='observation_input')\nflattened_observation = Flatten()(observation_input)\nx = Concatenate()([action_input, flattened_observation])\nx = Dense(64)(x)\nx = Activation('relu')(x)\nx = Dense(64)(x)\nx = Activation('relu')(x)\nx = Dense(1)(x)\nx = Activation('linear')(x)\ncritic = Model(inputs=[action_input, observation_input], outputs=x)\n\nrl.agents.ddpg.DDPGAgent( memory=SequentialMemory(limit=10000, window_length=1),gamma=0.99,  actor=actor, critic=critic,\n                             critic_action_input=action_input,nb_steps_warmup_critic=100, nb_steps_warmup_actor=100)\n",
                "type": "Text_excerpt",
                "original_header": "Test Case prioritization problem",
                "parent_header": [
                    "A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks",
                    "Hyperparameters lists"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        }
    ],
    "application_domain": [
        {
            "result": {
                "type": "String",
                "value": "Reinforcement Learning"
            },
            "confidence": 0.9416666666666665,
            "technique": "supervised_classification"
        }
    ],
    "description": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This repository contains the training scripts, datasets, and experiments corresponding to our paper [here](https://arxiv.org/abs/2208.12136)\n \n",
                "original_header": "A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks"
            },
            "confidence": 0.9158642524957642,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2208.12136"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/npaulinastevia/DRL_se/master/README.md"
        }
    ]
}