{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-03 21:03:09"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/FlowSs/GIST",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "FlowSs",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2023-10-27T20:32:13Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2023-10-31T14:52:55Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "name": [
        {
            "result": {
                "value": "GIST",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "FlowSs/GIST",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/FlowSs/GIST/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/FlowSs/GIST/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 0,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 0,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/FlowSs/GIST/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 117819
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "executable_example": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/FlowSs/GIST/main/demo/demo.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/demo/demo.ipynb"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "This replication package contains the artifact associated with the paper \"GIST: Generated Inputs Sets Transferability in Deep Learning\" in order to help in replicating the results. This paper was submitted to the *ACM Transactions on Software Engineering and Methodology* journal.\n\n**A preprint version of the paper is available on [arxiv](https://arxiv.org/abs/2311.00801)**\n",
                "type": "Text_excerpt",
                "original_header": "Replication package for \"GIST: Generated Inputs Sets Transferability in Deep Learning\""
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        },
        {
            "result": {
                "value": "In order to answer our research questions, we first had to follow the steps detailed in our methodology, i.e. generate for each model the test sets for each procedure, extract prediction/features and cluster for each model under test/procedure. Note that if you download all the required files (see *A note on replication*), you will not need to do that.\n",
                "type": "Text_excerpt",
                "original_header": "Setup"
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        },
        {
            "result": {
                "value": "Given the models and generated test sets, one can use the script `generate_pred_sets.py` in order to obtain the features needed. Note that those obtained feature files are part of what is downloaded in the [**pred_sets**](https://zenodo.org/records/10028594) repository on Zenodo.\n\nUsage of the script is:\n\n```\nusage: generate_pred_sets.py [-h] [-m MODEL [MODEL ...]] [-d DATASET] [-t TYPE] [-tm TARGET_MODEL] [-b BATCH_SIZE] [--device DEVICE] [--override]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL [MODEL ...], --model MODEL [MODEL ...]\n                        Model to use for computation. By default, will use ALL registered models for the given dataset. If given, will use only the provided\n                        list of models (e.g. --model preresnet20 vgg19). All instances (i.e. seed) trained are used, e.g. --model vgg19 will use vgg19_0,\n                        vgg19_1, ...\n  -d DATASET, --dataset DATASET\n                        Dataset to use for computation\n  -t TYPE, --type TYPE  Test set type (train, fuzz, gen)\n  -tm TARGET_MODEL, --target_model TARGET_MODEL\n                        Model to be tested on a generated dataset of all models provided with --model argument, e.g --target_model vgg19_0, will test\n                        generated dataset from all instances of all model seeds (except vgg19 ones) on vgg19_0.\n  -b BATCH_SIZE, --batch_size BATCH_SIZE\n  --device DEVICE\n  --override\n```\n\nBasic usage is:\n\n```\npython generate_pred_sets.py -d cifar10 -t fuzz -tm preresnet110_0 -b 128 --device cuda\n```\n\nThis will extract the features files from the model *PreResnet110* (seed 0) (i.e. the target) by using all other models (except *PreResnet110*) generated fuzz test sets and save it in a numpy array name `preresnet110_0_fuzz_X.npz` inside the `pred_sets/cifar10/` directory, where X is the name of model from which we used the test set (e.g. `preresnet20_0`). The batch size is set to 128 and it uses GPU acceleration. This must be done on all models, both being or not being a target. This will allow us to experimentally verify that a given metric M works for all models, by considering them alternately to be the model under test. Note that it will skip the extraction if the file already exists unless the argument `--override` is provided.\n",
                "type": "Text_excerpt",
                "original_header": "Extracting features/prediction",
                "parent_header": [
                    "Setup"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Models are provided (see previous parts). If you wish to retrain them from scratch the models, we used two libraries: \n",
                "original_header": "Training the models"
            },
            "confidence": 0.9504425548242745,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        }
    ],
    "usage": [
        {
            "result": {
                "value": "A notebook for demonstration of the method is available in the repository `demo/` (`demo.ipynb`). It shows GIST working with a simple example and can also serve as a basic skeleton for customization for other properties or proxies. \n\nThe demo only requires Python, basic packages scientific packages (numpy, matplotlib and scipy) and the data used which can be downloaded on [Zenodo](https://zenodo.org/records/10028594) along with this repository.\n",
                "type": "Text_excerpt",
                "original_header": "Demo notebook"
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        }
    ],
    "requirements": [
        {
            "result": {
                "value": "Requires Python >= 3.8\n\n<details>\n<summary>List of requirements (click to open/collapse)</summary>\n\nname: gist\n\nchannels:\n  - pytorch\n  - defaults\n\ndependencies:\n  - _libgcc_mutex=0.1=main\n  - _openmp_mutex=5.1=1_gnu\n  - abseil-cpp=20211102.0=hd4dd3e8_0\n  - aiohttp=3.8.3=py38h5eee18b_0\n  - aiosignal=1.2.0=pyhd3eb1b0_0\n  - arrow-cpp=11.0.0=py38h613000e_0\n  - async-timeout=4.0.2=py38h06a4308_0\n  - attrs=22.1.0=py38h06a4308_0\n  - aws-c-common=0.4.57=he6710b0_1\n  - aws-c-event-stream=0.1.6=h2531618_5\n  - aws-checksums=0.1.9=he6710b0_0\n  - aws-sdk-cpp=1.8.185=hce553d0_0\n  - blas=1.0=mkl\n  - boost-cpp=1.73.0=h7f8727e_12\n  - bottleneck=1.3.5=py38h7deecbd_0\n  - brotlipy=0.7.0=py38h27cfd23_1003\n  - bzip2=1.0.8=h7b6447c_0\n  - c-ares=1.19.0=h5eee18b_0\n  - ca-certificates=2023.01.10=h06a4308_0\n  - certifi=2023.5.7=py38h06a4308_0\n  - cffi=1.15.1=py38h5eee18b_3\n  - charset-normalizer=2.0.4=pyhd3eb1b0_0\n  - cryptography=39.0.1=py38h9ce1e76_0\n  - cudatoolkit=11.3.1=h2bc3f7f_2\n  - ffmpeg=4.3=hf484d3e_0\n  - filelock=3.9.0=py38h06a4308_0\n  - freetype=2.12.1=h4a9f257_0\n  - frozenlist=1.3.3=py38h5eee18b_0\n  - fsspec=2023.4.0=py38h06a4308_0\n  - gflags=2.2.2=he6710b0_0\n  - giflib=5.2.1=h5eee18b_3\n  - glog=0.5.0=h2531618_0\n  - gmp=6.2.1=h295c915_3\n  - gnutls=3.6.15=he1e5248_0\n  - grpc-cpp=1.46.1=h33aed49_1\n  - icu=58.2=he6710b0_3\n  - idna=3.4=py38h06a4308_0\n  - intel-openmp=2023.1.0=hdb19cb5_46305\n  - jpeg=9e=h5eee18b_1\n  - krb5=1.19.4=h568e23c_0\n  - lame=3.100=h7b6447c_0\n  - lcms2=2.12=h3be6417_0\n  - ld_impl_linux-64=2.38=h1181459_1\n  - lerc=3.0=h295c915_0\n  - libboost=1.73.0=h28710b8_12\n  - libbrotlicommon=1.0.9=h5eee18b_7\n  - libbrotlidec=1.0.9=h5eee18b_7\n  - libbrotlienc=1.0.9=h5eee18b_7\n  - libcurl=7.88.1=h91b91d3_0\n  - libdeflate=1.17=h5eee18b_0\n  - libedit=3.1.20221030=h5eee18b_0\n  - libev=4.33=h7f8727e_1\n  - libevent=2.1.12=h8f2d780_0\n  - libffi=3.4.4=h6a678d5_0\n  - libgcc-ng=11.2.0=h1234567_1\n  - libgomp=11.2.0=h1234567_1\n  - libiconv=1.16=h7f8727e_2\n  - libidn2=2.3.2=h7f8727e_0\n  - libnghttp2=1.46.0=hce63b2e_0\n  - libpng=1.6.39=h5eee18b_0\n  - libprotobuf=3.20.3=he621ea3_0\n  - libssh2=1.10.0=h8f2d780_0\n  - libstdcxx-ng=11.2.0=h1234567_1\n  - libtasn1=4.19.0=h5eee18b_0\n  - libthrift=0.15.0=hcc01f38_0\n  - libtiff=4.5.0=h6a678d5_2\n  - libunistring=0.9.10=h27cfd23_0\n  - libwebp=1.2.4=h11a3e52_1\n  - libwebp-base=1.2.4=h5eee18b_1\n  - lz4-c=1.9.4=h6a678d5_0\n  - mkl=2023.1.0=h6d00ec8_46342\n  - mkl-service=2.4.0=py38h5eee18b_1\n  - mkl_fft=1.3.6=py38h417a72b_1\n  - mkl_random=1.2.2=py38h417a72b_1\n  - multidict=6.0.2=py38h5eee18b_0\n  - ncurses=6.4=h6a678d5_0\n  - nettle=3.7.3=hbbd107a_1\n  - numexpr=2.8.4=py38hc78ab66_1\n  - openh264=2.1.1=h4ff587b_0\n  - openssl=1.1.1t=h7f8727e_0\n  - orc=1.7.4=hb3bc3d3_1\n  - packaging=23.0=py38h06a4308_0\n  - pandas=1.5.3=py38h417a72b_0\n  - pillow=9.4.0=py38h6a678d5_0\n  - pip=23.0.1=py38h06a4308_0\n  - pyarrow=11.0.0=py38h992f0b0_0\n  - pycparser=2.21=pyhd3eb1b0_0\n  - pyopenssl=23.0.0=py38h06a4308_0\n  - pysocks=1.7.1=py38h06a4308_0\n  - python=3.8.16=h7a1cb2a_3\n  - python-dateutil=2.8.2=pyhd3eb1b0_0\n  - python-xxhash=2.0.2=py38h5eee18b_1\n  - pytorch=1.12.1=py3.8_cuda11.3_cudnn8.3.2_0\n  - pytorch-mutex=1.0=cuda\n  - pytz=2022.7=py38h06a4308_0\n  - pyyaml=6.0=py38h5eee18b_1\n  - re2=2022.04.01=h295c915_0\n  - readline=8.2=h5eee18b_0\n  - regex=2022.7.9=py38h5eee18b_0\n  - requests=2.29.0=py38h06a4308_0\n  - responses=0.13.3=pyhd3eb1b0_0\n  - setuptools=66.0.0=py38h06a4308_0\n  - six=1.16.0=pyhd3eb1b0_1\n  - snappy=1.1.9=h295c915_0\n  - sqlite=3.41.2=h5eee18b_0\n  - tbb=2021.8.0=hdb19cb5_0\n  - tk=8.6.12=h1ccaba5_0\n  - tokenizers=0.11.4=py38h3dcd8bd_1\n  - torchaudio=0.12.1=py38_cu113\n  - torchvision=0.13.1=py38_cu113\n  - tqdm=4.65.0=py38hb070fc8_0\n  - typing-extensions=4.5.0=py38h06a4308_0\n  - typing_extensions=4.5.0=py38h06a4308_0\n  - urllib3=1.26.15=py38h06a4308_0\n  - utf8proc=2.6.1=h27cfd23_0\n  - wheel=0.38.4=py38h06a4308_0\n  - xxhash=0.8.0=h7f8727e_3\n  - xz=5.4.2=h5eee18b_0\n  - yaml=0.2.5=h7b6447c_0\n  - yarl=1.8.1=py38h5eee18b_0\n  - zlib=1.2.13=h5eee18b_0\n  - zstd=1.5.5=hc292b87_0\n  - pip:\n      - about-time==4.2.1\n      - absl-py==1.4.0\n      - accelerate==0.21.0\n      - alive-progress==3.1.4\n      - anytree==2.9.0\n      - autograd==1.6.2\n      - beautifulsoup4==4.12.2\n      - bert-score==0.3.13\n      - boto3==1.28.28\n      - botocore==1.31.28\n      - bpemb==0.3.4\n      - cachetools==5.3.1\n      - click==8.0.4\n      - cloudpickle==2.2.1\n      - cma==3.2.2\n      - conllu==4.5.3\n      - contourpy==1.0.7\n      - cupy-cuda113==10.6.0\n      - cycler==0.11.0\n      - cython==0.29.35\n      - datasets==2.4.0\n      - deprecated==1.2.14\n      - dill==0.3.5.1\n      - docopt==0.6.2\n      - easydict==1.10\n      - editdistance==0.6.2\n      - fastrlock==0.8.1\n      - flair==0.12.2\n      - fonttools==4.39.4\n      - ftfy==6.1.1\n      - future==0.18.3\n      - gdown==4.4.0\n      - gensim==4.3.1\n      - google-auth==2.21.0\n      - google-auth-oauthlib==1.0.0\n      - grapheme==0.6.0\n      - grpcio==1.56.0\n      - hdbscan==0.8.29\n      - huggingface-hub==0.16.4\n      - hyperopt==0.2.7\n      - imageio==2.31.2\n      - importlib-metadata==6.6.0\n      - importlib-resources==5.12.0\n      - janome==0.5.0\n      - jieba==0.42.1\n      - jmespath==1.0.1\n      - joblib==1.2.0\n      - kiwisolver==1.4.4\n      - langdetect==1.0.9\n      - language-tool-python==2.7.1\n      - lazy-loader==0.3\n      - lemminflect==0.2.3\n      - llvmlite==0.40.0\n      - lru-dict==1.2.0\n      - lxml==4.9.3\n      - markdown==3.4.3\n      - markupsafe==2.1.3\n      - matplotlib==3.7.1\n      - more-itertools==10.1.0\n      - mpld3==0.3\n      - multiprocess==0.70.13\n      - networkx==3.1\n      - nltk==3.8.1\n      - num2words==0.5.12\n      - numba==0.57.0\n      - numpy==1.24.4\n      - oauthlib==3.2.2\n      - openhownet==2.0\n      - pinyin==0.4.0\n      - pptree==3.1\n      - protobuf==4.23.3\n      - psutil==5.9.5\n      - py4j==0.10.9.7\n      - pyasn1==0.5.0\n      - pyasn1-modules==0.3.0\n      - pycld2==0.41\n      - pymoo==0.6.0.1\n      - pynndescent==0.5.10\n      - pynvml==11.5.0\n      - pyparsing==3.0.9\n      - pytorch-revgrad==0.2.0\n      - pywavelets==1.4.1\n      - requests-oauthlib==1.3.1\n      - rsa==4.9\n      - s3transfer==0.6.2\n      - safetensors==0.3.2\n      - scikit-image==0.21.0\n      - scikit-learn==1.2.2\n      - scipy==1.10.1\n      - seaborn==0.12.2\n      - segtok==1.5.11\n      - sentencepiece==0.1.99\n      - smart-open==6.3.0\n      - soupsieve==2.4.1\n      - sqlitedict==2.1.0\n      - tabulate==0.9.0\n      - tensorboard==2.13.0\n      - tensorboard-data-server==0.7.1\n      - terminaltables==3.1.10\n      - textattack==0.3.8\n      - threadpoolctl==3.1.0\n      - tifffile==2023.7.10\n      - torchtext==0.13.1\n      - transformer-smaller-training-vocab==0.2.4\n      - transformers==4.31.0\n      - umap-learn==0.5.3\n      - wcwidth==0.2.6\n      - werkzeug==2.3.6\n      - wikipedia-api==0.6.0\n      - word2number==1.1\n      - wrapt==1.15.0\n      - zipp==3.15.0\n</details>\n",
                "type": "Text_excerpt",
                "original_header": "Requirements"
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        }
    ],
    "description": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "* For CIFAR10: We used [CIFAR-ZOO](https://github.com/BIGBALLON/CIFAR-ZOO) implementations of the models. We kept the same hyperparameters as they did with the exception of the number of epochs (300 -> 50) and the epochs to change the learning rate ([150, 225] -> [20, 40]) in order to fasten computation.  \n",
                "original_header": "Training the models"
            },
            "confidence": 0.9132924965223493,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "* Text: We used the [TextAttack](https://github.com/QData/TextAttack) library and implemented a Custom Attack following the description presented in [Reevaluating Adversarial Examples in Natural Language](https://aclanthology.org/2020.findings-emnlp.341/). \n**DISCLAIMER FOR THE SCRAPPED DATA: ** Data were scrapped from publically available reviews of rotten tomatoes for research purposes only. Data were taken as they were on the website and no further modifications were applied, besides removing some data based on a review score threshold as described in the paper. As such, no moderation filtering on the review was done on top of the existing one from rotten tomatoes. Use at your own discretion.\n \n",
                "original_header": "Procedures used"
            },
            "confidence": 0.9223105574088861,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This will calculate the clusters using *PreResnet110* (seed 0) as the model under test, pooling all the extracted features of the fuzz procedures that were computed previously. The results will be saved as a `.json` file in the corresponding `pred_sets/` directory and start as `cov_type2_...`. The option `--save_mod` allows to saving of the UMAP and HBSCAN models if needed and `--save_clust` allows saving the data themselves in the clusters which are needed to evaluate the accuracy over clusters (see *Clusters Validations* below), saving the data in the `data/` directory (name starting with `data_cov_X.npz`). The hyperparameters used are present in this script as well. Readers can find in the `results_files/` directory the obtained results (DBCV, silhouette score...) we obtained in our experiments for each of the procedures.\n \n",
                "original_header": "Clustering"
            },
            "confidence": 0.9603837617038009,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "In our paper, we first applied the clustering method and verified that it corresponds to different fault types similar to [Black-Box Testing of Deep Neural Networks through Test Case Diversity](https://ieeexplore.ieee.org/abstract/document/10041782).  \nOnce the splits are generated, the model needs to be retrained using each of those splits (and for each of the clusters we wish to evaluate accuracy). We provided them in the [**models**](https://zenodo.org/records/10028594) repository on Zenodo. \nFinally, using the script `calculate_clusters_accuracy.py`, one can obtain the in/out cluster's accuracy leading to the results we presented in Table 2 in our paper. For instance:\n```\npython calculate_clusters_accuracy.py -d cifar10 -t fuzz -o densenet100bc -i 1 --cluster 31 --device cuda\n```\nwill return\n```\nCalculating...\nWorking on split 0\nFuzz dataset size for model densenet100bc_1 is 10244 \nWorking on split 1\nFuzz dataset size for model densenet100bc_1 is 10244 \nWorking on split 2\nFuzz dataset size for model densenet100bc_1 is 10244 \nUsing cluster 31, average over 3 splits\nAveraged pct of faults reduced (non-covered cluster) in dat aug:  0.7407407407407408\nAveraged pct of faults reduced (non-covered cluster) not in dat aug:  0.1029253710953912\n#####################################\n```\n \n",
                "original_header": "Clusters Validations"
            },
            "confidence": 0.9375221841896861,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Each `res_PROCEDURE.csv` is structured as follows: each row is the result for a model under test seed for a given similarity metric. It is structured in blocks of 6 rows (one for each metric), e.g. the first 6 rows are the results for the 6 metrics (PWCCA, CKA, Ortho, Acc, Dis and JDiv) for Densenet100bc seed 0, then the 6 next are for the 6 metrics for Densenet100bc seed 1, ...etc Available data are the obtained Kendall-tau, the associated p-value, the Top-1 and Top-5 criterion as well as the fault type coverage value obtained for the Top-1 and Top-5 criteria. \n",
                "original_header": "RQ2: Can the introduced similarities be used as a proxy for the defined properties?"
            },
            "confidence": 0.9516959498480665,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": ""
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        }
    ],
    "images": [
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/FlowSs/GIST/main/image.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2311.00801"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/FlowSs/GIST/main/README.md"
        }
    ]
}