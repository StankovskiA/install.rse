{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-03 19:00:52"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/emsejournal/openscience",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "emsejournal",
                "type": "Organization"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2018-06-28T15:35:56Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-09-09T19:14:12Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "description": [
        {
            "result": {
                "value": " Empirical Software Engineering journal (EMSE) open science and reproducible research initiative",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\nOpenness in science is key to fostering progress via transparency, reproducibility, and replicability. Especially open data and open source are two fundamental pillars in open science as both build the core for excellence in evidence-based research. The Empirical Software Engineering journal (EMSE) has therefore decided to explicitly foster open science and reproducible research by encouraging and supporting authors to share their (anonymised and curated) empirical data and source code in form of replication packages. The overall goals are:\n* Increasing the transparency, reproducibility, and replicability of research endeavours. This supports the immediate credibility of authors' work, and it also provides a common basis for joint community efforts grounded on shared data.\n* Building up an overall body of knowledge in the community leading to widely accepted and well-formed software engineering theories in the long run. \n",
                "original_header": "EMSE Open Science Initiative"
            },
            "confidence": 0.9916169096729118,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/emsejournal/openscience/master/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "As for any initiative in a research community, the success of the Open Science Initiative, too, depends on the willingness and the possibilities of authors to disclose their data. Therefore, we strive to implement the Open Science Initiative at EMSE as a community effort with services that aim at encouraging and supporting authors of EMSE articles in opening up their research. The steering and motivating principle is that only openness in empirical research increases the transparency of research in a way such that the authors' empirical analyses can be reproduced, fully understood, and ideally replicated by others not involved in the research. To this end, we aim at promoting a data-sharing culture where authors publicly archive their data and related material required to understand and reproduce the claims and analyses presented by them in their manuscripts. Our hope is to move our community as a whole forward to the point where open science becomes the norm. \nAll submissions to EMSE will undergo the same known review process regardless of whether authors decide to disclose their data or not. Yet, as the leading journal in empirical research methodologies and their application to software engineering, we strongly encourage all authors to make an effort in supporting this initiative by making data available upon submission (either privately or publicly) and especially upon acceptance (publicly). Authors who cannot disclose non-public data (e.g. industrial data sets that fall under non-disclosure agreements), are asked to please provide an explicit and short statement in their manuscript. \nTo make research data sets and research software accessible and citable, we encourage authors to:\n* archive data on preserved archives such as [zenodo.org](https://zenodo.org/) and [figshare.com](https://figshare.com/) so that replication packages remain available in the very long term (on Zenodo, there is a [dedicated community for empirical software engineering](https://zenodo.org/communities/empirical-software-engineering/)).\n* use an appropriate license, e.g., the [CC-BY 4.0 license](https://creativecommons.org/licenses/by/4.0/) for data and the [MIT License](https://choosealicense.com/licenses/mit/) for code. Look at [choosealicense.com](https://choosealicense.com/) for more information about suitable open source licenses. \nAuthors should therefore use archival repositories and avoid putting data and software on their own (institutional or private) websites or systems like Dropbox, version control systems (SVN, Git), or service like Academia.edu and ResearchGate. Personal websites are prone to changes and errors, and more than 30% of them will not work in a 4 year period. Moreover, nobody should have the ability to delete data once it is public. Finally, the package disclosed via an archival repository should link to the paper (DOI) upon final production of the manuscript.\n \n",
                "original_header": "Open Science Principles at EMSE"
            },
            "confidence": 0.9571738319108601,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/emsejournal/openscience/master/README.md"
        }
    ],
    "name": [
        {
            "result": {
                "value": "openscience",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "emsejournal/openscience",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/emsejournal/openscience/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/emsejournal/openscience/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 33,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "open-science, reproducible-experiments, reproducible-research, reproducible-science",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 11,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/emsejournal/openscience/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/emsejournal/openscience/master/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "type": [
        {
            "result": {
                "value": "non-software",
                "type": "String"
            },
            "confidence": 1,
            "technique": "software_type_heuristics"
        }
    ],
    "faq": [
        {
            "result": {
                "value": "**How should the replication packages be disclosed?**  \nWe encourage authors to archive their data as part of replication packages on preserved archives such as [zenodo.org](https://zenodo.org/) or [figshare.com](https://figshare.com/) so that the data will receive a [DOI](https://www.doi.org/) and become citable. Further, we recommend the authors to use the [CC0](https://creativecommons.org/publicdomain/zero/1.0/) dedication (or the [CC-BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)) when publishing the data (automatic when using, for instance, zenodo.org or figshare.com).\n\nThose archives allow updating published replication packages any time. We strongly recommend the authors to update the package information after the review process, once the manuscript is in production and receives a DOI, with a reference to the published manuscript so that the package is citable along the published article.\n\n**In this EMSE open science process, what\u2019s the difference between reproducibility and replicability?**  \nThere is no consensus across disciplines about the difference between reproducibility and replicability. Often, replicability is seen as the ability to repeat the same study under the very same conditions yielding same results.\nReproducibility is seen as the ability to independently reproduce the study yielding same or similar results with a given precision. In the EMSE open science process, we make no specific difference for now. The goal is to encourage open data and code so that researchers can reproduce the results (partially or completely), and/or perform further research using this data and code.\n\n**What happens if the data violates one or more of the FAIR principles?**  \n[FAIR](https://www.force11.org/group/fairgroup/fairprinciples) is an interesting initiative which we follow attentively. It\u2019s good if authors get to know, and ideally follow those principles, but it\u2019s not required.\n\n**Is restricted access sharing allowed (e.g., data is stored online but the link is not public and provided only if the interested party explicitly asks for it or signs a formal agreement with the data owner)?**  \nWe certainly understand that in certain contexts, unconditional data disclosure to the public is not possible. Open science, however, means that data is publicly accessible by anyone which is why the open science badges cannot be granted when data is shared in a restricted manner.\n\n**When it comes to data about the humans, do we want to encourage or adhere to some kind of privacy regulations such as GDPR?**  \nPrivacy is a very important concern taken seriously by the open science board. When it is required for the data, proper consent and anonymisation of data is mandatory, in compliance with existing ethical codes of conduct and regulations such as [GDPR](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation). Note that besides regional/local regulations and policies, also considering potential institutional entities such as an [IRB](https://en.wikipedia.org/wiki/Institutional_review_board) and necessary approvals of a study by such entities, sharing data about humans (e.g. interview or survey data) must always be based on the explicit consent given by the participants. That is, anonymizing the resulting data sets is not enough to meet our ethical standards. Further note that such consents may also be withdrawn by participants along a study (e.g., after presenting the results). While the open science board is advised to check for proper anomyzation when deemed necessary, it remains the sole responsibility of the authors to put careful considerations into privacy-related and ethical concerns, to obtain the necessary approvals and consents prior to submission of their data sets, and to ensure compliance of their actions with existing regulations.\n\n\n**What happens if data or access to it are modified by the authors after the approval of the Open Science chairs?**\nWe count on the  authors\u2019 ethics to avoid this inappropriate behaviour. Note that once data has been released on an archival website (e.g. Zenodo), later modifications are not possible anymore. This is also one of the reasons why we refrain from disclosing data sets only on institutional websites or personal webpages.\n\n**What does replication mean for qualitative studies, say ethnography research or action research?**   \nComplete replication of qualitative human studies is challenging, as human practices are rarely purely rational and reproducible. In our understanding, however, data sharing can at least support comprehending the analysis results yielded by the researchers and the conclusions they have drawn based on their data. We consider, therefore, a qualitative study to be (sufficiently) reproducible when the shared data allows other researchers to understand the claims and analyses presented by the authors. For interview research, for example, the shared data should include the instrumentation, transcripts (potentially anonymised), field notes, and codebooks so that others not involved in the study fully understand how the authors inferred their conclusions.\n\n**What does replication mean for systematic reviews?**  \nSimilarly as for qualitative studies, secondary studies are difficult to fully reproduce. There exist multiple reasons for this, such as the different functionalities of different search engines yielding different results when repeatedly used in an independent manner. We consider a secondary study to be sufficiently reproducible when the reporting in the manuscript and the shared data and scripts allow to understand the claims and analyses presented by the authors.\n\n**Why is the open science review process single blind?**  \nIt is for now single blind in order to be the least disruptive and to maximise acceptance (both from authors and from reviewers). We will consider alternatives in the near future.\n\n**Will the absence of badges on research papers, as a result of confidential research data/artefacts, give the impression of a lower class science?**\nNo, the badge signals open data and open science. The absence of badge states does not mean that this is lower-class science, it only means that the authors cannot participate to the process or that the data was not open enough. In the presence of confidential research data or artefacts, authors will be encouraged to state in their paper the reasons for confidentiality in order to prevent misleading impressions.\n\n**Will the absence of badges discourage research done on industrial proprietary datasets, that is potentially very valuable?**  \nThe journal does encourage and support high-quality research based on proprietary datasets which provides novel and unique insights.\n\n**Are there multiple badges of different flavours? What is the connection to the [ACM badging](https://www.acm.org/publications/policies/artifact-review-badging) (used by [ROSE](https://2018.fseconference.org/track/rosefest-2018))?**  \nFirst, there will be a single badge \"Open science\" in order to keep things simple. Then, multiple badges (up to the 5 levels of ACM) may be introduced. The badges are also being discussed with the publisher (Springer).\n\n**Do artifacts need to be executable? What about models or diagrams?** \nMany areas of software engineering do not generate executable artifacts. Non-code artifacts such as UML diagrams/models, requirements text, design documents, etc. are all valid artifacts. If the artifact addresses the topic of the paper and supports replication, that's fine. See here for a lengthier, but not complete, [list of artifacts](https://github.com/researchart/all/blob/master/ListOfArtifacts.md). Finally, we prefer if such artifacts are machine-readable and open, e.g., using an open format such as JSON or XMI, as opposed to Visio, PNG, or PDF.\n",
                "type": "Text_excerpt",
                "original_header": "FAQ",
                "parent_header": [
                    "EMSE Open Science Initiative"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/emsejournal/openscience/master/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "EMSE Open Science Initiative"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/emsejournal/openscience/master/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/1904.06499"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/emsejournal/openscience/master/README.md"
        }
    ]
}