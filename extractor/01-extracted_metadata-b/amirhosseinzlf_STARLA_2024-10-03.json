{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-03 20:05:21"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/amirhosseinzlf/STARLA",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "amirhosseinzlf",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2022-04-08T15:19:57Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-08-30T11:18:41Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "license": [
        {
            "result": {
                "value": "https://api.github.com/licenses/mit",
                "type": "License",
                "name": "MIT License",
                "url": "https://api.github.com/licenses/mit",
                "spdx_id": "MIT"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "MIT License\n\nCopyright (c) 2022 amirhosseinzlf\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
                "type": "File_dump"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/LICENSE"
        }
    ],
    "description": [
        {
            "result": {
                "value": "Search-based Testing Approach of Reinforcement Learning Agent",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "In this project, we propose a Search-based Testing Approach for Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent. This approach effectively searches for failing executions of the agent where we have a limited testing budget. To achieve this, we rely on machine learning models to guide our search and we use a dedicated genetic algorithm to narrow the search toward faulty episodes. These episodes are sequences of states and actions produced by the DRL agent. We apply STARLA on a DQN agent trained in a Cartpole environment for 50K time steps.\n\n\n",
                "type": "Text_excerpt",
                "original_header": "Introduction",
                "parent_header": [
                    "STARLA: Search-Based Testing Approach for Deep Reinforcement Learning Agents"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "STARLA Requires a DRL agent and its training data as input as tries to effectively and efficiently generate failing test cases (episodes) to reveal the faults of agents policy. Detailed approach is depicted in the following diagram:\n\n\n![Approach4_page-0001](https://user-images.githubusercontent.com/23516995/168500802-50486e30-2c5d-43c2-a080-9cc01d964e30.jpg)\n\n\nAs depicted, the main objective of STARLA is to generate and find episodes with high fault probabilities in order to assess whether an RL agent can be safely deployed. \nThe algorithm uses the data from the Agent to build ML models that predict the probabilities of fault (to which extent episodes are similar to faulty episodes). The outputs of these models are combined with the reward of the agent and certainty level. They are meant to guide the Genetic search toward faulty episodes. \n\nIn the Genetic search, we use specific crossover and mutation functions. Also as we have multiple fitness functions, we are using MOSA Algorithm[3]. For more explanations please see our paper. [arXiv:2206.07813](https://arxiv.org/abs/2206.07813)\n\n",
                "type": "Text_excerpt",
                "original_header": "Description of the Approach",
                "parent_header": [
                    "STARLA: Search-Based Testing Approach for Deep Reinforcement Learning Agents"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This repository is a companion page for the following paper \n> \"Search-Based Testing Approach for Deep Reinforcement Learning Agents\". \n",
                "original_header": "Publication"
            },
            "confidence": 0.9567188040220702,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "*In this research question we investigate the accuracy of ML classifiers in predicting faulty episodes of the RL agent.* \nWe use Random Forest to predict the probabilities of reward and functional faults in a given episode.\nTo build our training dataset, we sampled episodes from both episodes generated through random executions of the agent and episodes from the training phase of the agent. Episodes are encoded based on the presence or absence of their abstract states. We have two different ML models, one for predicting the probability of a reward fault and the other one for predicting the probability of a functional fault. We considered 70 % of data for training and 30% for testing. \n",
                "original_header": "RQ2: Can we rely on ML models to predict faulty episodes?"
            },
            "confidence": 0.9857124221423335,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "For this reason, we need to rely on an interpretable ML model, in this case, a Decision Tree model, to learn such rules.\nWe assess the accuracy of decision trees and therefore our ability to learn accurate rules based on the faulty episodes that we identify with STARLA. \nIn practice, engineers will need to use such an approach to assess the safety of using an RL agent and understand the reasons of faults.\nIn this part, we assess the accuracy of trained models that extract the rules of functional and reward faults based on k-fold cross-validation. \n\nSuch highly accurate rules can help developers understand the conditions under which the agent fails. One can analyze, the concrete states that correspond to abstract states leading to faults to extract real-world conditions of failure. \nFor example, we extracted the following faulty rule in the Cartpole problem $Not(S^\\phi_{5})$ and $S^\\phi_{12}$ and $S^\\phi_{23}$ from our decision tree. First we extract all faulty episodes following this rule. Then, we extract from these episodes all concrete states belonging to the abstract states with the condition of presence in **R1**, i.e., $S^\\phi_{12}$ and $S^\\phi_{23}$.\nFor abstract states $S^\\phi_5$ where the rule states they should be absent, we extract the set of all corresponding concrete states from all episodes in the final dataset.\nFinally, for each abstract state in the rule, we analyze the distribution of each characteristic of the corresponding concrete states (i.e., the position of the cart, the velocity, the angle of the pole and the angular velocity) to interpret the situations under which the agent fails. below you see the boxplots of the mentioned distributions. \n\nMoreover, we rely on the median values of the distribution of the states' characteristics to illustrate each abstract state and hence the failing conditions. \nWe illustrate in the following figure an interpretation of such conditions. \nWe realized that the presence of abstract states $S^\\phi_{12}$ and $S^\\phi_{23}$ represent situations where the cart is close to the right border of the track and the angle of the pole is towards the right. To compensate for the large angle of the pole, as you can see in the figure, the agent has no choice but to push the cart to the right, which results in a fault because of passing the border. Moreover, abstract state $S^\\phi_{5}$ represents a situation where the angle of the pole is not large, and the position of the cart is toward the right but not close to the border. In such situation, the agent will be able to control the pole in the remaining area and keep the pole upright without crossing the border, which justifies why such abstract state should be absent in faulty episodes that satisfy rule **R1**. \n**Answer:** By using STARLA and interpretable ML models, such as Decision Trees, we can accurately learn rules that characterize the faulty episodes of RL agents. \n",
                "original_header": "RQ3. Can we learn accurate rules to characterize the faulty episodes of RL agents?"
            },
            "confidence": 0.9469273554348696,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "name": [
        {
            "result": {
                "value": "STARLA",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "amirhosseinzlf/STARLA",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/amirhosseinzlf/STARLA/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/amirhosseinzlf/STARLA/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 9,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 1,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/amirhosseinzlf/STARLA/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Jupyter Notebook",
                "name": "Jupyter Notebook",
                "type": "Programming_language",
                "size": 15620526
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "executable_example": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RUN_STARLA_MTC.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RUN_STARLA_MTC.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RE_EXECUTE_MTC.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RE_EXECUTE_MTC.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RQ/RQ2_MTC.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RQ/RQ2_MTC.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RQ/RQ3_MTC.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RQ/RQ3_MTC.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RQ/RQ1_MTC.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Mountain_Car/RQ/RQ1_MTC.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/STARLA.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/STARLA.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/Execute_Results.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/Execute_Results.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/RQ/RQ2.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/RQ/RQ2.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/RQ/RQ1.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/RQ/RQ1.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/RQ/RQ3.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/Cart-Pole/RQ/RQ3.ipynb"
        }
    ],
    "type": [
        {
            "result": {
                "value": "notebook-application",
                "type": "String"
            },
            "confidence": 1,
            "technique": "software_type_heuristics"
        }
    ],
    "usage": [
        {
            "result": {
                "value": "We use the Cartpole environment from the OpenAI Gym library[2] as first case study. Cartpole environment is an open-source and widely used environment for RL agents\n\nIn the Cart-Pole (also known as invert pendulum), a pole is attached to a cart, which moves along a track. The movement of the cart is bidirectional so the available actions are pushing the cart to the left and right. However, the movement of the cart is restricted and the maximum rage is 2.4 from the central point. \nThe pole starts upright, and the goal is to balance it by moving the cart left or right.\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/23516995/168501958-b4e278ab-dce6-419c-bb35-4dc1f85b6d99.jpg\"> \n</p>\n\nAs depicted in the figure, the state of the system is characterized by four elements:\n\n\u2022 The position of the cart.\n\n\u2022 The velocity of the cart.\n\n\u2022 The angle of the pole.\n\n\u2022 The angular velocity of the pole.\n\n\nWe provide a reward of +1 for each time step when the pole is still upright. \nThe episodes end in three cases: \n1. The cart is away from the center with a distance more than 2.4 units\n2. The pole\u2019s angle is more than 12 degrees from vertical\n3. The pole remains upright during 200 time-steps.\n\nConsider a situation in which we are trying to reach a reward above 70.\n\nWe define functional faults in the Cart-Pole problem as follows:\n\n- **Functional fault:** If in a given episode, the cart moves away from the center with a distance above 2.4 units, regardless of the accumulated reward, we consider that there is a functional fault in that episode.\n",
                "type": "Text_excerpt",
                "original_header": "Use Case 1: Cartpole",
                "parent_header": [
                    "Use cases"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "In the second case study, we have a DQN agent (impelented by stable baselines[1]) in Mountain Car environment from the OpenAI Gym library[2]. Mountain car environment is an open-source and another widely used environment for RL agents\n\nIn the Mountain Car problem, an under-powered car is located in a valley between two hills. \nSince the gravity is stronger than the engine of the car, the car cannot climb up the steep slope even with full throttle. The objective is to control the car and strategically use its momentum to reach the goal state on top of the right hill as soon as possible. The agent is penalized by -1 for each time step until termination. \n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/23516995/212111530-f4f0f644-f946-495a-80ce-97d720910032.JPG\"> \n</p>\n\nThe state of the agent is defined based on:\n\n1. the location of the car along the x-axis.\n2. the velocity of the car.\n\nThere are three discrete actions that can be used to control the car:\n\n\u2022 Accelerate to the left.\n\n\u2022 Accelerate to the right.\n\n\u2022 Do not accelerate.\n\n\nEpisodes can have three termination scenarios: \n\n1. reaching the goal state,\n2. crossing the left border, or \n3. exceeding the limit of200 time steps.\n\nIn our custom version of the Mountain Car, climbing the left hill is considered an unsafe situation. Consequently, reaching to the leftmost position in the environment results in a termination with the lowest reward. \n\nWe define functional faults as follows:\n\n- **Functional fault:** If in an episode, the car climbs the left hill and passes the left border of the environment, we consider that there is a functional fault and the reward is equal to the minimum reward (-200).\n\n",
                "type": "Text_excerpt",
                "original_header": "Use Case 2: Mountain Car",
                "parent_header": [
                    "Use cases"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "This project is implemented in python with GoogleColab (Jupyter-notebook).\n\n\nWe have two main notebook files the first one is `STARLA.ipynb` which contains the implementation of our search-based testing approach. The second one `Execute_Results.ipynb` is the final step to execute the results as is meant to prepare data required for answering RQ1 & RQ3.\n\n`STARLA.ipynb` contains the implementation of our search-based testing approach. The results are stored as files. \n\n`Execute_Results.ipynb` removed the duplicated episodes in the results and executed the final set of episodes. This is to keep only valid and consistent failing episodes. Thes results are saved as files.\n\nMountain Car folder contains the implementation of STARLA on Mountain Car problem. Files follow the same structore as well. \n`RUN_STARLA_MTC.ipynb` contains the implementation of our search-based testing approach for Mountain Car enrironment, `RE_EXECUTE_MTC.ipynb` is the final step to execute the results.\n\nAs our algorithm is randomized to have a fair comparison we need to run our algorithm and the baseline many times and compare the results. \n\n",
                "type": "Text_excerpt",
                "original_header": "Code Breakdown",
                "parent_header": [
                    "Use cases"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "1. Clone the repo on your Google drive and run the codes using Google Colab https://colab.research.google.com/.\n2. Download the Dataset of replication package from Harvard Dataverse [here](https://doi.org/10.7910/DVN/XWIWVS) and upload it to you Google drive (if you change the location of the files you need to update their path in notebooks).\n3. To generate test episodes: open `STARLA.ipynb` Mount your drive and run the code.\n4. To execute the final results run `Execute_Results.ipynb`.\n\nThe code to generate the results of research questions are in the `RQ` folder \n\n\n",
                "type": "Text_excerpt",
                "original_header": "Getting Started",
                "parent_header": [
                    "Use cases",
                    "Requirements"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "This is the root directory of the repository. The directory is structured as follows:\n\n    Replication package of STARLA\n     .\n     |\n     |STARLA/\n     |\n     |--- Cart-Pole/                               Cart-Pole use case\n     |\n     |---------- /RQ/                              Codes to replicate RQ1 - RQ2 and RQ3\n     |\n     |---------- STARLA.ipynb                      Implementation of the algorithm on Cart-Pole problem\n     |\n     |---------- Execute_Results.ipynb             Execution of the result (required for RQ1 and RQ3)            \n     |\n     |\n     |--- Mountain_Car/                            Mountain Car use case\n     |\n     |---------- /RQ/                              Codes to replicate RQ1 - RQ2 and RQ3\n     |\n     |---------- RUN_STARLA_MTC.ipynb                      Implementation of the algorithm on Mountain Car problem\n     |\n     |---------- RE_EXECUTE_MTC.ipynb             Execution of the result (required for RQ1 and RQ3)          \n  ",
                "type": "Text_excerpt",
                "original_header": "Repository Structure",
                "parent_header": [
                    "Use cases",
                    "Requirements"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "  A Dataset is provided to reproduce the results. This dataset contains our DRL agent, training data of the agent, episodes of random testing of the agent, episodes generated STARLA, execution data of generated episodes as well as the data required to compare the similarities of states and answer RQs for both use cases. Thus the dataset is devided into two parts:\n  1. Dataset_Cart_Pole \n  2. Dataset_MTC\n  \n  below is the structure of the dataset:\n\n    Dataset\n     .\n     |Dataset_Cart_pole/\n     |\n     |--- /dqn-cartpole-50000-with127-GA-Mut-2.pkl             Trained DQN agent 50k steps in Cartpole environment \n     |\n     |--- /dict_GA_Mut_10-09-2020.csv                          Training data of ML models \n     |\n     |--- /Abstract_unique1_for_d=1.pickle                     Abstract states data     \n     |\n     |--- /mutation_number_t.pickle                            Number of Mutations that happened during the search\n     |\n     |--- /random_test_data.pickle                             Random tests episodes representing the final policy of the agent. This also provides the data as a baseline for comparison.\n     |\n     |--- /random_test_data_start_state.pickle                 Initial states of random episodes\n     |\n     |--- /Results/                                            Generated episodes as a result of running STARLA.ipynb. This folder contains results of 20 executions\n     |\n     |--- /Executions/                                         Executed results \n     |\n     |--- /Execution-Similarity/                               Executed results + similarity of states\n     |\n     |\n     |\n     |\n     |Dataset_MTC/\n     |\n     |--- /dqn-4-1-6-89946.zip                                 Trained DQN agent 90k steps in Mountain Car environment \n     |\n     |--- /Final_episodes_trainand_Test_2062_FIXED2.pickle     Training data of ML models \n     |\n     |--- /newly_seen_abs.pickle                               Newly seen abstract states during the search     \n     |\n     |--- /ToTalMutationNumber0.pickle                         Number of Mutations that happened during the search\n     |\n     |--- /RandomDataset/                                      Random tests episodes representing the final policy of the agent. This also provides the data as a baseline for comparison.\n     |\n     |--- /Abstraction/                                        Abstract class data\n     |\n     |--- /Results/                                            Results of running RUN_STARLA_MTC.ipynb. This folder contains results of 20 executions\n     |\n     |--- /Generation/                                         Generated episodes in 20 executions \n     |\n     |--- /Exe_Sim/                                            Executed results + similarity of states\n                 \n     \n----------------\n     \n  \n",
                "type": "Text_excerpt",
                "original_header": "Dataset Structure",
                "parent_header": [
                    "Use cases",
                    "Requirements"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "requirements": [
        {
            "result": {
                "value": "This project is implemented using the following Libraries:\n - stable-baselines==2.10.2\n - pymoo==0.4.2.2\n\nTo install dependencies: \n  - `!pip install stable-baselines==2.10.2`\n  - `!pip install pymoo==0.4.2.2`\n\nThe code was developed and tested based on the following packages:\n\n- python 3.7\n- Tensorflow 1.15.2\n- matplotlib 3.2.2\n- sklearn 1.0.2\n- gym 0.17.3\n- numpy 1.21.6\n- pandas 1.3.5\n---------------\nChange the version of TensorFlow using the line below:\n\n`%tensorflow_version 1.x`\n\nHere is the documentation on how to use this replication package.\n\n",
                "type": "Text_excerpt",
                "original_header": "Requirements",
                "parent_header": [
                    "Use cases"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "This is the root directory of the repository. The directory is structured as follows:\n\n    Replication package of STARLA\n     .\n     |\n     |STARLA/\n     |\n     |--- Cart-Pole/                               Cart-Pole use case\n     |\n     |---------- /RQ/                              Codes to replicate RQ1 - RQ2 and RQ3\n     |\n     |---------- STARLA.ipynb                      Implementation of the algorithm on Cart-Pole problem\n     |\n     |---------- Execute_Results.ipynb             Execution of the result (required for RQ1 and RQ3)            \n     |\n     |\n     |--- Mountain_Car/                            Mountain Car use case\n     |\n     |---------- /RQ/                              Codes to replicate RQ1 - RQ2 and RQ3\n     |\n     |---------- RUN_STARLA_MTC.ipynb                      Implementation of the algorithm on Mountain Car problem\n     |\n     |---------- RE_EXECUTE_MTC.ipynb             Execution of the result (required for RQ1 and RQ3)          \n  ",
                "type": "Text_excerpt",
                "original_header": "Repository Structure",
                "parent_header": [
                    "Use cases",
                    "Requirements"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "  A Dataset is provided to reproduce the results. This dataset contains our DRL agent, training data of the agent, episodes of random testing of the agent, episodes generated STARLA, execution data of generated episodes as well as the data required to compare the similarities of states and answer RQs for both use cases. Thus the dataset is devided into two parts:\n  1. Dataset_Cart_Pole \n  2. Dataset_MTC\n  \n  below is the structure of the dataset:\n\n    Dataset\n     .\n     |Dataset_Cart_pole/\n     |\n     |--- /dqn-cartpole-50000-with127-GA-Mut-2.pkl             Trained DQN agent 50k steps in Cartpole environment \n     |\n     |--- /dict_GA_Mut_10-09-2020.csv                          Training data of ML models \n     |\n     |--- /Abstract_unique1_for_d=1.pickle                     Abstract states data     \n     |\n     |--- /mutation_number_t.pickle                            Number of Mutations that happened during the search\n     |\n     |--- /random_test_data.pickle                             Random tests episodes representing the final policy of the agent. This also provides the data as a baseline for comparison.\n     |\n     |--- /random_test_data_start_state.pickle                 Initial states of random episodes\n     |\n     |--- /Results/                                            Generated episodes as a result of running STARLA.ipynb. This folder contains results of 20 executions\n     |\n     |--- /Executions/                                         Executed results \n     |\n     |--- /Execution-Similarity/                               Executed results + similarity of states\n     |\n     |\n     |\n     |\n     |Dataset_MTC/\n     |\n     |--- /dqn-4-1-6-89946.zip                                 Trained DQN agent 90k steps in Mountain Car environment \n     |\n     |--- /Final_episodes_trainand_Test_2062_FIXED2.pickle     Training data of ML models \n     |\n     |--- /newly_seen_abs.pickle                               Newly seen abstract states during the search     \n     |\n     |--- /ToTalMutationNumber0.pickle                         Number of Mutations that happened during the search\n     |\n     |--- /RandomDataset/                                      Random tests episodes representing the final policy of the agent. This also provides the data as a baseline for comparison.\n     |\n     |--- /Abstraction/                                        Abstract class data\n     |\n     |--- /Results/                                            Results of running RUN_STARLA_MTC.ipynb. This folder contains results of 20 executions\n     |\n     |--- /Generation/                                         Generated episodes in 20 executions \n     |\n     |--- /Exe_Sim/                                            Executed results + similarity of states\n                 \n     \n----------------\n     \n  \n",
                "type": "Text_excerpt",
                "original_header": "Dataset Structure",
                "parent_header": [
                    "Use cases",
                    "Requirements"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "run": [
        {
            "result": {
                "value": "In this scenario, we can consider that episodes of random executions of the agent are available. \nOne example is when the agent is tested to some extent. However, before final deployment, we want further test the agent using STARLA. \nAnother situation is when the RL agent is trained and tested using both a simulator and hardware in the loop [4].\n\nIn this situation, an agent is trained and tested on a simulator in order to have a \u201dwarm-start\u201d learning on real hardware [4]. Since STARLA produces episodes with a high fault probability, we can use it to test the agent when executed on real hardware to further assess the reliability of the agent. In such situation, STARLA uses episodes that are generated with a simulator and executes the newly generated episodes on the hardware.\n\nMore precisely, the total testing budget in this scenario is equal to:\n\nMutated episodes that have been executed during the search + Faulty episodes generated by STARLA (executed after the search)\n\n",
                "type": "Text_excerpt",
                "original_header": "Scenario1: Randomly executed episodes are available or inexpensive:",
                "parent_header": [
                    "Research Questions",
                    "RQ1: Do we find more faults than Random Testing with the same testing budget?"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "In the second scenario, we assume that the agent is trained but not tested so far and we want to test the agent using STARLA. Therefore, we need to use part of our testing budget for random executions, to generate the required episodes. \n\nMore precisely, the total testing budget in this scenario is equal to:\n\nThe number of episodes in the initial population (generated through random executions of the agent) + Mutated episodes that have been executed during the search + Faulty episodes generated by STARLA (executed after the search)\n\n\n\n\n<p align=\"center\" width=\"100%\">\n   <img width=\"45%\" alt=\"CartPole\" src=\"https://user-images.githubusercontent.com/38301008/212175197-bba293be-da5b-4ea0-b894-e7803b262bf3.png\">\n </p>\n <p align=\"center\" width=\"50%\">\n   Number of detected functional faults in the Cartpole case study\n</p>\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/38301008/212175170-6ba47d61-2260-44ec-b18c-fdc3d80d28a6.png\" > \n</p>\n<p align=\"center\" width=\"50%\">\n   Number of detected functional faults in the Mountain Car case study\n</p>\n\n\n**Answer:** For both scenarios and in both case studies, we find significantly more functional faults with STARLA than with Random Testing using the same testing budget. \n\n",
                "type": "Text_excerpt",
                "original_header": "Scenario2: Randomly executed episodes are generated with STARLA and should be accounted for in the testing budget:",
                "parent_header": [
                    "Research Questions",
                    "RQ1: Do we find more faults than Random Testing with the same testing budget?"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "acknowledgement": [
        {
            "result": {
                "value": "--------------\nThis work was supported by a research grant from General Motors as well as the Canada Research Chair and Discovery Grant programs of the Natural Sciences and Engineering Research Council of Canada (NSERC).\n\n",
                "type": "Text_excerpt",
                "original_header": "Acknowledgements",
                "parent_header": [
                    "Research Questions"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "-----\n1- [stable-baselines](https://github.com/hill-a/stable-baselines)\n\n2- [gym](https://github.com/openai/gym)\n\n3- [MOSA](https://ieeexplore.ieee.org/document/7840029)\n\n4- [virtual vs. real:Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization](https://dl.acm.org/doi/abs/10.1109/ICRA.2017.7989186)\n\n",
                "type": "Text_excerpt",
                "original_header": "References",
                "parent_header": [
                    "Research Questions"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "--------\n\nIf you've found STARLA useful for your research, please cite our paper as follows:\n\n    @misc{https://doi.org/10.48550/arxiv.2206.07813,\n    doi = {10.48550/ARXIV.2206.07813},\n    url = {https://arxiv.org/abs/2206.07813},\n    author = {Zolfagharian, Amirhossein and Abdellatif, Manel and Briand, Lionel and Bagherzadeh, Mojtaba and S, Ramesh},\n    keywords = {Software Engineering (cs.SE), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {A Search-Based Testing Approach for Deep Reinforcement Learning Agents},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {arXiv.org perpetual, non-exclusive license}\n    }\n\n",
                "type": "Text_excerpt",
                "original_header": "Cite our paper:",
                "parent_header": [
                    "Research Questions"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "value": "@misc{https://doi.org/10.48550/arxiv.2206.07813,\n    copyright = {arXiv.org perpetual, non-exclusive license},\n    year = {2022},\n    publisher = {arXiv},\n    title = {A Search-Based Testing Approach for Deep Reinforcement Learning Agents},\n    keywords = {Software Engineering (cs.SE), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    author = {Zolfagharian, Amirhossein and Abdellatif, Manel and Briand, Lionel and Bagherzadeh, Mojtaba and S, Ramesh},\n    url = {https://arxiv.org/abs/2206.07813},\n    doi = {10.48550/ARXIV.2206.07813},\n}",
                "type": "Text_excerpt",
                "format": "bibtex",
                "doi": "10.48550/ARXIV.2206.07813",
                "title": "A Search-Based Testing Approach for Deep Reinforcement Learning Agents",
                "author": "Zolfagharian, Amirhossein and Abdellatif, Manel and Briand, Lionel and Bagherzadeh, Mojtaba and S, Ramesh",
                "url": "https://arxiv.org/abs/2206.07813"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "NOTE:TensorFlow 1.X is no longer supported by Google Colab. It is recommended that you create your own virtual environment with Python 3.7 and install the necessary requirements.\n \n",
                "original_header": "RQ3. Can we learn accurate rules to characterize the faulty episodes of RL agents?"
            },
            "confidence": 0.9999984634657874,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "identifier": [
        {
            "result": {
                "type": "Url",
                "value": "https://doi.org/10.7910/DVN/XWIWVS"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "STARLA: Search-Based Testing Approach for Deep Reinforcement Learning Agents"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "images": [
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/23516995/168500802-50486e30-2c5d-43c2-a080-9cc01d964e30.jpg"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/23516995/168501958-b4e278ab-dce6-419c-bb35-4dc1f85b6d99.jpg"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/23516995/212111530-f4f0f644-f946-495a-80ce-97d720910032.JPG"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/38301008/212175197-bba293be-da5b-4ea0-b894-e7803b262bf3.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/38301008/212175170-6ba47d61-2260-44ec-b18c-fdc3d80d28a6.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/38301008/212181441-1f3237dd-f95e-4a77-9e6a-01717ce33f6c.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/38301008/212180368-aad9d8f4-c7ea-464f-8274-77bbade2c3e1.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/23516995/171912017-75548e5b-9151-42f1-afbc-ffafbd8d163e.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/23516995/171912194-b58beb6f-7cdd-402a-99d2-193b1c2f1664.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/23516995/171912131-db832638-ea55-4754-a297-a8ae05a98b64.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://user-images.githubusercontent.com/23516995/171913026-a1713863-4ac8-46d9-b930-6a20b7ba1a53.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2206.07813"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2206.07813},\n    author = {Zolfagharian, Amirhossein and Abdellatif, Manel and Briand, Lionel and Bagherzadeh, Mojtaba and S, Ramesh},\n    keywords = {Software Engineering (cs.SE"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2206.07813](https://arxiv.org/abs/2206.07813)\n\n\n# Use cases\n\n## Use Case 1: Cartpole\n\nWe use the Cartpole environment from the OpenAI Gym library[2] as first case study. Cartpole environment is an open-source and widely used environment for RL agents\n\nIn the Cart-Pole (also known as invert pendulum), a pole is attached to a cart, which moves along a track. The movement of the cart is bidirectional so the available actions are pushing the cart to the left and right. However, the movement of the cart is restricted and the maximum rage is 2.4 from the central point. \nThe pole starts upright, and the goal is to balance it by moving the cart left or right.\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/23516995/168501958-b4e278ab-dce6-419c-bb35-4dc1f85b6d99.jpg\"> \n</p>\n\nAs depicted in the figure, the state of the system is characterized by four elements:\n\n\u2022 The position of the cart.\n\n\u2022 The velocity of the cart.\n\n\u2022 The angle of the pole.\n\n\u2022 The angular velocity of the pole.\n\n\nWe provide a reward of +1 for each time step when the pole is still upright. \nThe episodes end in three cases: \n1. The cart is away from the center with a distance more than 2.4 units\n2. The pole\u2019s angle is more than 12 degrees from vertical\n3. The pole remains upright during 200 time-steps.\n\nConsider a situation in which we are trying to reach a reward above 70.\n\nWe define functional faults in the Cart-Pole problem as follows:\n\n- **Functional fault:** If in a given episode, the cart moves away from the center with a distance above 2.4 units, regardless of the accumulated reward, we consider that there is a functional fault in that episode.\n\n## Use Case 2: Mountain Car\n\nIn the second case study, we have a DQN agent (impelented by stable baselines[1]) in Mountain Car environment from the OpenAI Gym library[2]. Mountain car environment is an open-source and another widely used environment for RL agents\n\nIn the Mountain Car problem, an under-powered car is located in a valley between two hills. \nSince the gravity is stronger than the engine of the car, the car cannot climb up the steep slope even with full throttle. The objective is to control the car and strategically use its momentum to reach the goal state on top of the right hill as soon as possible. The agent is penalized by -1 for each time step until termination. \n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/23516995/212111530-f4f0f644-f946-495a-80ce-97d720910032.JPG\"> \n</p>\n\nThe state of the agent is defined based on:\n\n1. the location of the car along the x-axis.\n2. the velocity of the car.\n\nThere are three discrete actions that can be used to control the car:\n\n\u2022 Accelerate to the left.\n\n\u2022 Accelerate to the right.\n\n\u2022 Do not accelerate.\n\n\nEpisodes can have three termination scenarios: \n\n1. reaching the goal state,\n2. crossing the left border, or \n3. exceeding the limit of200 time steps.\n\nIn our custom version of the Mountain Car, climbing the left hill is considered an unsafe situation. Consequently, reaching to the leftmost position in the environment results in a termination with the lowest reward. \n\nWe define functional faults as follows:\n\n- **Functional fault:** If in an episode, the car climbs the left hill and passes the left border of the environment, we consider that there is a functional fault and the reward is equal to the minimum reward (-200).\n\n\n## Code Breakdown\nThis project is implemented in python with GoogleColab (Jupyter-notebook).\n\n\nWe have two main notebook files the first one is `STARLA.ipynb` which contains the implementation of our search-based testing approach. The second one `Execute_Results.ipynb` is the final step to execute the results as is meant to prepare data required for answering RQ1 & RQ3.\n\n`STARLA.ipynb` contains the implementation of our search-based testing approach. The results are stored as files. \n\n`Execute_Results.ipynb` removed the duplicated episodes in the results and executed the final set of episodes. This is to keep only valid and consistent failing episodes. Thes results are saved as files.\n\nMountain Car folder contains the implementation of STARLA on Mountain Car problem. Files follow the same structore as well. \n`RUN_STARLA_MTC.ipynb` contains the implementation of our search-based testing approach for Mountain Car enrironment, `RE_EXECUTE_MTC.ipynb` is the final step to execute the results.\n\nAs our algorithm is randomized to have a fair comparison we need to run our algorithm and the baseline many times and compare the results. \n\n\n## Requirements\n\nThis project is implemented using the following Libraries:\n - stable-baselines==2.10.2\n - pymoo==0.4.2.2\n\nTo install dependencies: \n  - `!pip install stable-baselines==2.10.2`\n  - `!pip install pymoo==0.4.2.2`\n\nThe code was developed and tested based on the following packages:\n\n- python 3.7\n- Tensorflow 1.15.2\n- matplotlib 3.2.2\n- sklearn 1.0.2\n- gym 0.17.3\n- numpy 1.21.6\n- pandas 1.3.5\n---------------\nChange the version of TensorFlow using the line below:\n\n`%tensorflow_version 1.x`\n\nHere is the documentation on how to use this replication package.\n\n\n### Getting Started\n\n1. Clone the repo on your Google drive and run the codes using Google Colab https://colab.research.google.com/.\n2. Download the Dataset of replication package from Harvard Dataverse [here](https://doi.org/10.7910/DVN/XWIWVS) and upload it to you Google drive (if you change the location of the files you need to update their path in notebooks).\n3. To generate test episodes: open `STARLA.ipynb` Mount your drive and run the code.\n4. To execute the final results run `Execute_Results.ipynb`.\n\nThe code to generate the results of research questions are in the `RQ` folder \n\n\n\n### Repository Structure\n\nThis is the root directory of the repository. The directory is structured as follows:\n\n    Replication package of STARLA\n     .\n     |\n     |STARLA/\n     |\n     |--- Cart-Pole/                               Cart-Pole use case\n     |\n     |---------- /RQ/                              Codes to replicate RQ1 - RQ2 and RQ3\n     |\n     |---------- STARLA.ipynb                      Implementation of the algorithm on Cart-Pole problem\n     |\n     |---------- Execute_Results.ipynb             Execution of the result (required for RQ1 and RQ3)            \n     |\n     |\n     |--- Mountain_Car/                            Mountain Car use case\n     |\n     |---------- /RQ/                              Codes to replicate RQ1 - RQ2 and RQ3\n     |\n     |---------- RUN_STARLA_MTC.ipynb                      Implementation of the algorithm on Mountain Car problem\n     |\n     |---------- RE_EXECUTE_MTC.ipynb             Execution of the result (required for RQ1 and RQ3)          \n  \n### Dataset Structure \n\n  A Dataset is provided to reproduce the results. This dataset contains our DRL agent, training data of the agent, episodes of random testing of the agent, episodes generated STARLA, execution data of generated episodes as well as the data required to compare the similarities of states and answer RQs for both use cases. Thus the dataset is devided into two parts:\n  1. Dataset_Cart_Pole \n  2. Dataset_MTC\n  \n  below is the structure of the dataset:\n\n    Dataset\n     .\n     |Dataset_Cart_pole/\n     |\n     |--- /dqn-cartpole-50000-with127-GA-Mut-2.pkl             Trained DQN agent 50k steps in Cartpole environment \n     |\n     |--- /dict_GA_Mut_10-09-2020.csv                          Training data of ML models \n     |\n     |--- /Abstract_unique1_for_d=1.pickle                     Abstract states data     \n     |\n     |--- /mutation_number_t.pickle                            Number of Mutations that happened during the search\n     |\n     |--- /random_test_data.pickle                             Random tests episodes representing the final policy of the agent. This also provides the data as a baseline for comparison.\n     |\n     |--- /random_test_data_start_state.pickle                 Initial states of random episodes\n     |\n     |--- /Results/                                            Generated episodes as a result of running STARLA.ipynb. This folder contains results of 20 executions\n     |\n     |--- /Executions/                                         Executed results \n     |\n     |--- /Execution-Similarity/                               Executed results + similarity of states\n     |\n     |\n     |\n     |\n     |Dataset_MTC/\n     |\n     |--- /dqn-4-1-6-89946.zip                                 Trained DQN agent 90k steps in Mountain Car environment \n     |\n     |--- /Final_episodes_trainand_Test_2062_FIXED2.pickle     Training data of ML models \n     |\n     |--- /newly_seen_abs.pickle                               Newly seen abstract states during the search     \n     |\n     |--- /ToTalMutationNumber0.pickle                         Number of Mutations that happened during the search\n     |\n     |--- /RandomDataset/                                      Random tests episodes representing the final policy of the agent. This also provides the data as a baseline for comparison.\n     |\n     |--- /Abstraction/                                        Abstract class data\n     |\n     |--- /Results/                                            Results of running RUN_STARLA_MTC.ipynb. This folder contains results of 20 executions\n     |\n     |--- /Generation/                                         Generated episodes in 20 executions \n     |\n     |--- /Exe_Sim/                                            Executed results + similarity of states\n                 \n     \n----------------\n     \n  \n\n# Research Questions\n\n\nOur experimental evaluation answers the research questions below.\n\n## RQ1: Do we find more faults than Random Testing with the same testing budget?\n\n*In this research question we want to study the effectiveness of STARLA in finding more faults than Random Testing when we consider the same testing budget, measured as the number of executed episodes.*\n\nTo do so, we consider two practical testing scenarios:\nIn both scenarios training episodes of the RL agent are given.\n### Scenario1: Randomly executed episodes are available or inexpensive: \nIn this scenario, we can consider that episodes of random executions of the agent are available. \nOne example is when the agent is tested to some extent. However, before final deployment, we want further test the agent using STARLA. \nAnother situation is when the RL agent is trained and tested using both a simulator and hardware in the loop [4].\n\nIn this situation, an agent is trained and tested on a simulator in order to have a \u201dwarm-start\u201d learning on real hardware [4]. Since STARLA produces episodes with a high fault probability, we can use it to test the agent when executed on real hardware to further assess the reliability of the agent. In such situation, STARLA uses episodes that are generated with a simulator and executes the newly generated episodes on the hardware.\n\nMore precisely, the total testing budget in this scenario is equal to:\n\nMutated episodes that have been executed during the search + Faulty episodes generated by STARLA (executed after the search)\n\n\n### Scenario2: Randomly executed episodes are generated with STARLA and should be accounted for in the testing budget: \nIn the second scenario, we assume that the agent is trained but not tested so far and we want to test the agent using STARLA. Therefore, we need to use part of our testing budget for random executions, to generate the required episodes. \n\nMore precisely, the total testing budget in this scenario is equal to:\n\nThe number of episodes in the initial population (generated through random executions of the agent) + Mutated episodes that have been executed during the search + Faulty episodes generated by STARLA (executed after the search)\n\n\n\n\n<p align=\"center\" width=\"100%\">\n   <img width=\"45%\" alt=\"CartPole\" src=\"https://user-images.githubusercontent.com/38301008/212175197-bba293be-da5b-4ea0-b894-e7803b262bf3.png\">\n </p>\n <p align=\"center\" width=\"50%\">\n   Number of detected functional faults in the Cartpole case study\n</p>\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/38301008/212175170-6ba47d61-2260-44ec-b18c-fdc3d80d28a6.png\" > \n</p>\n<p align=\"center\" width=\"50%\">\n   Number of detected functional faults in the Mountain Car case study\n</p>\n\n\n**Answer:** For both scenarios and in both case studies, we find significantly more functional faults with STARLA than with Random Testing using the same testing budget. \n\n\n## RQ2: Can we rely on ML models to predict faulty episodes?\n\n*In this research question we investigate the accuracy of ML classifiers in predicting faulty episodes of the RL agent.*\n\nWe use Random Forest to predict the probabilities of reward and functional faults in a given episode.\nTo build our training dataset, we sampled episodes from both episodes generated through random executions of the agent and episodes from the training phase of the agent. Episodes are encoded based on the presence or absence of their abstract states. We have two different ML models, one for predicting the probability of a reward fault and the other one for predicting the probability of a functional fault. We considered 70 % of data for training and 30% for testing.\n\n**Answer:** Using the mentioned ML classifier and feature representation, we can accurately classify the episodes of RL agents as having functional faults or no fault at all.\n\n\n\n## RQ3. Can we learn accurate rules to characterize the faulty episodes of RL agents?\n\n*Here, we investigate the learning of interpretable rules that characterize faulty episodes to understand the conditions under which the RL agent can be expected to fail.*\n\nFor this reason, we need to rely on an interpretable ML model, in this case, a Decision Tree model, to learn such rules.\nWe assess the accuracy of decision trees and therefore our ability to learn accurate rules based on the faulty episodes that we identify with STARLA. \nIn practice, engineers will need to use such an approach to assess the safety of using an RL agent and understand the reasons of faults.\nIn this part, we assess the accuracy of trained models that extract the rules of functional and reward faults based on k-fold cross-validation.\n\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"50%\" src=\"https://user-images.githubusercontent.com/38301008/212181441-1f3237dd-f95e-4a77-9e6a-01717ce33f6c.png\" > \n</p>\n<p align=\"center\" width=\"50%\">\n   Accuracy of the fault detection rules in the Cartpole case study\n</p>\n\n<p align=\"center\" width=\"100%\">\n   <img width=\"50%\" alt=\"CartPole\" src=\"https://user-images.githubusercontent.com/38301008/212180368-aad9d8f4-c7ea-464f-8274-77bbade2c3e1.png\">\n </p>\n <p align=\"center\" width=\"50%\">\n   Accuracy of the fault detection rulesin the Mountain Car case study\n</p>\n\n\nSuch highly accurate rules can help developers understand the conditions under which the agent fails. One can analyze, the concrete states that correspond to abstract states leading to faults to extract real-world conditions of failure. \nFor example, we extracted the following faulty rule in the Cartpole problem $Not(S^\\phi_{5"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2206.07813](https://arxiv.org/abs/2206.07813)\n\n\n## Description of the Approach\n\nSTARLA Requires a DRL agent and its training data as input as tries to effectively and efficiently generate failing test cases (episodes) to reveal the faults of agents policy. Detailed approach is depicted in the following diagram:\n\n\n![Approach4_page-0001](https://user-images.githubusercontent.com/23516995/168500802-50486e30-2c5d-43c2-a080-9cc01d964e30.jpg)\n\n\nAs depicted, the main objective of STARLA is to generate and find episodes with high fault probabilities in order to assess whether an RL agent can be safely deployed. \nThe algorithm uses the data from the Agent to build ML models that predict the probabilities of fault (to which extent episodes are similar to faulty episodes). The outputs of these models are combined with the reward of the agent and certainty level. They are meant to guide the Genetic search toward faulty episodes. \n\nIn the Genetic search, we use specific crossover and mutation functions. Also as we have multiple fitness functions, we are using MOSA Algorithm[3]. For more explanations please see our paper. [https://arxiv.org/abs/2206.07813](https://arxiv.org/abs/2206.07813)\n\n\n# Use cases\n\n## Use Case 1: Cartpole\n\nWe use the Cartpole environment from the OpenAI Gym library[2] as first case study. Cartpole environment is an open-source and widely used environment for RL agents\n\nIn the Cart-Pole (also known as invert pendulum), a pole is attached to a cart, which moves along a track. The movement of the cart is bidirectional so the available actions are pushing the cart to the left and right. However, the movement of the cart is restricted and the maximum rage is 2.4 from the central point. \nThe pole starts upright, and the goal is to balance it by moving the cart left or right.\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/23516995/168501958-b4e278ab-dce6-419c-bb35-4dc1f85b6d99.jpg\"> \n</p>\n\nAs depicted in the figure, the state of the system is characterized by four elements:\n\n\u2022 The position of the cart.\n\n\u2022 The velocity of the cart.\n\n\u2022 The angle of the pole.\n\n\u2022 The angular velocity of the pole.\n\n\nWe provide a reward of +1 for each time step when the pole is still upright. \nThe episodes end in three cases: \n1. The cart is away from the center with a distance more than 2.4 units\n2. The pole\u2019s angle is more than 12 degrees from vertical\n3. The pole remains upright during 200 time-steps.\n\nConsider a situation in which we are trying to reach a reward above 70.\n\nWe define functional faults in the Cart-Pole problem as follows:\n\n- **Functional fault:** If in a given episode, the cart moves away from the center with a distance above 2.4 units, regardless of the accumulated reward, we consider that there is a functional fault in that episode.\n\n## Use Case 2: Mountain Car\n\nIn the second case study, we have a DQN agent (impelented by stable baselines[1]) in Mountain Car environment from the OpenAI Gym library[2]. Mountain car environment is an open-source and another widely used environment for RL agents\n\nIn the Mountain Car problem, an under-powered car is located in a valley between two hills. \nSince the gravity is stronger than the engine of the car, the car cannot climb up the steep slope even with full throttle. The objective is to control the car and strategically use its momentum to reach the goal state on top of the right hill as soon as possible. The agent is penalized by -1 for each time step until termination. \n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/23516995/212111530-f4f0f644-f946-495a-80ce-97d720910032.JPG\"> \n</p>\n\nThe state of the agent is defined based on:\n\n1. the location of the car along the x-axis.\n2. the velocity of the car.\n\nThere are three discrete actions that can be used to control the car:\n\n\u2022 Accelerate to the left.\n\n\u2022 Accelerate to the right.\n\n\u2022 Do not accelerate.\n\n\nEpisodes can have three termination scenarios: \n\n1. reaching the goal state,\n2. crossing the left border, or \n3. exceeding the limit of200 time steps.\n\nIn our custom version of the Mountain Car, climbing the left hill is considered an unsafe situation. Consequently, reaching to the leftmost position in the environment results in a termination with the lowest reward. \n\nWe define functional faults as follows:\n\n- **Functional fault:** If in an episode, the car climbs the left hill and passes the left border of the environment, we consider that there is a functional fault and the reward is equal to the minimum reward (-200).\n\n\n## Code Breakdown\nThis project is implemented in python with GoogleColab (Jupyter-notebook).\n\n\nWe have two main notebook files the first one is `STARLA.ipynb` which contains the implementation of our search-based testing approach. The second one `Execute_Results.ipynb` is the final step to execute the results as is meant to prepare data required for answering RQ1 & RQ3.\n\n`STARLA.ipynb` contains the implementation of our search-based testing approach. The results are stored as files. \n\n`Execute_Results.ipynb` removed the duplicated episodes in the results and executed the final set of episodes. This is to keep only valid and consistent failing episodes. Thes results are saved as files.\n\nMountain Car folder contains the implementation of STARLA on Mountain Car problem. Files follow the same structore as well. \n`RUN_STARLA_MTC.ipynb` contains the implementation of our search-based testing approach for Mountain Car enrironment, `RE_EXECUTE_MTC.ipynb` is the final step to execute the results.\n\nAs our algorithm is randomized to have a fair comparison we need to run our algorithm and the baseline many times and compare the results. \n\n\n## Requirements\n\nThis project is implemented using the following Libraries:\n - stable-baselines==2.10.2\n - pymoo==0.4.2.2\n\nTo install dependencies: \n  - `!pip install stable-baselines==2.10.2`\n  - `!pip install pymoo==0.4.2.2`\n\nThe code was developed and tested based on the following packages:\n\n- python 3.7\n- Tensorflow 1.15.2\n- matplotlib 3.2.2\n- sklearn 1.0.2\n- gym 0.17.3\n- numpy 1.21.6\n- pandas 1.3.5\n---------------\nChange the version of TensorFlow using the line below:\n\n`%tensorflow_version 1.x`\n\nHere is the documentation on how to use this replication package.\n\n\n### Getting Started\n\n1. Clone the repo on your Google drive and run the codes using Google Colab https://colab.research.google.com/.\n2. Download the Dataset of replication package from Harvard Dataverse [here](https://doi.org/10.7910/DVN/XWIWVS) and upload it to you Google drive (if you change the location of the files you need to update their path in notebooks).\n3. To generate test episodes: open `STARLA.ipynb` Mount your drive and run the code.\n4. To execute the final results run `Execute_Results.ipynb`.\n\nThe code to generate the results of research questions are in the `RQ` folder \n\n\n\n### Repository Structure\n\nThis is the root directory of the repository. The directory is structured as follows:\n\n    Replication package of STARLA\n     .\n     |\n     |STARLA/\n     |\n     |--- Cart-Pole/                               Cart-Pole use case\n     |\n     |---------- /RQ/                              Codes to replicate RQ1 - RQ2 and RQ3\n     |\n     |---------- STARLA.ipynb                      Implementation of the algorithm on Cart-Pole problem\n     |\n     |---------- Execute_Results.ipynb             Execution of the result (required for RQ1 and RQ3)            \n     |\n     |\n     |--- Mountain_Car/                            Mountain Car use case\n     |\n     |---------- /RQ/                              Codes to replicate RQ1 - RQ2 and RQ3\n     |\n     |---------- RUN_STARLA_MTC.ipynb                      Implementation of the algorithm on Mountain Car problem\n     |\n     |---------- RE_EXECUTE_MTC.ipynb             Execution of the result (required for RQ1 and RQ3)          \n  \n### Dataset Structure \n\n  A Dataset is provided to reproduce the results. This dataset contains our DRL agent, training data of the agent, episodes of random testing of the agent, episodes generated STARLA, execution data of generated episodes as well as the data required to compare the similarities of states and answer RQs for both use cases. Thus the dataset is devided into two parts:\n  1. Dataset_Cart_Pole \n  2. Dataset_MTC\n  \n  below is the structure of the dataset:\n\n    Dataset\n     .\n     |Dataset_Cart_pole/\n     |\n     |--- /dqn-cartpole-50000-with127-GA-Mut-2.pkl             Trained DQN agent 50k steps in Cartpole environment \n     |\n     |--- /dict_GA_Mut_10-09-2020.csv                          Training data of ML models \n     |\n     |--- /Abstract_unique1_for_d=1.pickle                     Abstract states data     \n     |\n     |--- /mutation_number_t.pickle                            Number of Mutations that happened during the search\n     |\n     |--- /random_test_data.pickle                             Random tests episodes representing the final policy of the agent. This also provides the data as a baseline for comparison.\n     |\n     |--- /random_test_data_start_state.pickle                 Initial states of random episodes\n     |\n     |--- /Results/                                            Generated episodes as a result of running STARLA.ipynb. This folder contains results of 20 executions\n     |\n     |--- /Executions/                                         Executed results \n     |\n     |--- /Execution-Similarity/                               Executed results + similarity of states\n     |\n     |\n     |\n     |\n     |Dataset_MTC/\n     |\n     |--- /dqn-4-1-6-89946.zip                                 Trained DQN agent 90k steps in Mountain Car environment \n     |\n     |--- /Final_episodes_trainand_Test_2062_FIXED2.pickle     Training data of ML models \n     |\n     |--- /newly_seen_abs.pickle                               Newly seen abstract states during the search     \n     |\n     |--- /ToTalMutationNumber0.pickle                         Number of Mutations that happened during the search\n     |\n     |--- /RandomDataset/                                      Random tests episodes representing the final policy of the agent. This also provides the data as a baseline for comparison.\n     |\n     |--- /Abstraction/                                        Abstract class data\n     |\n     |--- /Results/                                            Results of running RUN_STARLA_MTC.ipynb. This folder contains results of 20 executions\n     |\n     |--- /Generation/                                         Generated episodes in 20 executions \n     |\n     |--- /Exe_Sim/                                            Executed results + similarity of states\n                 \n     \n----------------\n     \n  \n\n# Research Questions\n\n\nOur experimental evaluation answers the research questions below.\n\n## RQ1: Do we find more faults than Random Testing with the same testing budget?\n\n*In this research question we want to study the effectiveness of STARLA in finding more faults than Random Testing when we consider the same testing budget, measured as the number of executed episodes.*\n\nTo do so, we consider two practical testing scenarios:\nIn both scenarios training episodes of the RL agent are given.\n### Scenario1: Randomly executed episodes are available or inexpensive: \nIn this scenario, we can consider that episodes of random executions of the agent are available. \nOne example is when the agent is tested to some extent. However, before final deployment, we want further test the agent using STARLA. \nAnother situation is when the RL agent is trained and tested using both a simulator and hardware in the loop [4].\n\nIn this situation, an agent is trained and tested on a simulator in order to have a \u201dwarm-start\u201d learning on real hardware [4]. Since STARLA produces episodes with a high fault probability, we can use it to test the agent when executed on real hardware to further assess the reliability of the agent. In such situation, STARLA uses episodes that are generated with a simulator and executes the newly generated episodes on the hardware.\n\nMore precisely, the total testing budget in this scenario is equal to:\n\nMutated episodes that have been executed during the search + Faulty episodes generated by STARLA (executed after the search)\n\n\n### Scenario2: Randomly executed episodes are generated with STARLA and should be accounted for in the testing budget: \nIn the second scenario, we assume that the agent is trained but not tested so far and we want to test the agent using STARLA. Therefore, we need to use part of our testing budget for random executions, to generate the required episodes. \n\nMore precisely, the total testing budget in this scenario is equal to:\n\nThe number of episodes in the initial population (generated through random executions of the agent) + Mutated episodes that have been executed during the search + Faulty episodes generated by STARLA (executed after the search)\n\n\n\n\n<p align=\"center\" width=\"100%\">\n   <img width=\"45%\" alt=\"CartPole\" src=\"https://user-images.githubusercontent.com/38301008/212175197-bba293be-da5b-4ea0-b894-e7803b262bf3.png\">\n </p>\n <p align=\"center\" width=\"50%\">\n   Number of detected functional faults in the Cartpole case study\n</p>\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"45%\" src=\"https://user-images.githubusercontent.com/38301008/212175170-6ba47d61-2260-44ec-b18c-fdc3d80d28a6.png\" > \n</p>\n<p align=\"center\" width=\"50%\">\n   Number of detected functional faults in the Mountain Car case study\n</p>\n\n\n**Answer:** For both scenarios and in both case studies, we find significantly more functional faults with STARLA than with Random Testing using the same testing budget. \n\n\n## RQ2: Can we rely on ML models to predict faulty episodes?\n\n*In this research question we investigate the accuracy of ML classifiers in predicting faulty episodes of the RL agent.*\n\nWe use Random Forest to predict the probabilities of reward and functional faults in a given episode.\nTo build our training dataset, we sampled episodes from both episodes generated through random executions of the agent and episodes from the training phase of the agent. Episodes are encoded based on the presence or absence of their abstract states. We have two different ML models, one for predicting the probability of a reward fault and the other one for predicting the probability of a functional fault. We considered 70 % of data for training and 30% for testing.\n\n**Answer:** Using the mentioned ML classifier and feature representation, we can accurately classify the episodes of RL agents as having functional faults or no fault at all.\n\n\n\n## RQ3. Can we learn accurate rules to characterize the faulty episodes of RL agents?\n\n*Here, we investigate the learning of interpretable rules that characterize faulty episodes to understand the conditions under which the RL agent can be expected to fail.*\n\nFor this reason, we need to rely on an interpretable ML model, in this case, a Decision Tree model, to learn such rules.\nWe assess the accuracy of decision trees and therefore our ability to learn accurate rules based on the faulty episodes that we identify with STARLA. \nIn practice, engineers will need to use such an approach to assess the safety of using an RL agent and understand the reasons of faults.\nIn this part, we assess the accuracy of trained models that extract the rules of functional and reward faults based on k-fold cross-validation.\n\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"50%\" src=\"https://user-images.githubusercontent.com/38301008/212181441-1f3237dd-f95e-4a77-9e6a-01717ce33f6c.png\" > \n</p>\n<p align=\"center\" width=\"50%\">\n   Accuracy of the fault detection rules in the Cartpole case study\n</p>\n\n<p align=\"center\" width=\"100%\">\n   <img width=\"50%\" alt=\"CartPole\" src=\"https://user-images.githubusercontent.com/38301008/212180368-aad9d8f4-c7ea-464f-8274-77bbade2c3e1.png\">\n </p>\n <p align=\"center\" width=\"50%\">\n   Accuracy of the fault detection rulesin the Mountain Car case study\n</p>\n\n\nSuch highly accurate rules can help developers understand the conditions under which the agent fails. One can analyze, the concrete states that correspond to abstract states leading to faults to extract real-world conditions of failure. \nFor example, we extracted the following faulty rule in the Cartpole problem $Not(S^\\phi_{5"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/amirhosseinzlf/STARLA/main/README.md"
        }
    ]
}