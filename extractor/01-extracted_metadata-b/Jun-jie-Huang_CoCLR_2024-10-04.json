{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 00:48:12"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/Jun-jie-Huang/CoCLR",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "Jun-jie-Huang",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2021-05-24T06:52:11Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-09-20T16:12:09Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "license": [
        {
            "result": {
                "value": "https://api.github.com/licenses/mit",
                "type": "License",
                "name": "MIT License",
                "url": "https://api.github.com/licenses/mit",
                "spdx_id": "MIT"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "MIT License\n\nCopyright (c) 2021 Jun-jie-Huang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
                "type": "File_dump"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/LICENSE"
        }
    ],
    "name": [
        {
            "result": {
                "value": "CoCLR",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "Jun-jie-Huang/CoCLR",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/Jun-jie-Huang/CoCLR/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/Jun-jie-Huang/CoCLR/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 43,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 8,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/Jun-jie-Huang/CoCLR/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 104344
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "requirements": [
        {
            "result": {
                "value": "\r\n```\r\ntorch==1.4.0\r\ntransformers==2.5.0\r\ntqdm\r\nscikit-learn\r\nnltk\r\n```\r\n\r",
                "type": "Text_excerpt",
                "original_header": "Requirements",
                "parent_header": [
                    "CoSQA and CoCLR for Code Search and Question Answering"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        }
    ],
    "download": [
        {
            "result": {
                "value": "\r\nPlease to the first point in [Model Checkpoint section](#model-checkpoint)\r\n\r",
                "type": "Text_excerpt",
                "original_header": "Step 1: download the checkpoint trained on CodeSearchNet",
                "parent_header": [
                    "CoSQA and CoCLR for Code Search and Question Answering",
                    "Code Search",
                    "CoCLR on Code Search"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "\r\nIf you find this project useful, please cite it using the following format:\r\n\r\n```\r\n@inproceedings{huang2021cosqa,\r\n    title = \"{C}o{SQA}: 20,000+ Web Queries for Code Search and Question Answering\",\r\n    author = \"Huang, Junjie  and Tang, Duyu  and Shou, Linjun  and Gong, Ming  and Xu, Ke  and Jiang, Daxin  and Zhou, Ming  and Duan, Nan\",\r\n    booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)\",\r\n    month = aug,\r\n    year = \"2021\",\r\n    address = \"Online\",\r\n    publisher = \"Association for Computational Linguistics\",\r\n    url = \"https://aclanthology.org/2021.acl-long.442\",\r\n    doi = \"10.18653/v1/2021.acl-long.442\",\r\n    pages = \"5690--5700\",\r\n}\r\n\r\n@inproceedings{Lu2021CodeXGLUE,\r\n author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and GONG, MING and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and LIU, Shujie},\r\n booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},\r\n editor = {J. Vanschoren and S. Yeung},\r\n pages = {},\r\n title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},\r\n url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/c16a5320fa475530d9583c34fd356ef5-Paper-round1.pdf},\r\n volume = {1},\r\n year = {2021}\r\n}\r\n\r\n```\r\n\r\n",
                "type": "Text_excerpt",
                "original_header": "Reference",
                "parent_header": [
                    "CoSQA and CoCLR for Code Search and Question Answering"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "value": "@inproceedings{huang2021cosqa,\n    pages = {5690--5700},\n    doi = {10.18653/v1/2021.acl-long.442},\n    url = {https://aclanthology.org/2021.acl-long.442},\n    publisher = {Association for Computational Linguistics},\n    address = {Online},\n    year = {2021},\n    month = {August},\n    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n    author = {Huang, Junjie  and Tang, Duyu  and Shou, Linjun  and Gong, Ming  and Xu, Ke  and Jiang, Daxin  and Zhou, Ming  and Duan, Nan},\n    title = {{C}o{SQA}: 20,000+ Web Queries for Code Search and Question Answering},\n}",
                "type": "Text_excerpt",
                "format": "bibtex",
                "doi": "10.18653/v1/2021.acl-long.442",
                "title": "{C}o{SQA}: 20,000+ Web Queries for Code Search and Question Answering",
                "author": "Huang, Junjie  and Tang, Duyu  and Shou, Linjun  and Gong, Ming  and Xu, Ke  and Jiang, Daxin  and Zhou, Ming  and Duan, Nan",
                "url": "https://aclanthology.org/2021.acl-long.442"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "value": "@inproceedings{Lu2021CodeXGLUE,\n    year = {2021},\n    volume = {1},\n    url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/c16a5320fa475530d9583c34fd356ef5-Paper-round1.pdf},\n    title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},\n    pages = {},\n    editor = {J. Vanschoren and S. Yeung},\n    booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},\n    author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and GONG, MING and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and LIU, Shujie},\n}",
                "type": "Text_excerpt",
                "format": "bibtex",
                "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation",
                "author": "Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and GONG, MING and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and LIU, Shujie",
                "url": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/c16a5320fa475530d9583c34fd356ef5-Paper-round1.pdf"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        }
    ],
    "description": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\nThis repository contains the data and source code used in ACL 2021 main conference paper  [CoSQA: 20,000+ Web Queries for Code Search and Question Answering](https://aclanthology.org/2021.acl-long.442.pdf).  The CoSQA dataset includes 20,604 human annotated labels for pairs of natural language web queries and codes. The source code contains baseline methods and proposed contrastive learning method dubbed CoCLR to enhance query-code matching. The dataset and source code are created by Beihang University, MSRA NLC group and STCA NLP group. Our codes follow MIT License and our datasets follow Computational Use of Data Agreement (CUDA) License. \r\n\r \n",
                "original_header": "CoSQA and CoCLR for Code Search and Question Answering"
            },
            "confidence": 0.9906987116666301,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\n- data/qa: this folder contains the query/code pairs for training, dev and test data. For better usage, we copy the CoSQA dataset and WebQueryTest from [CodeXGLUE -- Code Search (WebQueryTest)](https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code/NL-code-search-WebQuery).\r\n- data/retrieval: this folder contains the data for training, validating and testing a code retriever. The code to obtain data for ablation study in the [paper](https://aclanthology.org/2021.acl-long.442.pdf) is also included.\r\n- code_qa: this folder contains the source code to run code question answering task.\r\n- code_search: this folder contains the source code to run code search task.\r\n\r \n",
                "original_header": "Repository Structure"
            },
            "confidence": 0.9709214376304759,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\nTo evaluate on CodeXGLUE WebQueryTest, you can first download the test file from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE) and move the file to `data` directory with the following commands.\r\n\r\n```\r\ngit clone https://github.com/microsoft/CodeXGLUE\r\ncp CodeXGLUE/Text-Code/NL-code-search-WebQuery/data/test_webquery.json ./data/qa/\r\n```\r\n\r\nThen you can evaluate you model and submit the `--test_predictions_output` to CodeXGLUE challenge for the results on the test set.\r\n\r\nYou can submit the `--test_predictions_output` to CodeXGLUE challenge for the results on the test set.\r\nBASH2*\r\n\r \n",
                "original_header": "Step 3: evaluate on CodeXGLUE - code search (WebQueryTest)"
            },
            "confidence": 0.999999841791306,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\nTo apply CoCLR on the task of code question answering, you can run the commands with the following steps.\r\n\r\n#### Step 1: download the checkpoint trained on CodeSearchNet\r\n\r\nPlease to the first point in [Model Checkpoint section](#model-checkpoint)\r\n\r \n",
                "original_header": "CoCLR on Code Question Answering"
            },
            "confidence": 0.9980265431893839,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\n```\r\ncd data\r\npython augment_qra.py --task qa --qra_mode delete\r\npython augment_qra.py --task qa --qra_mode copy\r\npython augment_qra.py --task qa --qra_mode switch\r\ncd ../\r\n```\r\n\r \n",
                "original_header": "Step 2: create query-rewritten data"
            },
            "confidence": 0.9999680844762308,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\nTo train a search model without CoCLR, you can use the following command:\r\n\r\n```\r\nmodel=./model/search_codebert\r\nCUDA_VISIBLE_DEVICES=\"0\" python ./code_search/run_siamese_test.py \\\r\n\t\t--model_type roberta \\\r\n\t\t--do_train \\\r\n\t\t--do_eval \\\r\n\t\t--eval_all_checkpoints \\\r\n        --data_dir ./data/search/ \\\r\n\t\t--train_data_file cosqa-retrieval-train-19604.json \\\r\n\t\t--eval_data_file cosqa-retrieval-dev-500.json \\\r\n\t\t--retrieval_code_base code_idx_map.txt \\\r\n\t\t--code_type code \\\r\n\t\t--max_seq_length 200 \\\r\n\t\t--per_gpu_train_batch_size 32 \\\r\n\t\t--per_gpu_retrieval_batch_size 67 \\\r\n\t\t--learning_rate 1e-6 \\\r\n\t\t--num_train_epochs 10 \\\r\n\t\t--gradient_accumulation_steps 1 \\\r\n\t\t--evaluate_during_training \\\r\n\t\t--checkpoint_path ./model/codesearchnet-checkpoint \\\r\n        --output_dir ${model} \\\r\n        --encoder_name_or_path microsoft/codebert-base \\\r\n        2>&1 | tee ./search-train-codebert.log\r\n\r\n```\r\n\r\nYou can evaluate the model on the test set with the following command:\r\n\r\nBASH2*\r\n\r \n",
                "original_header": "Step 2: training and evaluating"
            },
            "confidence": 0.9013025463526841,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\nTo apply CoCLR on the task of code search, you can run the commands with the following steps.\r\n\r\n#### Step 1: download the checkpoint trained on CodeSearchNet\r\n\r\nPlease to the first point in [Model Checkpoint section](#model-checkpoint)\r\n\r\n#### Step 2: create query-rewritten data\r\n\r\n```\r\ncd data\r\npython augment_qra.py --task retrieval --qra_mode delete\r\npython augment_qra.py --task retrieval --qra_mode copy\r\npython augment_qra.py --task retrieval --qra_mode switch\r\ncd ../\r\n```\r\n\r \n",
                "original_header": "CoCLR on Code Search"
            },
            "confidence": 0.9999992556901062,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\nTo see the effects of different component of code in code search, we provide to run the ablation study. You can first create the test set of codes that some parts are removed, and then evaluate on these dataset with the following commands. You can select `--mode` with `header_only`, `doc_only`, `body_only`, `no_header`, `no_doc`, `no_body`.\r\n\r\n```\r\ncd data/search\r\npython split_code_for_retrieval.py\r\nmode=no_doc\r\nCUDA_VISIBLE_DEVICES=\"0\" python ./code_search/run_siamese_test.py \\\r\n\t\t--model_type roberta \\\r\n\t\t--do_retrieval \\\r\n\t\t--data_dir ./data/search/ablation_test_code_component/${mode} \\\r\n\t\t--test_data_file cosqa-retrieval-test-500.json \\\r\n\t\t--retrieval_code_base code_idx_map.txt \\\r\n\t\t--code_type code \\\r\n\t\t--max_seq_length 200 \\\r\n\t\t--per_gpu_retrieval_batch_size 67 \\\r\n\t\t--output_dir ${model}/checkpoint-best-mrr/ \\\r\n\t\t--encoder_name_or_path microsoft/codebert-base \\\r\n\t\t--pred_model_dir ${model}/checkpoint-best-mrr \\\r\n\t\t--retrieval_predictions_output ${model}/retrieval_outputs.txt \\\r\n\t\t2>&1 | tee ./search-test-ablation-codebert-coclr-${qra}-${mode}.log\r\n```\r\n\r \n",
                "original_header": "Ablation with Model Component"
            },
            "confidence": 0.9906518343328605,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\n1. The checkpoint trained on CodeSearchNet can be downloaded through [this link](https://drive.google.com/drive/folders/1rM5A6dPf05Q5mP9kWjfsdRIpsqfI4IBi?usp=sharing). You can first download the checkpoint. Then move it to `./model/` and rename the dirname to `codesearchnet-checkpoint`. You can also use the data in [CodeXGLUE code search (WebQueryTest)](https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code/NL-code-search-WebQuery/data) to train the models by your self.\r\n\r\n2. The checkpoint with best code question answering results can be downloaded through [this link](https://drive.google.com/drive/folders/1VjZOEI_N25R_30ZL2hYNaY-43FfpQ_MD?usp=sharing) and move to `./model/`.\r\n\r\n3. The checkpoint with best code search results can be downloaded through [this link](https://drive.google.com/drive/folders/1rmyqG68nmnjSFg4t8ywaSwJBCluKE4l2?usp=sharing) and move to `./model/`.\r\n\r \n",
                "original_header": "Model Checkpoint"
            },
            "confidence": 0.9084063633832858,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        }
    ],
    "invocation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\n```\r\nmodel=./model/qa_codebert\r\nCUDA_VISIBLE_DEVICES=\"0\" python ./code_qa/run_siamese_test.py \\\r\n\t\t--model_type roberta \r\n\t\t--augment \r\n\t\t--do_train \r\n\t\t--do_eval \r\n\t\t--eval_all_checkpoints \r\n\t\t--data_dir ./data/qa/ \\\t\r\n\t\t--train_data_file cosqa-train.json \r\n\t\t--eval_data_file cosqa-dev.json \r\n\t\t--max_seq_length 200 \r\n\t\t--per_gpu_train_batch_size 32 \r\n\t\t--per_gpu_eval_batch_size 16 \r\n\t\t--learning_rate 5e-6 \r\n\t\t--num_train_epochs 10 \r\n\t\t--gradient_accumulation_steps 1 \r\n\t\t--evaluate_during_training \\\r\n\t\t--warmup_steps 500 \\\r\n\t\t--checkpoint_path ./model/codesearchnet-checkpoint \\\r\n\t\t--output_dir ${model} \\\r\n\t\t--encoder_name_or_path microsoft/codebert-base \\\r\n        2>&1 | tee ./qa-train-codebert.log\r\n```\r\n\r \n",
                "original_header": "Step 2: training"
            },
            "confidence": 0.9211411917541573,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\n```\r\nqra=switch\r\nmodel=./model/qa_codebert_${qra}\r\nCUDA_VISIBLE_DEVICES=\"0\" python ./code_qa/run_siamese_test.py \\\r\n\t\t--model_type roberta \\\r\n\t\t--augment \\\r\n\t\t--do_train \\\r\n\t\t--do_eval \\\r\n\t\t--eval_all_checkpoints \\\r\n\t\t--data_dir ./data/qa/ \\\r\n\t\t--train_data_file cosqa-train-qra-${qra}-29707.json \\\r\n\t\t--eval_data_file cosqa-dev.json \\\r\n\t\t--max_seq_length 200 \\\r\n\t\t--per_gpu_train_batch_size 32 \\\r\n\t\t--per_gpu_eval_batch_size 16 \\\r\n\t\t--learning_rate 1e-5 \\\r\n\t\t--warmup_steps 1000 \\\r\n\t\t--num_train_epochs 10 \\\r\n\t\t--gradient_accumulation_steps 1 \\\r\n\t\t--evaluate_during_training \\\r\n\t\t--checkpoint_path ./model/codesearchnet-checkpoint \\\r\n\t\t--output_dir ${model} \\\r\n\t\t--encoder_name_or_path microsoft/codebert-base \\\r\n        2>&1 | tee ./qa-train-codebert-coclr-${qra}.log\r\n```\r\n\r \n",
                "original_header": "Step 3: training"
            },
            "confidence": 0.9296769252722128,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\nYou can submit the `--test_predictions_output` to CodeXGLUE challenge for the results on the test set.\r\n\r\n```\r\nqra=switch\r\nmodel=./model/qa_codebert_${qra}\r\nCUDA_VISIBLE_DEVICES=\"0\" python ./code_qa/run_siamese_test.py \\\r\n\t\t--model_type roberta  \\\r\n\t\t--augment \\\r\n\t\t--do_test \\\r\n\t\t--data_dir ./data/qa \\\r\n\t\t--test_data_file test_webquery.json \\\r\n\t\t--max_seq_length 200 \\\r\n\t\t--per_gpu_eval_batch_size 2 \\\r\n\t\t--output_dir ${model}/checkpoint-best-aver/ \\\r\n\t\t--encoder_name_or_path microsoft/codebert-base \\\r\n\t\t--pred_model_dir ${model}/checkpoint-best-aver/ \\\r\n\t\t--test_predictions_output ${model}/webquery_predictions.txt \\\r\n\t\t2>&1| tee ./qa-test-codebert-coclr-${qra}.log\r\n```\r\n\r \n",
                "original_header": "Step 4: evaluate on CodeXGLUE - code search (WebQueryTest)"
            },
            "confidence": 0.9189803988681191,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\nTo train a search model without CoCLR, you can use the following command:\r\n\r\n```\r\nmodel=./model/search_codebert\r\nCUDA_VISIBLE_DEVICES=\"0\" python ./code_search/run_siamese_test.py \\\r\n\t\t--model_type roberta \\\r\n\t\t--do_train \\\r\n\t\t--do_eval \\\r\n\t\t--eval_all_checkpoints \\\r\n        --data_dir ./data/search/ \\\r\n\t\t--train_data_file cosqa-retrieval-train-19604.json \\\r\n\t\t--eval_data_file cosqa-retrieval-dev-500.json \\\r\n\t\t--retrieval_code_base code_idx_map.txt \\\r\n\t\t--code_type code \\\r\n\t\t--max_seq_length 200 \\\r\n\t\t--per_gpu_train_batch_size 32 \\\r\n\t\t--per_gpu_retrieval_batch_size 67 \\\r\n\t\t--learning_rate 1e-6 \\\r\n\t\t--num_train_epochs 10 \\\r\n\t\t--gradient_accumulation_steps 1 \\\r\n\t\t--evaluate_during_training \\\r\n\t\t--checkpoint_path ./model/codesearchnet-checkpoint \\\r\n        --output_dir ${model} \\\r\n        --encoder_name_or_path microsoft/codebert-base \\\r\n        2>&1 | tee ./search-train-codebert.log\r\n\r\n```\r\n\r\nYou can evaluate the model on the test set with the following command:\r\n\r\nBASH2*\r\n\r \n",
                "original_header": "Step 2: training and evaluating"
            },
            "confidence": 0.9163568017680732,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\r\n```\r\nqra=switch\r\nmodel=./model/search_codebert_${qra}\r\nCUDA_VISIBLE_DEVICES=\"0\" python ./code_search/run_siamese_test.py \\\r\n\t\t--model_type roberta \\\r\n\t\t--augment \\\r\n\t\t--do_train \\\r\n\t\t--do_eval \\\r\n\t\t--eval_all_checkpoints \\\r\n        --data_dir ./data/search/ \\\r\n\t\t--train_data_file cosqa-retrieval-train-19604-qra-${qra}-28624.json \\\r\n\t\t--eval_data_file cosqa-retrieval-dev-500.json \\\r\n\t\t--retrieval_code_base code_idx_map.txt \\\r\n\t\t--code_type code \\\r\n\t\t--max_seq_length 200 \\\r\n\t\t--per_gpu_train_batch_size 32 \\\r\n\t\t--per_gpu_retrieval_batch_size 67 \\\r\n\t\t--learning_rate 1e-6 \\\r\n\t\t--num_train_epochs 10 \\\r\n\t\t--gradient_accumulation_steps 1 \\\r\n\t\t--evaluate_during_training \\\r\n\t\t--checkpoint_path ./model/codesearchnet-checkpoint \\\r\n        --output_dir ${model} \\\r\n        --encoder_name_or_path microsoft/codebert-base \\\r\n        2>&1 | tee ./search-train-codebert-coclr-${qra}.log\r\n\r\nCUDA_VISIBLE_DEVICES=\"0\" python ./code_search/run_siamese_test.py \\\r\n\t\t--model_type roberta \\\r\n\t\t--do_retrieval \\\r\n\t\t--data_dir ./data/search/ \\\r\n\t\t--test_data_file cosqa-retrieval-test-500.json \\\r\n\t\t--retrieval_code_base code_idx_map.txt \\\r\n\t\t--code_type code \\\r\n\t\t--max_seq_length 200 \\\r\n\t\t--per_gpu_retrieval_batch_size 67 \\\r\n\t\t--output_dir ${model}/checkpoint-best-mrr/ \\\r\n\t\t--encoder_name_or_path microsoft/codebert-base \\\r\n\t\t--pred_model_dir ${model}/checkpoint-best-mrr \\\r\n\t\t--retrieval_predictions_output ${model}/retrieval_outputs.txt \\\r\n\t\t2>&1 | tee ./search-test-codebert-coclr-${qra}.log\r\n```\r\n\r \n",
                "original_header": "Step 3: training and evaluating"
            },
            "confidence": 0.9198292309989491,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "CoSQA and CoCLR for Code Search and Question Answering"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jun-jie-Huang/CoCLR/main/README.md"
        }
    ]
}