{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-03 19:14:50"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/aebeljs/VeRLPy",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "aebeljs",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2020-11-08T11:14:05Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-09-20T02:30:41Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "license": [
        {
            "result": {
                "value": "https://api.github.com/licenses/mit",
                "type": "License",
                "name": "MIT License",
                "url": "https://api.github.com/licenses/mit",
                "spdx_id": "MIT"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "description": [
        {
            "result": {
                "value": "VeRLPy is an open-source python library developed to improve the digital hardware verification process by using Reinforcement Learning (RL). It provides a generic Gym environment implementation for building cocotb-based testbenches for verifying any hardware design.",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "name": [
        {
            "result": {
                "value": "VeRLPy",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "aebeljs/VeRLPy",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/aebeljs/VeRLPy/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/aebeljs/VeRLPy/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 23,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "hardware-verification, reinforcement-learning",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 3,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/aebeljs/VeRLPy/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 25581
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "The easiest way to start using VeRLPy is to install it using `pip install verlpy`\n\nVeRLPy is currently dependent on OpenAI [Gym](https://gym.openai.com/), [cocotb](https://docs.cocotb.org/en/stable/), [cocotb-bus](https://github.com/cocotb/cocotb-bus), and [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/). These packages should get installed alongside VeRLPy when installing using `pip`. For running the verification, a simulator compatible with cocotb is additionally required. Please refer to the official  [cocotb](https://docs.cocotb.org/en/stable/) documentation to set this up.\n",
                "type": "Text_excerpt",
                "original_header": "Installation",
                "parent_header": [
                    "VeRLPy"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "Having familiarity with [cocotb](https://docs.cocotb.org/en/stable/), OpenAI [Gym](https://gym.openai.com/) and [this whitepaper on VeRLPy](https://arxiv.org/abs/2108.03978) will be very beneficial to get started with the VeRLPy library.\n\nThe hardware design provided in Verilog or VHDL is simulated by cocotb using the chosen simulator. VeRLPy aims to offer a clean interface for bringing RL logic into the conventional cocotb testbench while adhering to the OpenAI Gym environment structure allowing users to leverage the standard RL tools. The DUT and the verification testbench are the environment for the RL agent to act on. The agent chooses an action which is executed on the environment and the consequences of that action are informed back to the agent in terms of the state and the reward. \n",
                "type": "Text_excerpt",
                "original_header": "Usage Guide",
                "parent_header": [
                    "VeRLPy"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "To build a testbench using VeRLPy, we need to start by defining the verification goals in terms of functional coverage. This involves identifying the events occurring in the DUT that correspond to the features that are part of the design specifications. The reward given to the RL agent will be a function of how often these events occur during the simulation.\n\nIn addition, since there is the additional RL component involved, the MDP has to be defined. This means identifying what each timestep and each episode corresponds to in terms of verification logic and input stimulus to the DUT, and what the state and the action space of the MDP represent. \n\nEach RL episode starts with a call to to the `reset()` function of the Gym environment. Followed by this, there are one or more calls to the `step(action)` function until `done` is returned with a value of `True` from the RL environment.  Refer to [Gym](https://gym.openai.com/) for a more detailed understanding of how the control flow occurs in a Gym environment. \n",
                "type": "Text_excerpt",
                "original_header": "Identifying verification goals and defining the MDP",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "The library provides a Gym environment object `CocotbEnv` which can be inherited to build the verification testbench. This class has predefined functions/coroutines which interact with the RL agent to facilitate the learning process. These functions are triggered by the `reset()`, `step(action)`, `done` variable, etc. Please refer [here](https://github.com/aebeljs/VeRLPy/blob/main/src/cocotb_env.py) for more detailed explanations of these functions and their implementation details. We start with implementing this class that inherits `CocotbEnv`. \n\n```python\n# test_my_example_design.py\n\nimport cocotb\nfrom verlpy import CocotbEnv\n\nclass MyExampleDesignCocotbEnv(CocotbEnv):\n    def __init__(self, dut, observation_space):\n        super().__init__()\n        self.dut = dut # DUT object used for cocotb-based verification\n        self.observation_space = observation_space # state space of the RL agent\n\n        # add here any \"self.\" variables that need to be accessed in\n        # other functions below\n\n    @cocotb.coroutine\n    def setup_rl_episode(self):\n        # add here the logic to be \n        # executed on each call to reset() by the RL agent\n\n    @cocotb.coroutine\n    def rl_step(self):\n        # add here the verification logic to be \n        # executed on each call to step() by the RL agent\n        \n\n    @cocotb.coroutine\n    def terminate_rl_episode(self):\n        # add here the logic to be executed at the end\n        # of each RL episode when done == 1 for the Gym env\n\n\n    def finish_experiment(self):\n        # add here the logic to be executed after all\n        # the episodes are completed\n```\nNote that all the coroutines with the decorator `cocotb.coroutine` require a `yield` statement in the body like how it is in standard cocotb testbenches.\n",
                "type": "Text_excerpt",
                "original_header": "Inheriting CocotbEnv",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "The functions/coroutine implementations defined in `MyExampleDesignCocotbEnv` should contain the appropriate logic that must run from the cocotb side. Once this class is defined, the cocotb test can be added to invoke the verification logic from this class. While defining this, the state space of the MDP should also be passed as an argument as shown below.\n\n```python\n# test_my_example_design.py\n\nimport cocotb\nfrom verlpy import CocotbEnv, utils\nimport gym\n\nclass MyExampleDesignCocotbEnv(CocotbEnv):\n    def __init__(self, dut, observation_space):\n        super().__init__()\n        self.dut = dut\n        self.observation_space = observation_space\n\n        # add here any \"self.\" variables that need to be accessed in\n        # other functions below\n\n    @cocotb.coroutine\n    def setup_rl_episode(self):\n        # add here the logic to be \n        # executed on each call to reset() by the RL agent\n\n    @cocotb.coroutine\n    def rl_step(self):\n        # add here the verification logic to be \n        # executed on each call to step() by the RL agent\n        \n\n    @cocotb.coroutine\n    def terminate_rl_episode(self):\n        # add here the logic to be executed at the end\n        # of each RL episode when done == 1 for the Gym env\n\n\n    def finish_experiment(self):\n        # add here the logic to be executed after all\n        # the episodes are completed\n\n# entry point for the cocotb verification test\n@cocotb.test()\ndef run_test(dut):\n    cocotb_env = MyExampleDesignCocotbEnv(dut, gym.spaces.Discrete(1))\n    # gym.spaces.Discrete(1) => Just 1 state in the state space\n    yield cocotb_env.run()\n\n    # plot the results of the verification experiment\n    utils.visualize(cocotb_env.log_file_name)\n```\n\nVeRLPy also provides some plotting capabilities which can be accessed from `utils` as shown above.\n",
                "type": "Text_excerpt",
                "original_header": "Instantiating the verification environment object",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "The identified functional coverage events can be tracked by using cocotb coroutines like in conventional cocotb-based verification.\n\n```python\n# test_my_example_design.py\n\nimport cocotb\nfrom verlpy import CocotbEnv, utils\nimport gym\n\nclass MyExampleDesignCocotbEnv(CocotbEnv):\n    def __init__(self, dut, observation_space):\n        super().__init__()\n        self.dut = dut\n        self.observation_space = observation_space\n\n        # add here any \"self.\" variables that need to be accessed in\n        # other functions below\n\n    @cocotb.coroutine\n    def setup_rl_episode(self):\n        # add here the logic to be \n        # executed on each call to reset() by the RL agent\n        self.cocotb_coverage.clear() # clear last episode's coverage\n        self.coverage_coroutine = cocotb.fork(monitor_signals(self.dut, self.cocotb_coverage))\n\n    @cocotb.coroutine\n    def rl_step(self):\n        # add here the verification logic to be \n        # executed on each call to step() by the RL agent\n        \n\n    @cocotb.coroutine\n    def terminate_rl_episode(self):\n        # add here the logic to be executed at the end\n        # of each RL episode when done == 1 for the Gym env\n\n        self.coverage_coroutine.kill()\n\n\n    def finish_experiment(self):\n        # add here the logic to be executed after all\n        # the episodes are completed\n\n@cocotb.coroutine\ndef monitor_signals(dut, cocotb_coverage):\n    while True:\n        yield RisingEdge(dut.CLK)\n        s = [(int)(dut.reg_1.value == 1),\n             (int)((dut.reg_2.value) % 4 != 0),\n             (int)(dut.reg_3.value == 32)]\n        # Here reg_1, reg_2 and reg_3 are some\n        # key registers of interest in the DUT\n\n        s = ''.join(map(str, s))\n        cocotb_coverage.append(s)\n\n# entry point for the cocotb verification test\n@cocotb.test()\ndef run_test(dut):\n    cocotb_env = MyExampleDesignCocotbEnv(dut, gym.spaces.Discrete(1))\n    # gym.spaces.Discrete(1) => Just 1 state in the state space\n    yield cocotb_env.run()\n\n    # plot the results of the verification experiment\n    utils.visualize(cocotb_env.log_file_name)\n```\nThe `monitor_signals` coroutine added above monitors the DUT for events of interest that count towards the functional coverage. The  boolean logical expressions in the list `s` above correspond to the logical expressions for identifying each event. The number of times these events occur affect the reward signal given to the RL agent. `monitor_signals` should track these events and add them to the `cocotb_coverage` attribute of the `MyExampleDesignCocotbEnv` class that we wrote. `monitor_signals` is invoked in the `setup_rl_episode` coroutine along with the clock and reset coroutines. It is passed the `cocotb_coverage` attribute as an argument. Note that `monitor_signals` is killed in the `terminate_rl_episode` coroutine at the end of each RL episode. This is important for all coroutines since it might otherwise lead to performance issues with multiple \"alive\" coroutines still ongoing from previous episodes.\n",
                "type": "Text_excerpt",
                "original_header": "Adding coroutines to track events",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "A configuration file `config.ini` needs to be provided to specify the parameters related to the simulation and the RL agent. A sample coniguration file is provided below with comments for what each section and key corresponds to.\n\n```ini\n; This section is to provide the\n; main parameters for the verification runs\n[main]\n; number of RL steps for which the experiment is run\nnum_steps = 1000\n\n; number of functional events tracked\nnum_events = 3\n\n; weightage of each functional event for reward computation\nreward_function = [0, 0, 1]\n\n; set log_step as 0 for logging just aggregated results and 1 for logging details in each step\nlog_step = 0\n\n; set mode as 0 to generate the random baseline without RL and 1 for using RL\nmode = 1\n\n; specify the stable_baselines3 algorithm to be used from SAC, DDPG and TD3\nalgorithm = SAC\n\n; fsm_states contains the regex patterns for \n; state-based binary sequence generation\n; (leave as [] unless utils.get_next_state_of_FSM() is needed in the code)\nfsm_states = []\n\n; Provide the discrete action component names here. \n; The valid dscrete value set for the specified keys\n; should be given in the [discrete] section\ndiscrete_params = ['count_width', 'fmap_len']\n\n\n; This section is to provide the bounds\n; of the continuous dimensions of the action space.\n; If multiple dimensions are there, provide the list of bounds for each dimension\n; eg: lower_bounds = [0, 0, 1] and upper_bounds = [1, 1, 3] corresponds to\n; [0, 1] x [0, 1] x [1, 3] as the continuous action space\n[continuous]\nlower_bounds = [0, 5]\nupper_bounds = [1, 7]\n\n\n; This section is to provide the list of valid \n; discrete values for each discrete action\n; component named in discrete_params\n[discrete]\ncount_width = [1, 2, 3, 4, 5, 6, 7, 8]\nfmap_len = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n\n\n; This section is to provide the required\n; hyperparameters of the chosen stable_baselines3 algorithm\n[RL]\npolicy = 'MlpPolicy'\nlearning_starts = 100\nlearning_rate = 0.0003\ntrain_freq = (1, 'episode')\nverbose = 1\n```\nThe `reward_function` key specifies how the functional events tracked in the `self.cocotb_coverage` attribute need to be rewarded for improving coverage. `reward_function` set as `[0, 0, 1]` like above implies that if the third functional event occurs during a step, a reward is given to the RL agent. Refer to the [paper](https://arxiv.org/abs/2108.03978) for the actual computation details.\n\nThe `[continuous]` and `[discrete]` sections together specify the total action space of the RL agent. The continuous dimensions of the action space based on the above configuration file is the cross product `[0, 1] x [5, 7]`. The discrete dimensions of the the action space is the cross product `{1, 2, ..., 8} x {100, 200, ..., 1000}`. Therefore the complete action space is the cross product `[0, 1] x [5, 7] x {1, 2, ..., 8} x {100, 200, ..., 1000}`.\n",
                "type": "Text_excerpt",
                "original_header": "Configuration File",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "Finally, the body of each of the coroutines overriden in `MyExampleDesignCocotbEnv` need to be completed. The action suggested by the RL agent based on the `config.ini` can be accessed through the `self.continuous_actions` and `self.discrete_actions` attributes of the class. In the above example, `self.continuous_actions` will sample from `[0, 1] x [5, 7]` and `self.discrete_actions` will sample from `{1, 2, ..., 8} x {100, 200, ..., 1000}`.\n\nThe list `self.cocotb_coverage` needs to be updated with the strings corresponding to the covered events from the previous timestep of the RL episode for proper reward computation based on the reward function defined in the `config.ini` file. This update will happen on its own if `self.cocotb_coverage` is passed as the argument `cocotb_coverage` to the  `monitor_signals` coroutine defined above  Refer to the examples folder for more concrete examples on how this is done in various designs.\n",
                "type": "Text_excerpt",
                "original_header": "Filling in the verification logic",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "VeRLPy by default assumes a single step single state MDP. If a multi-step MDP is required, it can be implemented by overriding `compute_rl_observation` function in the `MyExampleDesignCocotbEnv` class. The internal elements of the DUT that need to be tracked for computing the observation/state after each step can be done so by utilizing a separate coroutine like how `monitor_signals` is used for tracking the coverage to compute the reward.\n",
                "type": "Text_excerpt",
                "original_header": "Multi-step RL",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "The make file can be written like how it is done usually in cocotb testbenches. Once it is done and `make` is run, the verification simulation runs and the logs, models and plots are generated. Use the `mode` configuration parameter in `config.ini` for running the verification with/without the RL feedback.\n",
                "type": "Text_excerpt",
                "original_header": "Make file",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        }
    ],
    "usage": [
        {
            "result": {
                "value": "Having familiarity with [cocotb](https://docs.cocotb.org/en/stable/), OpenAI [Gym](https://gym.openai.com/) and [this whitepaper on VeRLPy](https://arxiv.org/abs/2108.03978) will be very beneficial to get started with the VeRLPy library.\n\nThe hardware design provided in Verilog or VHDL is simulated by cocotb using the chosen simulator. VeRLPy aims to offer a clean interface for bringing RL logic into the conventional cocotb testbench while adhering to the OpenAI Gym environment structure allowing users to leverage the standard RL tools. The DUT and the verification testbench are the environment for the RL agent to act on. The agent chooses an action which is executed on the environment and the consequences of that action are informed back to the agent in terms of the state and the reward. \n",
                "type": "Text_excerpt",
                "original_header": "Usage Guide",
                "parent_header": [
                    "VeRLPy"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "To build a testbench using VeRLPy, we need to start by defining the verification goals in terms of functional coverage. This involves identifying the events occurring in the DUT that correspond to the features that are part of the design specifications. The reward given to the RL agent will be a function of how often these events occur during the simulation.\n\nIn addition, since there is the additional RL component involved, the MDP has to be defined. This means identifying what each timestep and each episode corresponds to in terms of verification logic and input stimulus to the DUT, and what the state and the action space of the MDP represent. \n\nEach RL episode starts with a call to to the `reset()` function of the Gym environment. Followed by this, there are one or more calls to the `step(action)` function until `done` is returned with a value of `True` from the RL environment.  Refer to [Gym](https://gym.openai.com/) for a more detailed understanding of how the control flow occurs in a Gym environment. \n",
                "type": "Text_excerpt",
                "original_header": "Identifying verification goals and defining the MDP",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "The library provides a Gym environment object `CocotbEnv` which can be inherited to build the verification testbench. This class has predefined functions/coroutines which interact with the RL agent to facilitate the learning process. These functions are triggered by the `reset()`, `step(action)`, `done` variable, etc. Please refer [here](https://github.com/aebeljs/VeRLPy/blob/main/src/cocotb_env.py) for more detailed explanations of these functions and their implementation details. We start with implementing this class that inherits `CocotbEnv`. \n\n```python\n# test_my_example_design.py\n\nimport cocotb\nfrom verlpy import CocotbEnv\n\nclass MyExampleDesignCocotbEnv(CocotbEnv):\n    def __init__(self, dut, observation_space):\n        super().__init__()\n        self.dut = dut # DUT object used for cocotb-based verification\n        self.observation_space = observation_space # state space of the RL agent\n\n        # add here any \"self.\" variables that need to be accessed in\n        # other functions below\n\n    @cocotb.coroutine\n    def setup_rl_episode(self):\n        # add here the logic to be \n        # executed on each call to reset() by the RL agent\n\n    @cocotb.coroutine\n    def rl_step(self):\n        # add here the verification logic to be \n        # executed on each call to step() by the RL agent\n        \n\n    @cocotb.coroutine\n    def terminate_rl_episode(self):\n        # add here the logic to be executed at the end\n        # of each RL episode when done == 1 for the Gym env\n\n\n    def finish_experiment(self):\n        # add here the logic to be executed after all\n        # the episodes are completed\n```\nNote that all the coroutines with the decorator `cocotb.coroutine` require a `yield` statement in the body like how it is in standard cocotb testbenches.\n",
                "type": "Text_excerpt",
                "original_header": "Inheriting CocotbEnv",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "The functions/coroutine implementations defined in `MyExampleDesignCocotbEnv` should contain the appropriate logic that must run from the cocotb side. Once this class is defined, the cocotb test can be added to invoke the verification logic from this class. While defining this, the state space of the MDP should also be passed as an argument as shown below.\n\n```python\n# test_my_example_design.py\n\nimport cocotb\nfrom verlpy import CocotbEnv, utils\nimport gym\n\nclass MyExampleDesignCocotbEnv(CocotbEnv):\n    def __init__(self, dut, observation_space):\n        super().__init__()\n        self.dut = dut\n        self.observation_space = observation_space\n\n        # add here any \"self.\" variables that need to be accessed in\n        # other functions below\n\n    @cocotb.coroutine\n    def setup_rl_episode(self):\n        # add here the logic to be \n        # executed on each call to reset() by the RL agent\n\n    @cocotb.coroutine\n    def rl_step(self):\n        # add here the verification logic to be \n        # executed on each call to step() by the RL agent\n        \n\n    @cocotb.coroutine\n    def terminate_rl_episode(self):\n        # add here the logic to be executed at the end\n        # of each RL episode when done == 1 for the Gym env\n\n\n    def finish_experiment(self):\n        # add here the logic to be executed after all\n        # the episodes are completed\n\n# entry point for the cocotb verification test\n@cocotb.test()\ndef run_test(dut):\n    cocotb_env = MyExampleDesignCocotbEnv(dut, gym.spaces.Discrete(1))\n    # gym.spaces.Discrete(1) => Just 1 state in the state space\n    yield cocotb_env.run()\n\n    # plot the results of the verification experiment\n    utils.visualize(cocotb_env.log_file_name)\n```\n\nVeRLPy also provides some plotting capabilities which can be accessed from `utils` as shown above.\n",
                "type": "Text_excerpt",
                "original_header": "Instantiating the verification environment object",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "The identified functional coverage events can be tracked by using cocotb coroutines like in conventional cocotb-based verification.\n\n```python\n# test_my_example_design.py\n\nimport cocotb\nfrom verlpy import CocotbEnv, utils\nimport gym\n\nclass MyExampleDesignCocotbEnv(CocotbEnv):\n    def __init__(self, dut, observation_space):\n        super().__init__()\n        self.dut = dut\n        self.observation_space = observation_space\n\n        # add here any \"self.\" variables that need to be accessed in\n        # other functions below\n\n    @cocotb.coroutine\n    def setup_rl_episode(self):\n        # add here the logic to be \n        # executed on each call to reset() by the RL agent\n        self.cocotb_coverage.clear() # clear last episode's coverage\n        self.coverage_coroutine = cocotb.fork(monitor_signals(self.dut, self.cocotb_coverage))\n\n    @cocotb.coroutine\n    def rl_step(self):\n        # add here the verification logic to be \n        # executed on each call to step() by the RL agent\n        \n\n    @cocotb.coroutine\n    def terminate_rl_episode(self):\n        # add here the logic to be executed at the end\n        # of each RL episode when done == 1 for the Gym env\n\n        self.coverage_coroutine.kill()\n\n\n    def finish_experiment(self):\n        # add here the logic to be executed after all\n        # the episodes are completed\n\n@cocotb.coroutine\ndef monitor_signals(dut, cocotb_coverage):\n    while True:\n        yield RisingEdge(dut.CLK)\n        s = [(int)(dut.reg_1.value == 1),\n             (int)((dut.reg_2.value) % 4 != 0),\n             (int)(dut.reg_3.value == 32)]\n        # Here reg_1, reg_2 and reg_3 are some\n        # key registers of interest in the DUT\n\n        s = ''.join(map(str, s))\n        cocotb_coverage.append(s)\n\n# entry point for the cocotb verification test\n@cocotb.test()\ndef run_test(dut):\n    cocotb_env = MyExampleDesignCocotbEnv(dut, gym.spaces.Discrete(1))\n    # gym.spaces.Discrete(1) => Just 1 state in the state space\n    yield cocotb_env.run()\n\n    # plot the results of the verification experiment\n    utils.visualize(cocotb_env.log_file_name)\n```\nThe `monitor_signals` coroutine added above monitors the DUT for events of interest that count towards the functional coverage. The  boolean logical expressions in the list `s` above correspond to the logical expressions for identifying each event. The number of times these events occur affect the reward signal given to the RL agent. `monitor_signals` should track these events and add them to the `cocotb_coverage` attribute of the `MyExampleDesignCocotbEnv` class that we wrote. `monitor_signals` is invoked in the `setup_rl_episode` coroutine along with the clock and reset coroutines. It is passed the `cocotb_coverage` attribute as an argument. Note that `monitor_signals` is killed in the `terminate_rl_episode` coroutine at the end of each RL episode. This is important for all coroutines since it might otherwise lead to performance issues with multiple \"alive\" coroutines still ongoing from previous episodes.\n",
                "type": "Text_excerpt",
                "original_header": "Adding coroutines to track events",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "A configuration file `config.ini` needs to be provided to specify the parameters related to the simulation and the RL agent. A sample coniguration file is provided below with comments for what each section and key corresponds to.\n\n```ini\n; This section is to provide the\n; main parameters for the verification runs\n[main]\n; number of RL steps for which the experiment is run\nnum_steps = 1000\n\n; number of functional events tracked\nnum_events = 3\n\n; weightage of each functional event for reward computation\nreward_function = [0, 0, 1]\n\n; set log_step as 0 for logging just aggregated results and 1 for logging details in each step\nlog_step = 0\n\n; set mode as 0 to generate the random baseline without RL and 1 for using RL\nmode = 1\n\n; specify the stable_baselines3 algorithm to be used from SAC, DDPG and TD3\nalgorithm = SAC\n\n; fsm_states contains the regex patterns for \n; state-based binary sequence generation\n; (leave as [] unless utils.get_next_state_of_FSM() is needed in the code)\nfsm_states = []\n\n; Provide the discrete action component names here. \n; The valid dscrete value set for the specified keys\n; should be given in the [discrete] section\ndiscrete_params = ['count_width', 'fmap_len']\n\n\n; This section is to provide the bounds\n; of the continuous dimensions of the action space.\n; If multiple dimensions are there, provide the list of bounds for each dimension\n; eg: lower_bounds = [0, 0, 1] and upper_bounds = [1, 1, 3] corresponds to\n; [0, 1] x [0, 1] x [1, 3] as the continuous action space\n[continuous]\nlower_bounds = [0, 5]\nupper_bounds = [1, 7]\n\n\n; This section is to provide the list of valid \n; discrete values for each discrete action\n; component named in discrete_params\n[discrete]\ncount_width = [1, 2, 3, 4, 5, 6, 7, 8]\nfmap_len = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n\n\n; This section is to provide the required\n; hyperparameters of the chosen stable_baselines3 algorithm\n[RL]\npolicy = 'MlpPolicy'\nlearning_starts = 100\nlearning_rate = 0.0003\ntrain_freq = (1, 'episode')\nverbose = 1\n```\nThe `reward_function` key specifies how the functional events tracked in the `self.cocotb_coverage` attribute need to be rewarded for improving coverage. `reward_function` set as `[0, 0, 1]` like above implies that if the third functional event occurs during a step, a reward is given to the RL agent. Refer to the [paper](https://arxiv.org/abs/2108.03978) for the actual computation details.\n\nThe `[continuous]` and `[discrete]` sections together specify the total action space of the RL agent. The continuous dimensions of the action space based on the above configuration file is the cross product `[0, 1] x [5, 7]`. The discrete dimensions of the the action space is the cross product `{1, 2, ..., 8} x {100, 200, ..., 1000}`. Therefore the complete action space is the cross product `[0, 1] x [5, 7] x {1, 2, ..., 8} x {100, 200, ..., 1000}`.\n",
                "type": "Text_excerpt",
                "original_header": "Configuration File",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "Finally, the body of each of the coroutines overriden in `MyExampleDesignCocotbEnv` need to be completed. The action suggested by the RL agent based on the `config.ini` can be accessed through the `self.continuous_actions` and `self.discrete_actions` attributes of the class. In the above example, `self.continuous_actions` will sample from `[0, 1] x [5, 7]` and `self.discrete_actions` will sample from `{1, 2, ..., 8} x {100, 200, ..., 1000}`.\n\nThe list `self.cocotb_coverage` needs to be updated with the strings corresponding to the covered events from the previous timestep of the RL episode for proper reward computation based on the reward function defined in the `config.ini` file. This update will happen on its own if `self.cocotb_coverage` is passed as the argument `cocotb_coverage` to the  `monitor_signals` coroutine defined above  Refer to the examples folder for more concrete examples on how this is done in various designs.\n",
                "type": "Text_excerpt",
                "original_header": "Filling in the verification logic",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "VeRLPy by default assumes a single step single state MDP. If a multi-step MDP is required, it can be implemented by overriding `compute_rl_observation` function in the `MyExampleDesignCocotbEnv` class. The internal elements of the DUT that need to be tracked for computing the observation/state after each step can be done so by utilizing a separate coroutine like how `monitor_signals` is used for tracking the coverage to compute the reward.\n",
                "type": "Text_excerpt",
                "original_header": "Multi-step RL",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "The make file can be written like how it is done usually in cocotb testbenches. Once it is done and `make` is run, the verification simulation runs and the logs, models and plots are generated. Use the `mode` configuration parameter in `config.ini` for running the verification with/without the RL feedback.\n",
                "type": "Text_excerpt",
                "original_header": "Make file",
                "parent_header": [
                    "VeRLPy",
                    "Usage Guide"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "If you find this work useful, consider citing it with this BibTex:\n```\n@article{DBLP:journals/corr/abs-2108-03978,\n  author    = {Aebel Joe Shibu and\n               Sadhana S and\n               Shilpa N and\n               Pratyush Kumar},\n  title     = {VeRLPy: Python Library for Verification of Digital Designs with Reinforcement\n               Learning},\n  journal   = {CoRR},\n  volume    = {abs/2108.03978},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2108.03978},\n  eprinttype = {arXiv},\n  eprint    = {2108.03978},\n  timestamp = {Wed, 11 Aug 2021 15:24:08 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-03978.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n",
                "type": "Text_excerpt",
                "original_header": "Citation",
                "parent_header": [
                    "VeRLPy"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        },
        {
            "result": {
                "value": "@article{DBLP:journals/corr/abs-2108-03978,\n    bibsource = {dblp computer science bibliography, https://dblp.org},\n    biburl = {https://dblp.org/rec/journals/corr/abs-2108-03978.bib},\n    timestamp = {Wed, 11 Aug 2021 15:24:08 +0200},\n    eprint = {2108.03978},\n    eprinttype = {arXiv},\n    url = {https://arxiv.org/abs/2108.03978},\n    year = {2021},\n    volume = {abs/2108.03978},\n    journal = {CoRR},\n    title = {VeRLPy: Python Library for Verification of Digital Designs with Reinforcement\nLearning},\n    author = {Aebel Joe Shibu and\nSadhana S and\nShilpa N and\nPratyush Kumar},\n}",
                "type": "Text_excerpt",
                "format": "bibtex",
                "title": "VeRLPy: Python Library for Verification of Digital Designs with Reinforcement\nLearning",
                "author": "Aebel Joe Shibu and\nSadhana S and\nShilpa N and\nPratyush Kumar",
                "url": "https://arxiv.org/abs/2108.03978"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        }
    ],
    "application_domain": [
        {
            "result": {
                "type": "String",
                "value": "Reinforcement Learning"
            },
            "confidence": 0.9533333333333333,
            "technique": "supervised_classification"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "VeRLPy"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        }
    ],
    "related_documentation": [
        {
            "result": {
                "type": "Url",
                "value": "https://stable-baselines3.readthedocs.io/",
                "format": "readthedocs"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2108.03978"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/aebeljs/VeRLPy/main/README.md"
        }
    ]
}