{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-03 18:44:19"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/Jinxhy/AppAIsecurity",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "Jinxhy",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2020-10-18T15:44:18Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-09-10T06:20:32Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "description": [
        {
            "result": {
                "value": "Robustness of on-device Models: AdversarialAttack to Deep Learning Models on Android Apps",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, speech recognition. To make it more accessible to\nend-users, many deep learning models are embedded in mobile apps. \n\nCompared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. __However, most deep learning models within Android apps can be easily obtained via mature reverse engineering, and the model exposure may invite adversarial attacks__. \n\n__In this study, we propose a simple but effective approach to hack deep learning models with adversarial attacks by identifying their highly similar pre-trained models from TensorFlow Hub__. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the model attack, we also carry out an empirical study to investigate the characteristic of deep learning models of hundreds of Android apps from Google Play. The results show that many of them are similar to each other and widely use fine-tuning techniques to pre-trained models on the Internet.\n\nTo demonstrate our task, we first show some common mobile and edge use cases achieved via on-device model inference, as shown in Fig 1.\n\n<p align=\"center\">\n  <img  src=\"figures/use_cases.png\" width=\"95%\" height=\"95%\"><br/>\n  <em>Fig. 1. Optimized on-device deep learning models for common mobile and edge use cases from https://www.tensorflow.org/lite/models.</em>\n</p>\n\nUnlike the central guardians of the cloud server, on-device models may be more vulnerable inside users\u2019 phones. For instance, most model files can be obtained by decompiling Android apps without any obfuscation or encryption. Such model files may be exposed to malicious attacks like adversarial attack. Considering the fact that many mobile apps with deep learning models are used for important tasks such as finance, social or even life-critical tasks like medical, driving-assistant, attacking the models inside those apps will be a disaster for users.\n",
                "type": "Text_excerpt",
                "original_header": "INTRODUCTION",
                "parent_header": [
                    "Robustness of on-device Models: AdversarialAttack to Deep Learning Models on Android Apps"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "We check the similarity of each extracted model in our collection with models in [TensorFlow Hub](https://tfhub.dev/s?deployment-format=lite&module-type=image-classification,image-object-detection,image-pose-detection,image-segmentation,image-style-transfer&subtype=module,placeholder) in terms of the structural and parameter similarity. Fig 4 shows the similarity distribution of models from mobile apps and those from TensorFlow Hub.\n<p align=\"center\">\n  <img  src=\"figures/distribution.png\" width=\"50%\" height=\"50%\"><br/>\n  <em>Fig. 4. Distributions of similarities of fine-tuned and pretrained models.</em>\n</p> \n",
                "original_header": "RQ2: HOW WIDELY PRE-TRAINED TFLITE MODELS ARE ADOPTED?"
            },
            "confidence": 0.967097216071314,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        }
    ],
    "name": [
        {
            "result": {
                "value": "AppAIsecurity",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "Jinxhy/AppAIsecurity",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/Jinxhy/AppAIsecurity/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/Jinxhy/AppAIsecurity/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 16,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 2,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/Jinxhy/AppAIsecurity/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 14677
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "support": [
        {
            "result": {
                "value": "In this work, we design a simple but effective way to adapt existing adversarial attacks to hack the deep learning models in real-world mobile apps. Apart from a pipeline of attacking the deep learning models, we also carry out an empirical study in the usage of deep learning models within thousands of real-world Android apps. We present those results by answering three research questions:\n- How similar are TFLite models used in mobile apps?\n- How widely pre-trained TFLite models are adopted?\n- How robust are fine-tuned TFLite models against adversarial attacks?\n",
                "type": "Text_excerpt",
                "original_header": "EMPIRICAL STUDY OF THE SECURITY OF ON-DEVICE MODELS",
                "parent_header": [
                    "Robustness of on-device Models: AdversarialAttack to Deep Learning Models on Android Apps"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "value": "For the preparation of our study, we crawled 62,822 mobile apps across various categories (e.g., Photograph, Social, Shopping) related to the image domain from Google Play. The identified TFLite deep learning apps and corresponding TFLite models are shown in Fig 2.\n<p align=\"center\">\n  <img  src=\"figures/iden_apps_models.png\" width=\"50%\" height=\"50%\"><br/>\n  <em>Fig. 2. Numbers of TFLite DL apps and models.</em>\n</p>\n\n",
                "type": "Text_excerpt",
                "original_header": "DATASET",
                "parent_header": [
                    "Robustness of on-device Models: AdversarialAttack to Deep Learning Models on Android Apps",
                    "EMPIRICAL STUDY OF THE SECURITY OF ON-DEVICE MODELS"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "For each selected model, according to its functionality, we manually find 10 random images from the Internet as the original input. Fig 7 shows the details of the dataset for one of the selected models.\n<p align=\"center\">\n  <img  src=\"figures/model_input.png\" width=\"100%\" height=\"100%\"><br/>\n  <em>Fig. 7. The original input for mobilenet.letgo.v1 1.0 224 quant.v7.tflite</em>\n</p>\n",
                "type": "Text_excerpt",
                "original_header": "DATASET PREPARING",
                "parent_header": [
                    "Robustness of on-device Models: AdversarialAttack to Deep Learning Models on Android Apps",
                    "RQ3: HOW ROBUST ARE FINE-TUNED TFLITE MODELS AGAINST ADVERSARIAL ATTACKS?"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "```\n@inproceedings{huang2021robustness,\n  title={Robustness of on-device models: Adversarial attack to deep learning models on android apps},\n  author={Huang, Yujin and Hu, Han and Chen, Chunyang},\n  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},\n  pages={101--110},\n  year={2021},\n  organization={IEEE}\n}\n```\n",
                "type": "Text_excerpt",
                "original_header": "CITATION",
                "parent_header": [
                    "Robustness of on-device Models: AdversarialAttack to Deep Learning Models on Android Apps"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "value": "@inproceedings{huang2021robustness,\n    organization = {IEEE},\n    year = {2021},\n    pages = {101--110},\n    booktitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},\n    author = {Huang, Yujin and Hu, Han and Chen, Chunyang},\n    title = {Robustness of on-device models: Adversarial attack to deep learning models on android apps},\n}",
                "type": "Text_excerpt",
                "format": "bibtex",
                "title": "Robustness of on-device models: Adversarial attack to deep learning models on android apps",
                "author": "Huang, Yujin and Hu, Han and Chen, Chunyang"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "Robustness of on-device Models: AdversarialAttack to Deep Learning Models on Android Apps"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        }
    ],
    "images": [
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/use_cases.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/iden_apps_models.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/model_relations.jpg"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/distribution.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/no_fine_tuned.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/selected_models.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/model_input.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/results.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/figures/sim_perf.jpg"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2101.04401"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/Jinxhy/AppAIsecurity/main/README.md"
        }
    ]
}