{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 00:05:41"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/salesforce/CodeTF",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "salesforce",
                "type": "Organization"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2023-05-02T05:05:27Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-10-01T05:35:52Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "license": [
        {
            "result": {
                "value": "https://api.github.com/licenses/apache-2.0",
                "type": "License",
                "name": "Apache License 2.0",
                "url": "https://api.github.com/licenses/apache-2.0",
                "spdx_id": "Apache-2.0"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "description": [
        {
            "result": {
                "value": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLM",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\n </div>   \n    \n## Table of Contents\n  - [Introduction](#introduction)\n  - [Installation](#installation-guide)\n  - [Getting Started](#getting-started)\n    - [Inferencing Pipeline](#inferencing-pipeline)\n    - [Model Zoo](#model-zoo)\n    - [Fine-Tuning Your Own Model](#fine-tuning-pipeline)\n    - [Evaluate On Well-Known Benchmarks](#evaluate-on-well-known-benchmarks)\n    - [Utilities to Manipulate Source Code Based on AST](#code-utilities)\n        - [AST Parser in Multiple Languages](#ast-parser-in-multiple-languages)\n        - [Extract Code Attributes](#extract-code-attributes)\n        - [Remove Comments](#remove-comments)\n  - [Ethical and Responsible Use](#ethical-and-responsible-use) \n  - [License](#license) \n## Introduction\nCodeTF is a one-stop Python transformer-based library for ***code large language models (Code LLMs)*** and ***code intelligence***, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on. It aims to facilitate easy integration of SOTA CodeLLMs into real-world applications. \nIn addition to the core LLMs's features for code, CodeTF offers utilities for code manipulation across various languages, including easy extraction of code attributes. Using tree-sitter as its core AST parser, it enables parsing of attributes such as function names, comments, and variable names. Pre-built libraries for numerous languages are provided, eliminating the need for complicated parser setup. CodeTF thus ensures a user-friendly and accessible environment for code intelligence tasks. \n- **Fast Model Serving**: We support an easy-to-use interface for rapid inferencing with **pre-quantized models** (int8, int16, float16). CodeTF handles all aspects of device management, so users do not have to worry about that aspect. If your model is large, we offer advanced features such as weight sharding across GPUs to serve the models more quickly.\n- **Fine-Tuning Your Own Models**: We provide an API for quickly fine-tuning your own LLMs for code using SOTA techniques for **parameter-efficient fine-tuning** (HuggingFace PEFT) on distributed environments.\n- **Supported Tasks**: nl2code, code summarization, code completion, code translation, code refinement, clone detection, defect prediction.\n- **Datasets+**: We have preprocessed well-known benchmarks (**Human-Eval, MBPP, CodeXGLUE, APPS, etc.**) and offer an easy-to-load feature for these datasets.\n- **Model Evaluator**: We provide interface to evaluate models on well-known benchmarks (e.g. Human-Eval) on popular metrics (e.g., pass@k) with little effort (**~15 LOCs**).\n- **Pretrained Models**: We supply pretrained checkpoints of state-of-the-art foundational language models of code (CodeBERT, CodeT5, CodeGen, CodeT5+, Incoder, StarCoder, etc.).\n- **Fine-Tuned Models**: We furnish fine-tuned checkpoints for 8+ downstream tasks.\n- **Utility to Manipulate Source Code**: We provide utilities to easily manipulate source code, such as user-friendly AST parsers (based on tree-sitter) in **15+ programming languages**, to extract important code features, such as function name, identifiers, etc. \n## Getting Started\n### Inferencing Pipeline\n    \nGetting started with CodeTF is simple and quick with our model loading pipeline function ``load_model_pipeline()``. Here's an example showing how to load codet5+ model and perform inference on code generation task:\n    \n```python\nfrom codetf.models import load_model_pipeline\n\ncode_generation_model = load_model_pipeline(model_name=\"codet5\", task=\"pretrained\",\n            model_type=\"plus-770M-python\", is_eval=True,\n            load_in_8bit=True, load_in_4bit=False, weight_sharding=False)\n            \nresult = code_generation_model.predict([\"def print_hello_world():\"])\nprint(result)\n```\nThere are a few notable arguments that need to be considered:\n-  ``model_name``: the name of the model, currently support ``codet5`` and ``causal-lm``. \n-  ``model_type``: type of model for each model name, e.g. ``base``, ``codegen-350M-mono``, ``j-6B``, etc.\n-  ``load_in_8bit`` and ``load_in_4bit``: inherit the dynamic quantization feature from [Huggingface Quantization](https://huggingface.co/docs/transformers/main/main_classes/quantization).\n-  ``weight_sharding``: our advance feature that leverages [HuggingFace Sharded Checkpoint](https://huggingface.co/docs/accelerate/v0.19.0/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch) to split a large model in several smaller shards in different GPUs. Please consider using this if you are dealing with large models. \n### Code Utilities\nIn addition to providing utilities for LLMs, CodeTF also equips users with tools for effective source code manipulation. This is crucial in the code intelligence pipeline, where operations like parsing code into an Abstract Syntax Tree (AST) or extracting code attributes (such as function names or identifiers) are often required (CodeT5). These tasks can be challenging to execute, especially when setup and multi-language support is needed. Our code utility interface offers a streamlined solution, facilitating easy parsing and attribute extraction from code across 15+ languages. \nNote that this is an ongoing process, we will add more features to extract complicated code attributes in the future. More examples can be found [here](https://github.com/salesforce/CodeTF/tree/main/test_code_utilities). \n## Notes\n- CodeTF is designed to complement and enhance the capabilities of [HuggingFace Transformers](https://huggingface.co/docs/transformers/index), rather than replace it. It serves as a specialized layer specifically tailored for code intelligence tasks, such as fine-tuning language models with code-specific features and evaluating on well-known code intelligence benchmarks. If users require more customization, they are encouraged to write their own training code from scratch.\n- CodeTF leverages the powerful functionality provided by [Accelerate](https://github.com/huggingface/accelerate) for both inference and training. With Accelerate, users do not need to manually manage GPUs or CPU devices for most operations, allowing for a streamlined and efficient workflow. \n## Ethical and Responsible Use\nCodeTF, while powerful, does not guarantee infallible code intelligence capabilities. Users may encounter inaccuracies or biases, possibly leading to misinterpretations or undesired behaviors. Risks include the generation of insecure code, propagation of poor coding practices, or inadvertent revelation of sensitive data. We strongly advise users to examine the pretrained models and system before practical adoption. CodeTF facilitates effective code analysis, prediction, and debugging, promoting reproducible research and development. We encourage its responsible use for enhancing software quality and developer productivity. \nHowever, misuse can lead to unethical outcomes such as unauthorized code manipulation, privacy breaches, or insecure coding practices. Users should familiarize themselves with guidelines for responsible AI before using CodeTF. Our commitment is to continually refine the library by identifying and mitigating potential biases and inappropriate behaviors. Users should review the models and system before practical implementation, and contribute towards refining the library to ensure ethical usage. \n"
            },
            "confidence": 0.9668197920468735,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md"
        }
    ],
    "name": [
        {
            "result": {
                "value": "CodeTF",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "salesforce/CodeTF",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/salesforce/CodeTF/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/salesforce/CodeTF/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 1453,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "ai4code, ai4se, code-generation, code-intelligence, code-learning-datasets, code-representation-learning, code-understanding, human-eval, multilingual-parsers, transformers, tree-sitter",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 100,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/salesforce/CodeTF/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 104993
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "releases": [
        {
            "result": {
                "type": "Release",
                "value": "https://api.github.com/repos/salesforce/CodeTF/releases/106063697",
                "tag": "v1.0.1.1",
                "name": "First release",
                "author": {
                    "name": "bdqnghi",
                    "type": "User"
                },
                "tarball_url": "https://api.github.com/repos/salesforce/CodeTF/tarball/v1.0.1.1",
                "zipball_url": "https://api.github.com/repos/salesforce/CodeTF/zipball/v1.0.1.1",
                "html_url": "https://github.com/salesforce/CodeTF/releases/tag/v1.0.1.1",
                "url": "https://api.github.com/repos/salesforce/CodeTF/releases/106063697",
                "release_id": 106063697,
                "date_created": "2023-06-03T04:18:25Z",
                "date_published": "2023-06-03T04:23:43Z"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "type": "Release",
                "value": "https://api.github.com/repos/salesforce/CodeTF/releases/111216645",
                "tag": "latest",
                "name": "CodeTF v1.0.2.2",
                "author": {
                    "name": "bdqnghi",
                    "type": "User"
                },
                "description": "- Fix issue of missing codetf.common\r\n- Other bugs related to trainer\r\n- Add flag to control if a model can be sharded or not",
                "tarball_url": "https://api.github.com/repos/salesforce/CodeTF/tarball/latest",
                "zipball_url": "https://api.github.com/repos/salesforce/CodeTF/zipball/latest",
                "html_url": "https://github.com/salesforce/CodeTF/releases/tag/latest",
                "url": "https://api.github.com/repos/salesforce/CodeTF/releases/111216645",
                "release_id": 111216645,
                "date_created": "2023-06-03T04:18:25Z",
                "date_published": "2023-07-06T08:08:46Z"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "code_of_conduct": [
        {
            "result": {
                "value": "# Salesforce Open Source Community Code of Conduct\n\n## About the Code of Conduct\n\nEquality is a core value at Salesforce. We believe a diverse and inclusive\ncommunity fosters innovation and creativity, and are committed to building a\nculture where everyone feels included.\n\nSalesforce open-source projects are committed to providing a friendly, safe, and\nwelcoming environment for all, regardless of gender identity and expression,\nsexual orientation, disability, physical appearance, body size, ethnicity, nationality, \nrace, age, religion, level of experience, education, socioeconomic status, or \nother similar personal characteristics.\n\nThe goal of this code of conduct is to specify a baseline standard of behavior so\nthat people with different social values and communication styles can work\ntogether effectively, productively, and respectfully in our open source community.\nIt also establishes a mechanism for reporting issues and resolving conflicts.\n\nAll questions and reports of abusive, harassing, or otherwise unacceptable behavior\nin a Salesforce open-source project may be reported by contacting the Salesforce\nOpen Source Conduct Committee at ossconduct@salesforce.com.\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of gender \nidentity and expression, sexual orientation, disability, physical appearance, \nbody size, ethnicity, nationality, race, age, religion, level of experience, education, \nsocioeconomic status, or other similar personal characteristics.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy toward other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Personal attacks, insulting/derogatory comments, or trolling\n* Public or private harassment\n* Publishing, or threatening to publish, others' private information\u2014such as\na physical or electronic address\u2014without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n* Advocating for or encouraging any of the above behaviors\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned with this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project email\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the Salesforce Open Source Conduct Committee \nat ossconduct@salesforce.com. All complaints will be reviewed and investigated \nand will result in a response that is deemed necessary and appropriate to the \ncircumstances. The committee is obligated to maintain confidentiality with \nregard to the reporter of an incident. Further details of specific enforcement \npolicies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership and the Salesforce Open Source Conduct \nCommittee.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][contributor-covenant-home],\nversion 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html. \nIt includes adaptions and additions from [Go Community Code of Conduct][golang-coc], \n[CNCF Code of Conduct][cncf-coc], and [Microsoft Open Source Code of Conduct][microsoft-coc].\n\nThis Code of Conduct is licensed under the [Creative Commons Attribution 3.0 License][cc-by-3-us].\n\n[contributor-covenant-home]: https://www.contributor-covenant.org (https://www.contributor-covenant.org/)\n[golang-coc]: https://golang.org/conduct\n[cncf-coc]: https://github.com/cncf/foundation/blob/master/code-of-conduct.md\n[microsoft-coc]: https://opensource.microsoft.com/codeofconduct/\n[cc-by-3-us]: https://creativecommons.org/licenses/by/3.0/us/",
                "type": "File_dump"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/CODE_OF_CONDUCT.md"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "has_script_file": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/salesforce/CodeTF/main/docs/build_docs.sh",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "application_domain": [
        {
            "result": {
                "type": "String",
                "value": "Natural Language Processing"
            },
            "confidence": 0.9266666666666667,
            "technique": "supervised_classification"
        }
    ],
    "installation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "\n    \n<p align=\"center\">\n    <br>\n    <img src=\"assets/logo.png\" width=\"500\"/>\n    <br>\n<p>\n<div align=\"center\">\n  <a href=\"https://opensource.org/license/apache-2-0/\">\n  <img alt=\"license\" src=\"https://img.shields.io/badge/License-Apache%202.0-green.svg\"/>\n  </a>\n   <a href=\"https://www.python.org/downloads/release/python-380/\">\n  <img alt=\"python\" src=\"https://img.shields.io/badge/python-3.8+-yellow.svg\"/>\n  </a> \n   <a href=\"https://pypi.org/project/salesforce-codetf/\">\n  <img alt=\"downloads\" src=\"https://static.pepy.tech/badge/salesforce-codetf\"/>\n  </a>  \n<a href=\"https://arxiv.org/pdf/2306.00029.pdf\">Technical Report</a>,\n<a href=\"https://opensource.salesforce.com/CodeTF/latest/index.html\">Documentation</a>,\n<a href=\"https://github.com/salesforce/CodeTF/tree/main/test_inference\">Examples</a>,\n    \n# CodeTF - A One-stop Transformer Library for State-of-the-art Code LLM \n\n## Installation Guide \n1. (Optional) Creating conda environment\n```bash\nconda create -n codetf python=3.8\nconda activate codetf\n```\n \n2. Install from [PyPI](https://pypi.org/project/salesforce-codetf/):\n```bash\npip install salesforce-codetf\n```\n    \n3. Alternatively, build CodeTF from source:\n```bash\ngit clone https://github.com/salesforce/CodeTF.git\ncd CodeTF\npip install -e .\n```\n \nAdditionally, to make sure the quantization feature works well, also install these dependencies:\n```bash\npip install -q -U git+https://github.com/huggingface/transformers.git\npip install -q -U git+https://github.com/huggingface/peft.git\npip install -q -U git+https://github.com/huggingface/accelerate.git\n``` \nComparing to [this script from HuggingFace](https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/human_eval.py), which requires ~230 LOCs to evaluate on pass@k, we only need 14 LOCs to do the same !!! \n"
            },
            "confidence": 0.9869824790197725,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md"
        }
    ],
    "invocation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "### Fine-Tuning Pipeline\nWant to train a custom LLM for code? We've got you covered. Below is an example using the ``Seq2SeqTrainer`` to fine-tune a [CodeT5+ pretrained model](https://github.com/salesforce/CodeT5), along with our dataset utilities, make it easy to fine-tune your models using the CodeXGLUE dataset. Here's an example:\n    \n```python\nfrom codetf.trainer.codet5_trainer import CodeT5Seq2SeqTrainer\nfrom codetf.data_utility.codexglue_dataset import CodeXGLUEDataset\nfrom codetf.models import load_model_pipeline\nfrom codetf.performance.evaluation_metric import EvaluationMetric\nfrom codetf.data_utility.base_dataset import CustomDataset\n\nmodel_class = load_model_pipeline(model_name=\"codet5\", task=\"pretrained\",\n            model_type=\"plus-220M\", is_eval=True)\n\ndataset = CodeXGLUEDataset(tokenizer=model_class.get_tokenizer())\ntrain, test, validation = dataset.load(subset=\"text-to-code\")\n\ntrain_dataset= CustomDataset(train[0], train[1])\ntest_dataset= CustomDataset(test[0], test[1])\nval_dataset= CustomDataset(validation[0], validation[1])\n\nevaluator = EvaluationMetric(metric=\"bleu\", tokenizer=model_class.tokenizer)\n\n# peft can be in [\"lora\", \"prefixtuning\"]\ntrainer = CodeT5Seq2SeqTrainer(train_dataset=train_dataset, \n                                validation_dataset=val_dataset, \n                                peft=\"lora\",\n                                pretrained_model_or_path=model_class.get_model(),\n                                tokenizer=model_class.tokenizer)\ntrainer.train()\n``` \n### Loading Preprocessed Data\nCodeTF provides the Dataset utility for several well-known datasets, such as CodeXGLUE, Human Eval, MBPP, and APPS. The following is an example of how to load the CodeXGLUE dataset:  \n```python\nfrom codetf.data_utility.codexglue_dataset import CodeXGLUEDataset\nfrom transformers import RobertaTokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-base\", use_fast=True)\ndataset = CodeXGLUEDataset(tokenizer=tokenizer)\ntrain, test, validation = dataset.load(subset=\"text-to-code\")\n```\n \n"
            },
            "confidence": 0.948045261462329,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "@misc{nghi2023codetf,\n    primaryclass = {cs.CV},\n    archiveprefix = {arXiv},\n    eprint = {2209.09019},\n    year = {2023},\n    author = {Nghi D. Q. Bui, Henry Le, Yue Wang, Akhilesh Deepak Gotmare, Junnan Li, Steven Hoi.},\n    title = {CodeTF: A Transformer-based Library for CodeLLM & Code Intelligence},\n}",
                "type": "Text_excerpt",
                "format": "bibtex",
                "title": "CodeTF: A Transformer-based Library for CodeLLM & Code Intelligence",
                "author": "Nghi D. Q. Bui, Henry Le, Yue Wang, Akhilesh Deepak Gotmare, Junnan Li, Steven Hoi."
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": ""
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md"
        }
    ],
    "logo": [
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/salesforce/CodeTF/main/assets/logo.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/pdf/2306.00029.pdf\">Technical Report</a>,\n<a href=\"https://opensource.salesforce.com/CodeTF/latest/index.html\">Documentation</a>,\n<a href=\"https://github.com/salesforce/CodeTF/tree/main/test_inference\">Examples</a>,\n    \n# CodeTF - A One-stop Transformer Library for State-of-the-art Code LLM\n\n\n </div>   \n    \n## Table of Contents\n  - [Introduction](#introduction"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2306.00029"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/salesforce/CodeTF/main/README.md"
        }
    ]
}