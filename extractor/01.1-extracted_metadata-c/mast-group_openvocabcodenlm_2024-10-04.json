{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 18:27:17"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/mast-group/OpenVocabCodeNLM",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "mast-group",
                "type": "Organization"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2018-10-11T10:30:53Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-01-04T16:26:59Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "license": [
        {
            "result": {
                "value": "https://api.github.com/licenses/apache-2.0",
                "type": "License",
                "name": "Apache License 2.0",
                "url": "https://api.github.com/licenses/apache-2.0",
                "spdx_id": "Apache-2.0"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n",
                "type": "File_dump"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/LICENSE"
        }
    ],
    "description": [
        {
            "result": {
                "value": "Contains the code for our ICSE 2020 paper: Big Code != Big Vocabulary: Open-Vocabulary Language Models for Source Code and for its earlier pre-print: Maybe Deep Neural Networks are the Best Choice for Modeling Source Code (https://arxiv.org/abs/1903.05734). This is the first open vocabulary language model for code that uses the byte pair encoding algorithm (BPE) to learn a segmentation of code tokens into subword units. ",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Contains the code for our ICSE 2020 submission: open vocabulary language model for source code that uses the byte pair encoding algorithm to learn a segmentation of code tokens into subtokens.  \n",
                "original_header": "OpenVocabNLMs"
            },
            "confidence": 0.9600711552258837,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "**create_subtoken_data.py** is also a preprocessing script that can be used to subtokenize data based on the heuristic of [Allamanis et al. (2015)](https://miltos.allamanis.com/publications/2015suggesting/). \n**reader.py** contains utility functions for reading data and providing batches for training and testing of models. \n**code_nlm.py** contains the implementation of our NLM for code and supports training, perplexity/cross-entropy calculation, code-completion simulation as well as dynamic versions of the test scenarios. The updated implementation has also some new features, previously not present in the code. That is measuring identifier specific performance for code completion. Another new feature implements a simple n-gram cache for identifiers that better simulates use of the model in an IDE where such information would be present. In order to use the identifier features a file containing identifier information must be provided through the options. \n \n",
                "original_header": "Code Structure"
            },
            "confidence": 0.9564167937575286,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        }
    ],
    "name": [
        {
            "result": {
                "value": "OpenVocabCodeNLM",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "mast-group/OpenVocabCodeNLM",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/mast-group/OpenVocabCodeNLM/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/mast-group/OpenVocabCodeNLM/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 83,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 24,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/mast-group/openvocabcodenlm/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 135517
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "Shell",
                "name": "Shell",
                "type": "Programming_language",
                "size": 3433
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "releases": [
        {
            "result": {
                "type": "Release",
                "value": "https://api.github.com/repos/mast-group/OpenVocabCodeNLM/releases/23180435",
                "tag": "v1.0",
                "name": "First official release",
                "author": {
                    "name": "mpatsis",
                    "type": "User"
                },
                "description": "This is the first official release of our library for training and testing open-vocabulary neural language models for code. Closed-vocabulary models are also supported.\r\nThe library supports entropy calculation, measuring code completion performance on a file / test dataset, dynamic updates, dynamic code completion, measuring identifier completion performance, and an optional identifier cache.\r\n\r\nThe implementation was used for the experiments in the paper: Big Code != Big Vocabulary:Open-Vocabulary Models for Source Code",
                "tarball_url": "https://api.github.com/repos/mast-group/OpenVocabCodeNLM/tarball/v1.0",
                "zipball_url": "https://api.github.com/repos/mast-group/OpenVocabCodeNLM/zipball/v1.0",
                "html_url": "https://github.com/mast-group/OpenVocabCodeNLM/releases/tag/v1.0",
                "url": "https://api.github.com/repos/mast-group/OpenVocabCodeNLM/releases/23180435",
                "release_id": 23180435,
                "date_created": "2020-01-28T11:15:57Z",
                "date_published": "2020-01-28T11:26:00Z"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "has_script_file": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/example.sh",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "Python>2.7.6 or Python==3.6 is required!\nPython>3.6 is not supported due to the tensorflow version not supporting it.\n\n```shell script\ngit clone https://github.com/mast-group/OpenVocabCodeNLM\ncd OpenVocabCodeNLM\npip install -r requirements.txt #python2\npip3 install -r requirements.txt #python3\n```\nThe experiments in the paper were performed using Python 2.7.14 but we have currently not experienced any unresolved issue with Python 3. </br>\nIn case you encounter any issues please open a new issue entry.\n\n",
                "type": "Text_excerpt",
                "original_header": "Installation"
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "The BPE implementation used can be found here: https://github.com/rsennrich/subword-nmt  \n",
                "original_header": "BPE"
            },
            "confidence": 0.9487045671080873,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        }
    ],
    "usage": [
        {
            "result": {
                "value": "If you want to try the implementation unzip the directory containing the sample data.\nThe sample data contain the small training set, validation, and test set used in the paper with a BPE encdoding size of 10000.\n",
                "type": "Text_excerpt",
                "original_header": "Usage Instructions"
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "Let's first define constants for pointing to the data and network parameters. You'll need to modify these to point to your own data and satisfy the hyperparameters that you want to use.\n```\n# Directory that contains train/validation/test data etc.\nDATA_HOME=sample_data/java/\n# Directory in which the model will be saved.\nMODEL_DIR=sample_data/java/model\nmkdir $MODEL_DIR\n\n# Filenames\nTRAIN_FILE=java_training_slp_pre_enc_bpe_10000\nVALIDATION_FILE=java_validation_slp_pre_enc_bpe_10000\nTEST_FILE=java_test_slp_pre_enc_bpe_10000\nTEST_PROJ_NAMES_FILE=testProjects\nID_MAP_FILE=sample_data/java/id_map_java_test_slp_pre_bpe_10000\n\n# Maximum training epochs\nEPOCHS=5 # Normally this would be larger. For instance 30-50\n# Initial learning rate\nLR=0.1 # This is the default value. You can skip it if you don't want to change it.",
                "type": "Text_excerpt",
                "original_header": "Option Constants",
                "parent_header": [
                    "Usage Instructions"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "BATCH_SIZE=32 # This is also the default.\n# RNN unroll timesteps for gradient calculation.\nSTEPS=20 # 20-50 is a good range of values for dynamic experiments.\n# 1 - Dropout probability\nKEEP_PROB=0.5 # This is also the default.\n# RNN hidden state size\nSTATE_DIMS=512 # This is also the default.\n# Checkpoint and validation loss calculation frequency.\nCHECKPOINT_EVERY=5000 # This is also the default.\n\n\n# Understanding boolean options.\n# Most boolean options are set to False  by default.\n# For using any boolean option set it to True.\n# For instance for using a GRU instead of an LSTM add to your command the option --gru True.\n```\n\n\nWe next present the various scenarios supported by our implementation.\n\n## Training\nThe training scenario creates a global model by training on the provided to it training data.\nWe will train a Java model with a BPE encoding of 10000 using the sample data.\nIn the following training example we set some of the hyperparameters (to their default values though).\nOptionally, you can set all of them to your intented values.\nSince the data is tokenized into subwords we need to let the script know so that it can calculate the metrics correctly.\nFor this reason we need to set the *word_level_perplexity* flag to **True**.\nIn order to also output validation cross-entropy instead of perplexity we set the *cross_entropy* option to **True**.\n\n```\n# Train a small java model for 1 epoch.\npython code_nlm.py --data_path $DATA_HOME --train_dir $MODEL_DIR --train_filename $TRAIN_FILE --validation_filename $VALIDATION_FILE --gru True --hidden_size $STATE_DIMS  --batch_size $BATCH_SIZE --word_level_perplexity True --cross_entropy True --steps_per_checkpoint $CHECKPOINT_EVERY --max_epoch $EPOCHS\n\n# Because we are using the default values we could shorten the above command to:\n# python code_nlm.py --data_path $DATA_HOME --train_dir $MODEL_DIR --train_filename $TRAIN_FILE --validation_filename $VALIDATION_FILE --gru True --word_level_perplexity True --cross_entropy True --max_epoch $EPOCHS\n```\n",
                "type": "Text_excerpt",
                "original_header": "Training",
                "parent_header": [
                    "Usage Instructions"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "```\n# Testing the model (Calculating test set entropy) \npython code_nlm.py --test True --data_path $DATA_HOME --train_dir $MODEL_DIR --test_filename $TEST_FILE --gru True --batch_size $BATCH_SIZE --word_level_perplexity True --cross_entropy True\n```\n",
                "type": "Text_excerpt",
                "original_header": "Test Entropy Calculation",
                "parent_header": [
                    "Usage Instructions",
                    "Test Scenarios"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "In order to dynamically adapt the model, the implementation needs to know when it is testing on a new project, so that it can revert the model back to the global one.\nThis is achieved via the *test_proj_filename* option.\n```\n# Batch size must always be set to 1 for this scenario! We are going through every file seperately.\n# In an IDE this could instead be sped up through some engineering.\npython code_nlm.py --dynamic_test True --data_path $DATA_HOME --train_dir $MODEL_DIR --test_filename $TEST_FILE --gru True --batch_size 1 --word_level_perplexity True --cross_entropy True --test_proj_filename $TEST_PROJ_NAMES_FILE --num_steps $STEPS\n```\n",
                "type": "Text_excerpt",
                "original_header": "Dynamically Adapt the Model on Test Data",
                "parent_header": [
                    "Usage Instructions",
                    "Test Scenarios",
                    "Test Entropy Calculation"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "In this scenario the *batch_size* option is used to set the beam size.\n```\npython code_nlm.py --completion True --data_path $DATA_HOME --train_dir $MODEL_DIR --test_filename $TEST_FILE --gru True --batch_size $BATCH_SIZE\n```\n",
                "type": "Text_excerpt",
                "original_header": "Test Code Completion",
                "parent_header": [
                    "Usage Instructions",
                    "Test Scenarios"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "Similarly to before we need to set the *test_proj_filename* option.\n```\npython code_nlm.py --completion True --dynamic True --data_path $DATA_HOME --train_dir $MODEL_DIR --test_filename $TEST_FILE --gru True --batch_size $BATCH_SIZE  --test_proj_filename $TEST_PROJ_NAMES_FILE --num_steps $STEPS\n```\n",
                "type": "Text_excerpt",
                "original_header": "Dynamic Code Completion on Test Data",
                "parent_header": [
                    "Usage Instructions",
                    "Test Scenarios",
                    "Test Code Completion"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "To run this experiment you need to provide a file containing a mapping that lets the implementation know for each subtoken whether it is part of an identifier or not.\nThis information would easily be present in an IDE.\nThe mapping is provided via the *identifier_map* option.\n```\npython code_nlm.py --completion True --dynamic True --data_path $DATA_HOME --train_dir $MODEL_DIR --test_filename $TEST_FILE --gru True --batch_size $BATCH_SIZE  --test_proj_filename $TEST_PROJ_NAMES_FILE --identifier_map $ID_MAP_FILE --num_steps $STEPS\n```\n",
                "type": "Text_excerpt",
                "original_header": "Dynamic Code Completion on Test Data and Measuring Identifier Specific Performance",
                "parent_header": [
                    "Usage Instructions",
                    "Test Scenarios",
                    "Test Code Completion"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "In an IDE setting we could improve the performance on identifiers by utilizing  a simple n-gram cache for identifiers that we have already encountered.\nThe *file_cache_weight* and  *cache_order* options can be used to control the cache's weight and the cache's order respectively.\nBy default we use a 6-gram with a weight of 0.2.\n```\npython code_nlm.py --completion True --dynamic True --data_path $DATA_HOME --train_dir $MODEL_DIR --test_filename $TEST_FILE --gru True --batch_size $BATCH_SIZE --test_proj_filename $TEST_PROJ_NAMES_FILE --identifier_map $ID_MAP_FILE --cache_ids True --num_steps $STEPS\n```\n",
                "type": "Text_excerpt",
                "original_header": "Adding a Simple Identifier n-gram Cache",
                "parent_header": [
                    "Usage Instructions",
                    "Test Scenarios",
                    "Test Code Completion"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        },
        {
            "result": {
                "value": "Similar to testing but calculates the average entropy of the files instead of the per token one.\n\n\n",
                "type": "Text_excerpt",
                "original_header": "Predictability",
                "parent_header": [
                    "Usage Instructions",
                    "Test Scenarios"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "OpenVocabNLMs"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/mast-group/openvocabcodenlm/master/README.md"
        }
    ]
}