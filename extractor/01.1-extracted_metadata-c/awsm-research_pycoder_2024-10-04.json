{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 18:27:55"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/awsm-research/pycoder",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "awsm-research",
                "type": "Organization"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2023-03-21T04:21:54Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-03-16T20:20:11Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "name": [
        {
            "result": {
                "value": "pycoder",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "awsm-research/pycoder",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/awsm-research/pycoder/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/awsm-research/pycoder/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 8,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 3,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/awsm-research/pycoder/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Jupyter Notebook",
                "name": "Jupyter Notebook",
                "type": "Programming_language",
                "size": 153918
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 113581
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "value": "Shell",
                "name": "Shell",
                "type": "Programming_language",
                "size": 347
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "executable_example": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/awsm-research/pycoder/main/line-level/code/create_line_gt.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/line-level/code/create_line_gt.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/awsm-research/pycoder/main/assets/notebooks/results.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/assets/notebooks/results.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/awsm-research/pycoder/main/assets/notebooks/evaluation_metrics.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/assets/notebooks/evaluation_metrics.ipynb"
        },
        {
            "result": {
                "value": "https://raw.githubusercontent.com/awsm-research/pycoder/main/assets/notebooks/inference.ipynb",
                "type": "Url",
                "format": "jupyter_notebook"
            },
            "confidence": 1,
            "technique": "file_exploration",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/assets/notebooks/inference.ipynb"
        }
    ],
    "has_script_file": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/awsm-research/pycoder/main/token-level/dataset/py150/download_and_extract.sh",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "requirements": [
        {
            "result": {
                "value": "- python 3.6 or 3.7\n- torch==1.4.0\n- transformers>=2.5.0 and < 4.0.0\n- fuzzywuzzy\n",
                "type": "Text_excerpt",
                "original_header": "Dependency",
                "parent_header": [
                    "PyCoder",
                    "Setting Up"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "If you use our code or PyCoder, please cite our [PyCoder paper](https://arxiv.org/abs/2211.04673).\n\nFor PyCoder:\n\n<pre><code>@article{takerngsaksiri2022syntax,\n  title={Syntax-Aware On-the-Fly Code Completion},\n  author={Takerngsaksiri, Wannita and Tantithamthavorn, Chakkrit and Li, Yuan-Fang},\n  journal={arXiv preprint arXiv:2211.04673},\n  year={2022}\n}</code></pre>\n\n\nAdditionally, please also cite the following papers in addition to our PyCoder.\n\nFor CodeXGLUE:\n\n<pre><code>@article{DBLP:journals/corr/abs-2102-04664,\n  author    = {Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin B. Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},\n  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding\n               and Generation},\n  journal   = {CoRR},\n  volume    = {abs/2102.04664},\n  year      = {2021}\n}</code></pre>\n\nFor PY150 dataset:\n\n<pre><code>@article{raychev2016probabilistic,\n  title={Probabilistic Model for Code with Decision Trees},\n  author={Raychev, Veselin and Bielik, Pavol and Vechev, Martin},\n  journal={ACM SIGPLAN Notices},\n  pages={731--747},\n  year={2016},\n  publisher={ACM New York, NY, USA}\n}</code></pre>\n\n",
                "type": "Text_excerpt",
                "original_header": "Reference",
                "parent_header": [
                    "PyCoder"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        }
    ],
    "description": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This repository contains the code for the paper [Syntax-Aware On-the-Fly Code Completion](https://arxiv.org/abs/2211.04673) \n",
                "original_header": "PyCoder"
            },
            "confidence": 0.9778160524883492,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "We use PY150 in our experiments and follow the data splitting from CodeXGLUE. Data statistics of PY150 dataset are shown in the below tables \n",
                "original_header": "Dataset"
            },
            "confidence": 0.9665613911924662,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "To download and preprocess the dataset by yourself, navigate to `token-level/dataset/py150` directory, and run\n```shell\nbash download_and_extract.sh\npython type_extract_and_preprocess.py --base_dir=py150_files --output_dir=token_completion\npython type_alignment.py --base_dir=token_completion --output=token_completion\n```\n \n",
                "original_header": "Dataset"
            },
            "confidence": 0.9996130155227444,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "To fine-tune (the example is PyCoder-Hard), navigate to `token-level/code` directory, run:\n```\nLANG=python          \nDATADIR=../dataset/py150/token_completion\nLITFILE=../dataset/py150/literals.json\nOUTPUTDIR=../save/<output_dir> # model saved here\nPRETRAINDIR=microsoft/CodeGPT-small-py\nLOGFILE=<log_dir>.log # log file saved here\nPER_NODE_GPU=1\n\npython -m torch.distributed.launch --nproc_per_node=$PER_NODE_GPU run_pycoder.py \\\n        --data_dir=$DATADIR \\\n        --lit_file=$LITFILE \\\n        --langs=$LANG \\\n        --output_dir=$OUTPUTDIR \\\n        --pretrain_dir=$PRETRAINDIR \\\n        --log_file=$LOGFILE \\\n        --model_type=gpt2 \\\n        --block_size=1024 \\\n        --do_train \\\n        --gpu_per_node $PER_NODE_GPU \\\n        --learning_rate=8e-5 \\\n        --weight_decay=0.01 \\\n        --evaluate_during_training \\\n        --per_gpu_train_batch_size=2 \\\n        --per_gpu_eval_batch_size=4 \\\n        --gradient_accumulation_steps=4 \\\n        --num_train_epochs=15 \\\n        --logging_steps=100 \\\n        --save_steps=1000 \\\n        --seed=42 \\\n        --overwrite_output_dir \\\n        --not_pretrain \\\n        --train_mode=both \\\n        --model_amount=1 \\\n        --loss_weight_value=0.9\n```\n \n",
                "original_header": "Training"
            },
            "confidence": 0.9780431946890885,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "```\nLANG=python                      \nDATADIR=../dataset/py150/token_completion\nLITFILE=../dataset/py150/literals.json\nOUTPUTDIR=../save/<output_dir> # predictions saved here\nPRETRAINDIR=../save/<model_dir>/<checkpoint_folder> #  directory of your saved model\nLOGFILE=<log_dir>.log # log file saved here\n\npython -u run_pycoder.py \\\n        --data_dir=$DATADIR \\\n        --lit_file=$LITFILE \\\n        --langs=$LANG \\\n        --output_dir=$OUTPUTDIR \\\n        --pretrain_dir=$PRETRAINDIR \\\n        --log_file=$LOGFILE \\\n        --model_type=gpt2 \\\n        --block_size=1024 \\\n        --do_eval \\\n        --per_gpu_eval_batch_size=4 \\\n        --logging_steps=100 \\\n        --seed=42 \\\n        --predict_mode=code \n```\n \n",
                "original_header": "Token-level"
            },
            "confidence": 0.9987763223562827,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "```\nLANG=python                      \nDATADIR=../dataset/py150/line_completion\nLITFILE=../dataset/py150/literals.json\nOUTPUTDIR=../../line-level/save/<output_dir> # predictions saved here\nPRETRAINDIR=../../token-level/save/<model_dir>/<checkpoint_folder> # directory of your saved model\nLOGFILE=<log_dir>.log # log file saved here\n\npython -u run_pycoder_line.py \\\n        --data_dir=$DATADIR \\\n        --lit_file=$LITFILE \\\n        --langs=$LANG \\\n        --output_dir=$OUTPUTDIR \\\n        --pretrain_dir=$PRETRAINDIR \\\n        --log_file=$LOGFILE \\\n        --model_type=gpt2 \\\n        --block_size=1024 \\\n        --eval_line \\\n        --logging_steps=100 \\\n        --seed=42 \\\n        --predict_mode=code \\\n        --calculate_mrr\n```\n \n",
                "original_header": "Line-level"
            },
            "confidence": 0.9867631441488199,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        }
    ],
    "invocation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "To download and preprocess the dataset by yourself, navigate to `token-level/dataset/py150` directory, and run\n```shell\nbash download_and_extract.sh\npython type_extract_and_preprocess.py --base_dir=py150_files --output_dir=token_completion\npython type_alignment.py --base_dir=token_completion --output=token_completion\n```\n \n",
                "original_header": "Dataset"
            },
            "confidence": 0.948763739471822,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "To fine-tune (the example is PyCoder-Hard), navigate to `token-level/code` directory, run:\n```\nLANG=python          \nDATADIR=../dataset/py150/token_completion\nLITFILE=../dataset/py150/literals.json\nOUTPUTDIR=../save/<output_dir> # model saved here\nPRETRAINDIR=microsoft/CodeGPT-small-py\nLOGFILE=<log_dir>.log # log file saved here\nPER_NODE_GPU=1\n\npython -m torch.distributed.launch --nproc_per_node=$PER_NODE_GPU run_pycoder.py \\\n        --data_dir=$DATADIR \\\n        --lit_file=$LITFILE \\\n        --langs=$LANG \\\n        --output_dir=$OUTPUTDIR \\\n        --pretrain_dir=$PRETRAINDIR \\\n        --log_file=$LOGFILE \\\n        --model_type=gpt2 \\\n        --block_size=1024 \\\n        --do_train \\\n        --gpu_per_node $PER_NODE_GPU \\\n        --learning_rate=8e-5 \\\n        --weight_decay=0.01 \\\n        --evaluate_during_training \\\n        --per_gpu_train_batch_size=2 \\\n        --per_gpu_eval_batch_size=4 \\\n        --gradient_accumulation_steps=4 \\\n        --num_train_epochs=15 \\\n        --logging_steps=100 \\\n        --save_steps=1000 \\\n        --seed=42 \\\n        --overwrite_output_dir \\\n        --not_pretrain \\\n        --train_mode=both \\\n        --model_amount=1 \\\n        --loss_weight_value=0.9\n```\n \n",
                "original_header": "Training"
            },
            "confidence": 0.923000756846711,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "```\nLANG=python                      \nDATADIR=../dataset/py150/token_completion\nLITFILE=../dataset/py150/literals.json\nOUTPUTDIR=../save/<output_dir> # predictions saved here\nPRETRAINDIR=../save/<model_dir>/<checkpoint_folder> #  directory of your saved model\nLOGFILE=<log_dir>.log # log file saved here\n\npython -u run_pycoder.py \\\n        --data_dir=$DATADIR \\\n        --lit_file=$LITFILE \\\n        --langs=$LANG \\\n        --output_dir=$OUTPUTDIR \\\n        --pretrain_dir=$PRETRAINDIR \\\n        --log_file=$LOGFILE \\\n        --model_type=gpt2 \\\n        --block_size=1024 \\\n        --do_eval \\\n        --per_gpu_eval_batch_size=4 \\\n        --logging_steps=100 \\\n        --seed=42 \\\n        --predict_mode=code \n```\n \n",
                "original_header": "Token-level"
            },
            "confidence": 0.9086823319426567,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "```\nLANG=python                      \nDATADIR=../dataset/py150/line_completion\nLITFILE=../dataset/py150/literals.json\nOUTPUTDIR=../../line-level/save/<output_dir> # predictions saved here\nPRETRAINDIR=../../token-level/save/<model_dir>/<checkpoint_folder> # directory of your saved model\nLOGFILE=<log_dir>.log # log file saved here\n\npython -u run_pycoder_line.py \\\n        --data_dir=$DATADIR \\\n        --lit_file=$LITFILE \\\n        --langs=$LANG \\\n        --output_dir=$OUTPUTDIR \\\n        --pretrain_dir=$PRETRAINDIR \\\n        --log_file=$LOGFILE \\\n        --model_type=gpt2 \\\n        --block_size=1024 \\\n        --eval_line \\\n        --logging_steps=100 \\\n        --seed=42 \\\n        --predict_mode=code \\\n        --calculate_mrr\n```\n \n",
                "original_header": "Line-level"
            },
            "confidence": 0.905399907783836,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "PyCoder"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        }
    ],
    "images": [
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/awsm-research/pycoder/main/assets/images/overview.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2211.04673"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/awsm-research/pycoder/main/README.md"
        }
    ]
}