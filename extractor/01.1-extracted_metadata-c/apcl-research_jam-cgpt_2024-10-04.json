{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 18:25:03"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/apcl-research/Jam-CGPT",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "apcl-research",
                "type": "Organization"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2023-08-10T14:30:59Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-02-05T21:44:11Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "name": [
        {
            "result": {
                "value": "Jam-CGPT",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "apcl-research/Jam-CGPT",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/apcl-research/Jam-CGPT/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/apcl-research/Jam-CGPT/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 2,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 1,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/apcl-research/jam-cgpt/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 123453
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "download": [
        {
            "result": {
                "value": "You can download all of the datasets in our paper in our [Hugginface repo](https://huggingface.co/datasets/apcl/Jam-CGPT/tree/main). Please put ``train.bin`` and  ``val.bin`` to the same ``dir`` as ``--dataset`` in ``config/finetune_model_350m_dataset_170k.py``. \n",
                "type": "Text_excerpt",
                "original_header": "Step 1: Download the finetuning dataset",
                "parent_header": [
                    "Jam-CGPT: Distilled GPT for Source Code Summarization",
                    "Finetuning"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        },
        {
            "result": {
                "value": "Please download the checkpoint files named ``ckpt_pretrain.pt`` in our [Hugginface repo](https://huggingface.co/apcl/Jam-CGPT/tree/main) for finetuning and put the checkpoint to the same  ``dir`` as ``--out_dir`` in ``config/finetune_model_350m_dataset_170k.py``\n",
                "type": "Text_excerpt",
                "original_header": "Step 2: Download the models for finetuning",
                "parent_header": [
                    "Jam-CGPT: Distilled GPT for Source Code Summarization",
                    "Finetuning"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        }
    ],
    "citation": [
        {
            "result": {
                "value": "This work was accepted to [Automated Software Engineering](https://link.springer.com/journal/10515), an academic journal.  If you use this work in an academic paper, please cite the following:\n```\n@misc{su2024distilled,\n      title={Distilled GPT for Source Code Summarization}, \n      author={Chia-Yi Su and Collin McMillan},\n      year={2024},\n      journal={Automated Software Engineering}\n}\n```\nPreprint PDF available here: https://arxiv.org/abs/2308.14731\n\n",
                "type": "Text_excerpt",
                "original_header": "Citation",
                "parent_header": [
                    "Jam-CGPT: Distilled GPT for Source Code Summarization"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        },
        {
            "result": {
                "value": "@inproceedings{su2023language,\n    series = {ESEC/FSE 2023},\n    location = {San Francisco, CA, USA},\n    booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},\n    year = {2023},\n    month = {December},\n    author = {Chia-Yi Su and Aakash Bansal and Vijayanta Jain and Sepideh Ghanavati and Collin McMillan},\n    title = {A Language Model of Java Methods with Train/Test Deduplication},\n}",
                "type": "Text_excerpt",
                "format": "bibtex",
                "title": "A Language Model of Java Methods with Train/Test Deduplication",
                "author": "Chia-Yi Su and Aakash Bansal and Vijayanta Jain and Sepideh Ghanavati and Collin McMillan"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        },
        {
            "result": {
                "value": "@misc{su2024distilled,\n    journal = {Automated Software Engineering},\n    year = {2024},\n    author = {Chia-Yi Su and Collin McMillan},\n    title = {Distilled GPT for Source Code Summarization},\n}",
                "type": "Text_excerpt",
                "format": "bibtex",
                "title": "Distilled GPT for Source Code Summarization",
                "author": "Chia-Yi Su and Collin McMillan"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        }
    ],
    "description": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "This repository contains all the code and detailed instructions to rebuild [Jam-CGPT](https://huggingface.co/apcl/Jam-CGPT) models in our HuggingFace [Automatic Program Comprehension Lab](https://huggingface.co/apcl) hub.\n \n",
                "original_header": "Code for Distilled GPT for Source Code Summarization"
            },
            "confidence": 0.9883342741040226,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "We also release the config file for pretaining the jam-cgpt 38m model and 110m model --``train_jam_cgpt_raw_38m.py`` and ``train_jam_cgpt_raw_110m.py``. You can find the script for pretraining the 350m model and instructions for pretraining in [Jam repo](https://github.com/apcl-research/jam). Data for pretraining is in our [Hugginface jam repo](https://huggingface.co/apcl/jam). Please cite the use of the dataset as follows:\n \n"
            },
            "confidence": 0.9593393092166763,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "To set up your local environment, run the following command. We recommend the use of a virtual environment for running the experiments.\n```\npip install -r requirements.txt\n```  \n- **If you only want to make an inference with our models**, please see [Inference](#inference).\n- If you want to finetune a model using our processed and tokenized dataset, please see [Finetuning](#finetuning)\n- If you want to recompile our datasets, please see [Dataset](#dataset)\n \n",
                "original_header": "To-do list"
            },
            "confidence": 0.9890006813101797,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "After you download the test set named ``jam_cgpt_test.tar.gz`` in our [Hugginface repo](https://huggingface.co/datasets/apcl/Jam-CGPT/tree/main), you can simiply run command below for inference.\n```\nCUDA_DEVICE_ORDER='PCI_BUS_ID' CUDA_VISIBLE_DEVICES='0' OMP_NUM_THREADS=2 time torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:4000 --nnodes=1 --nproc_per_node=1 sample_jam_cgpt.py config/finetune_model_350m_dataset_170k.py  --prediction_filename=predict_data170k_model350m.txt\n```\n    --outdir: directory of the model that you want to use for inference\n    --prediction_filename: prediction file name \n    --outfilename: checkpoint file name\n    \nNote that you need to download checkpoint files our [Hugginface repo](https://huggingface.co/apcl/Jam-CGPT/tree/main) and put the checkpoint files to the same  ``dir`` as ``--out_dir`` in ``config/finetune_model_350m_dataset_170k.py`` if you just want to make an inference with our models.\n \n",
                "original_header": "Inference"
            },
            "confidence": 0.9999985636455987,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "We also release all of our raw datasets for the experiments in our [Hugginface repo](https://huggingface.co/datasets/apcl/Jam-CGPT/tree/main) and the scripts for compiling the raw data to ``bin`` files in this Github repo. Before running the command, please create three dir: ``pkls``, ``bins``, and ``tmp``. Then, you can simply run the following command to generate ``train.bin`` and ``val.bin``.\n```\npython3 data/jam_cgpt_170k/prepare_fc_raw.py\n```\n- Note that you will need to put ``jam-cgpt-testfid.pkl``, ``jam-cgpt-valfid.pkl``, ``fundats-j1.pkl``, ``jam-cgpt-raw1.25m.pkl``, ``jam-cgpt-raw170k.pkl``, ``jam-cgpt-raw2.15m.pkl``, and ``jam-cgpt-raw620k.pkl`` to /nublar/datasets/jm52m/raw_data or you will need to change the parameters in the script.\n- Related parameters are as follows:\n  \n      --testfids-file: file lcation of function id on testset\n      --valfids-file: file location of function id on valset\n      --fundats-file: file location of function\n      --coms-file: file location of comments\n \n",
                "original_header": "Dataset"
            },
            "confidence": 0.9998604576055732,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        }
    ],
    "invocation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "After you download the test set named ``jam_cgpt_test.tar.gz`` in our [Hugginface repo](https://huggingface.co/datasets/apcl/Jam-CGPT/tree/main), you can simiply run command below for inference.\n```\nCUDA_DEVICE_ORDER='PCI_BUS_ID' CUDA_VISIBLE_DEVICES='0' OMP_NUM_THREADS=2 time torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:4000 --nnodes=1 --nproc_per_node=1 sample_jam_cgpt.py config/finetune_model_350m_dataset_170k.py  --prediction_filename=predict_data170k_model350m.txt\n```\n    --outdir: directory of the model that you want to use for inference\n    --prediction_filename: prediction file name \n    --outfilename: checkpoint file name\n    \nNote that you need to download checkpoint files our [Hugginface repo](https://huggingface.co/apcl/Jam-CGPT/tree/main) and put the checkpoint files to the same  ``dir`` as ``--out_dir`` in ``config/finetune_model_350m_dataset_170k.py`` if you just want to make an inference with our models.\n \n",
                "original_header": "Inference"
            },
            "confidence": 0.9158606572699657,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "Jam-CGPT: Distilled GPT for Source Code Summarization"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2308.14731\n"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/apcl-research/jam-cgpt/main/README.md"
        }
    ]
}