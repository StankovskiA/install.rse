{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 20:34:20"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/aakashba/projcon",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "aakashba",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2021-01-30T20:46:20Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2023-06-05T05:35:45Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "name": [
        {
            "result": {
                "value": "projcon",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "aakashba/projcon",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/aakashba/projcon/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/aakashba/projcon/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 9,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 1,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/aakashba/projcon/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 388421
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/aakashba/projcon/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "We began with the main java dataset of 2.1m methods aswell the complete 50 million method extended set requested from Le Clair et al{http://leclair.tech/data/funcom/}\n\nThe dataset was filtered for duplicates then project context was constructed from the extended set using the scripts in the \"builder\" folder.\n\nWe provide the compiled dataset as well as the scripts used to compile .This data can be found at :\nhttps://drive.google.com/drive/folders/10r-I5C76e-1vZuY5gqYQQNKgAY5MOJpg?usp=sharing\n\n",
                "type": "Text_excerpt",
                "original_header": "Step 0 - Dataset building",
                "parent_header": [
                    "Guide for replication of results for paper \"Project-Level Encoding for Neural Source Code Summarization of Subroutines\", accepted at ICPC 2021, Madrid, Spain (Virtual due to pandemic)."
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aakashba/projcon/main/README.md"
        },
        {
            "result": {
                "value": "To ensure no recursive errors or edits, create directories nfs>projects and clone this git repository.\nDownload and unpack all data from the aws link into this directory as well.\nCreate directory outdir, with 4 subdirectories  **outdir/{models, histories, viz, predictions}**\n**Use Requirements.txt to get your python 3.x virtual environment in sync with our setup.** Venv is preferred. Common issues that might arise from updating an existing venv and solutions :\n- GPU not recognized: checking the compatibility of your gpu cudnn/cuda or other drivers with the keras and tf versions fixes this.\n- Tf unable to allocate tensor: uninstall tensorflow and then update tensorflow-gpu only. Note we have not tested our setup with tf 2.x\n- keras \"learning rate\" error: clean uninstall keras and install keras 2.3.1 {pip upgrade is broken for this dependency so will not work}\n\nTo train the most basic project-level context model use the following command :\n```\ntime python3 train.py --model-type=attendgru-pc --batch-size=50 --epochs=10 --datfile=dataset_random.pkl --gpu=0\n```\nNote: --datfile=dataset_3Drandom.pkl for code2seq and graph2seq models or any custom models that use ast graphs you might wanna test. This is true for --datfile arg for all scripts in this project.\n\nScripts for Actionwords from S.Haque et al {https://github.com/actionwords/actionwords} for RQ2 table can be found in the firstwords folder and largely follow the same pattern as these scripts (predicts are provided as well)\n",
                "type": "Text_excerpt",
                "original_header": "Step 1 - Training",
                "parent_header": [
                    "Guide for replication of results for paper \"Project-Level Encoding for Neural Source Code Summarization of Subroutines\", accepted at ICPC 2021, Madrid, Spain (Virtual due to pandemic)."
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aakashba/projcon/main/README.md"
        },
        {
            "result": {
                "value": "Training print screen will display the epoch at which the model converges, that is when the validation accuracy is not increase much or just before it starts to decrease and validation loss goes up. Once epoch is identified run the following script and replace file in this example with the trained model epoch and timestamp.\n\n```\npython3 predict.py /nfs/projects/projcon/outdir/models/attendgru-pc_E09_random_1608163249.h5 --datfile=dataset_random.pkl --gpu=0\n```\npredicted comments for all models are provided in the predictions folder.",
                "type": "Text_excerpt",
                "original_header": "Step 2 - Predictions",
                "parent_header": [
                    "Guide for replication of results for paper \"Project-Level Encoding for Neural Source Code Summarization of Subroutines\", accepted at ICPC 2021, Madrid, Spain (Virtual due to pandemic)."
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aakashba/projcon/main/README.md"
        },
        {
            "result": {
                "value": "A script to run ensembles using mean predictions from two models can be run with this simple modification after isolating two best performing models files.\n```\npython3 predict_ensemble.py /nfs/projects/projcon/outdir/models/attendgru_E10_random_1609946700.h5 /nfs/projects/projcon/outdir/models/attendgru-pc_E09_random_1608163249.h5 --datfile=dataset_random.pkl --gpu=0\n```\npredicted comments for all ensembles are provided in the predictions folder.",
                "type": "Text_excerpt",
                "original_header": "Step 3 - Ensemble Predictions",
                "parent_header": [
                    "Guide for replication of results for paper \"Project-Level Encoding for Neural Source Code Summarization of Subroutines\", accepted at ICPC 2021, Madrid, Spain (Virtual due to pandemic)."
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aakashba/projcon/main/README.md"
        },
        {
            "result": {
                "value": "Bleu and Rouge scores as well a comparison script to insolate maximum improvement have been provided by the name of bleu.py, rougemetric.py and bleucompare.py all of them can be run with the similar commands\n```\n python3 rougemetric.py /nfs/projects/projcon/outdir/predictions/predict-attendgru_E10_random_1609946700-attendgru-pc_E09_random_1608163249.txt \n```\n```\n python3 bleu.py /nfs/projects/projcon/outdir/predictions/predict-attendgru_E10_random_1609946700-attendgru-pc_E09_random_1608163249.txt \n```\n",
                "type": "Text_excerpt",
                "original_header": "Step 4 - Metrics",
                "parent_header": [
                    "Guide for replication of results for paper \"Project-Level Encoding for Neural Source Code Summarization of Subroutines\", accepted at ICPC 2021, Madrid, Spain (Virtual due to pandemic)."
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/aakashba/projcon/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "Guide for replication of results for paper \"Project-Level Encoding for Neural Source Code Summarization of Subroutines\", accepted at ICPC 2021, Madrid, Spain (Virtual due to pandemic)."
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/aakashba/projcon/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2103.11599\n## Step 0 - Dataset building\nWe began with the main java dataset of 2.1m methods aswell the complete 50 million method extended set requested from Le Clair et al{http://leclair.tech/data/funcom/}\n\nThe dataset was filtered for duplicates then project context was constructed from the extended set using the scripts in the \"builder\" folder.\n\nWe provide the compiled dataset as well as the scripts used to compile .This data can be found at :\nhttps://drive.google.com/drive/folders/10r-I5C76e-1vZuY5gqYQQNKgAY5MOJpg?usp=sharing\n\n\n## Step 1 - Training\nTo ensure no recursive errors or edits, create directories nfs>projects and clone this git repository.\nDownload and unpack all data from the aws link into this directory as well.\nCreate directory outdir, with 4 subdirectories  **outdir/{models, histories, viz, predictions}**\n**Use Requirements.txt to get your python 3.x virtual environment in sync with our setup.** Venv is preferred. Common issues that might arise from updating an existing venv and solutions :\n- GPU not recognized: checking the compatibility of your gpu cudnn/cuda or other drivers with the keras and tf versions fixes this.\n- Tf unable to allocate tensor: uninstall tensorflow and then update tensorflow-gpu only. Note we have not tested our setup with tf 2.x\n- keras \"learning rate\" error: clean uninstall keras and install keras 2.3.1 {pip upgrade is broken for this dependency so will not work}\n\nTo train the most basic project-level context model use the following command :\n```\ntime python3 train.py --model-type=attendgru-pc --batch-size=50 --epochs=10 --datfile=dataset_random.pkl --gpu=0\n```\nNote: --datfile=dataset_3Drandom.pkl for code2seq and graph2seq models or any custom models that use ast graphs you might wanna test. This is true for --datfile arg for all scripts in this project.\n\nScripts for Actionwords from S.Haque et al {https://github.com/actionwords/actionwords} for RQ2 table can be found in the firstwords folder and largely follow the same pattern as these scripts (predicts are provided as well"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/aakashba/projcon/main/README.md"
        }
    ]
}