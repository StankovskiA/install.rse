{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 19:36:28"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/Jinxhy/SmartAppAttack",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "Jinxhy",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2021-12-05T05:19:39Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2024-09-10T06:20:37Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "description": [
        {
            "result": {
                "value": "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "We evaluate the attack effectiveness and generality in terms of four different settings including pre-trained models, datasets, transfer learning approaches and adversarial attack algorithms. The results demonstrate that the proposed attacks remain effective regardless of different settings, and significantly outperform state-of-the-art baselines. \nWe further conduct an empirical study on real-world deep learning mobile apps collected from Google Play. __Among 53 apps adopting transfer learning, we find that 71.7\\% of them can be successfully attacked, which includes popular ones in medicine, automation, and finance categories with critical usage scenarios.__ The results call for the awareness and actions of deep learning mobile app developers to secure the on-device models. \n",
                "original_header": "Abstract"
            },
            "confidence": 0.9525893846481804,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        }
    ],
    "name": [
        {
            "result": {
                "value": "SmartAppAttack",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "Jinxhy/SmartAppAttack",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/Jinxhy/SmartAppAttack/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/Jinxhy/SmartAppAttack/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 14,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 0,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/jinxhy/smartappattack/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Python",
                "name": "Python",
                "type": "Programming_language",
                "size": 36690
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "installation": [
        {
            "result": {
                "value": "In experiments, we use three different TensorFlow official pre-trained models including MobileNetV2, InceptionV3 and ResNet50V2 to build our victim fine-tuned models (i.e., on-device models). All the pre-trained models are trained on the ImageNet dataset of 1.3 million images, these models can effectively serve as generic models of the visual world and are capable of transfer learning.\n- MobileNetV2\n- InceptionV3\n- ResNet50V2\n",
                "type": "Text_excerpt",
                "original_header": "Pre-trained models",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Evaluation",
                    "Experimental setup"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "value": "Since most on-device models are commonly used in task domains related to the images, we follow the previous works to select three frequently-used image classification datasets to build the victim fine-tuned models for experiments. The classification tasks associated with these datasets represent typical scenarios developers may face during transfer learning.\n- CIFAR-10\n- GTSRB\n- Oxford Flowers\n",
                "type": "Text_excerpt",
                "original_header": "Datasets",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Evaluation",
                    "Experimental setup"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "value": "To evaluate the effectiveness of our attack on two transfer learning approaches (_Feature Extraction_ and _Fine-Tuning_), we unfreeze a different number of the top layers (except for the classifier) of a pre-trained model (e.g., MobileNetV2) and jointly train both the newly-added classifier as well as the last unfreezing layers of the base model to build our victim fine-tuned models. These resulting models are able to cover most tuning strategies.\n- _Feature Extraction_\n- _Fine-Tuning_\n",
                "type": "Text_excerpt",
                "original_header": "Transfer learning approaches",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Evaluation",
                    "Experimental setup"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "value": "For the evaluation of our attack effectiveness against different adversarial attacks, we focus on untargeted attacks in the white-box setting as our attack fools fine-tuned models to misclassify targeted images by constructing adversarial examples on known binary adversarial models. Considering a wide range of white-box untargeted attack algorithms have been proposed, it is unfeasible to cover all of them. We thus select three representative attacks including Fast Gradient Sign Method (FGSM), Carlini and Wagner (C&W), and Clipping-Aware Noise (CAN) attacks for experiments as they are either the basis of many powerful attacks or effective in computer vision tasks.\n- FGSM\n- C&W\n- CAN\n",
                "type": "Text_excerpt",
                "original_header": "Adversarial attack algorithms",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Evaluation",
                    "Experimental setup"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "value": "- Default Binary Adversarial Model Attack (BAMA), which crafts adversarial images based on a binary model trained on the targeted class (i.e., the class the attacker intends to force the victim model to misclassify) and non-targeted class (i.e., an arbitrary class recognized by the victim model except for the targeted one).\n- Enhanced Binary Adversarial Model Attack (E-BAMA), it is similar to the first setting but substitutes the non-targeted class with the most error-prone class (i.e., the class most likely to be misclassified as the targeted one) during binary model training.\n- Pre-trained Model Attack (PMA), which directly generates adversarial images solely based on the victim model's pre-trained model without taking any other model information into account, i.e., it ignores the structure and parameter information of a victim model.\n\n",
                "type": "Text_excerpt",
                "original_header": "Baselines",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Evaluation",
                    "Experimental setup"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        }
    ],
    "acknowledgement": [
        {
            "result": {
                "value": "<p align=\"center\">\n  <img  src=\"figures/skin.png\" width=\"45%\" height=\"45%\"><br/>\n  <em>Fig. 9: The behavior of skin cancer recognition app on normal and adversarial inputs.</em>\n</p>\n",
                "type": "Text_excerpt",
                "original_header": "Skin cancer recognition app:",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Attacking real-world deep learning mobile apps (TensorFlow Lite)"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "value": "<p align=\"center\">\n  <img  src=\"figures/traffic.png\" width=\"45%\" height=\"45%\"><br/>\n  <em>Fig. 10: The behavior of traffic sign recognition app on normal and adversarial inputs.</em>\n</p>\n",
                "type": "Text_excerpt",
                "original_header": "Traffic sign recognition app:",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Attacking real-world deep learning mobile apps (TensorFlow Lite)"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "value": "<p align=\"center\">\n  <img  src=\"figures/cash.png\" width=\"45%\" height=\"45%\"><br/>\n  <em>Fig. 11: The behavior of cash recognition app on normal and adversarial inputs.</em>\n</p>\n",
                "type": "Text_excerpt",
                "original_header": "Cash recognition app:",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Attacking real-world deep learning mobile apps (TensorFlow Lite)"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "value": "<p align=\"center\">\n  <img  src=\"figures/obstacle.png\" width=\"45%\" height=\"45%\"><br/>\n  <em>Fig. 12: The behavior of obstacle recognition app on normal and adversarial inputs.</em>\n</p>\n",
                "type": "Text_excerpt",
                "original_header": "Obstacle recognition app:",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Attacking real-world deep learning mobile apps (PyTorch Mobile)"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "value": "<p align=\"center\">\n  <img  src=\"figures/crop.png\" width=\"45%\" height=\"45%\"><br/>\n  <em>Fig. 13: The behavior of crop disease recognition app on normal and adversarial inputs.</em>\n</p>\n",
                "type": "Text_excerpt",
                "original_header": "Crop disease recognition app:",
                "parent_header": [
                    "Smart App Attack: Hacking Deep Learning Models in Android Apps",
                    "Attacking real-world deep learning mobile apps (PyTorch Mobile)"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": "Smart App Attack: Hacking Deep Learning Models in Android Apps"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        }
    ],
    "images": [
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/attack_workflow.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/binary_training.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/pre-trained.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/dataset.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/transfer.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/algorithm.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/four_setting.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/four_performance.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/skin.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/traffic.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/cash.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/obstacle.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        },
        {
            "result": {
                "type": "Url",
                "value": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/figures/crop.png"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/abs/2204.11075"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/jinxhy/smartappattack/main/README.md"
        }
    ]
}