{
    "somef_provenance": {
        "somef_version": "0.9.5",
        "somef_schema_version": "1.0.0",
        "date": "2024-10-04 19:31:09"
    },
    "code_repository": [
        {
            "result": {
                "value": "https://github.com/samarthtambad/big-data-pl",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "owner": [
        {
            "result": {
                "value": "samarthtambad",
                "type": "User"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_created": [
        {
            "result": {
                "value": "2020-04-03T19:14:49Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "date_updated": [
        {
            "result": {
                "value": "2021-03-03T01:27:58Z",
                "type": "Date"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "description": [
        {
            "result": {
                "value": "Analysing programming languages by community characteristics on Github and StackOverflow",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Starting from root directory of user svt258 \\\n```Raw```: project/data/raw/data \\\nBASH2*: project/data/cleaned \\\nBASH3*: project/data/stats \\\nBASH4*: project/data/analysis\n \n",
                "original_header": "Data Locations on HDFS"
            },
            "confidence": 0.976895170110588,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Size: 120 GB \\\nSource: https://archive.org/details/stackexchange\n##### Data Locations on HDFS\nStarting from root directory of user rhn235 \\\n```Raw```: project/data/raw/data \\\nBASH2*: project/data/cleaned \\\nBASH3*: project/data/stats \\\nBASH4*: project/data/analysis\n \n",
                "original_header": "2. StackOverflow"
            },
            "confidence": 0.9778110817097528,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Analysis of the community friendliness of a programming language from Github and StackOverflow data \n"
            },
            "confidence": 0.9286262425309476,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        }
    ],
    "name": [
        {
            "result": {
                "value": "big-data-pl",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "full_name": [
        {
            "result": {
                "value": "samarthtambad/big-data-pl",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "issue_tracker": [
        {
            "result": {
                "value": "https://api.github.com/repos/samarthtambad/big-data-pl/issues",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_url": [
        {
            "result": {
                "value": "https://api.github.com/repos/samarthtambad/big-data-pl/forks",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "stargazers_count": [
        {
            "result": {
                "value": 2,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "keywords": [
        {
            "result": {
                "value": "analytics, big-data, hadoop-filesystem, hdfs, sbt, scala, spark, tableau-desktop",
                "type": "String"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "forks_count": [
        {
            "result": {
                "value": 1,
                "type": "Number"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "download_url": [
        {
            "result": {
                "value": "https://github.com/samarthtambad/big-data-pl/releases",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "programming_languages": [
        {
            "result": {
                "value": "Scala",
                "name": "Scala",
                "type": "Programming_language",
                "size": 57155
            },
            "confidence": 1,
            "technique": "GitHub_API"
        }
    ],
    "readme_url": [
        {
            "result": {
                "value": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "documentation": [
        {
            "result": {
                "value": "https://github.com/samarthtambad/big-data-pl/tree/master/docs",
                "type": "Url"
            },
            "confidence": 1,
            "technique": "file_exploration"
        }
    ],
    "type": [
        {
            "result": {
                "value": "non-software",
                "type": "String"
            },
            "confidence": 1,
            "technique": "software_type_heuristics"
        }
    ],
    "run": [
        {
            "result": {
                "value": "Starting at the root of the project folder, perform the following steps to run the application.\n",
                "type": "Text_excerpt",
                "original_header": "Steps to run"
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nmodule load spark/2.4.0\nmodule load git/1.8.3.1\nmodule load sbt/1.2.8\nmodule load scala/2.11.8\n```\n",
                "type": "Text_excerpt",
                "original_header": "Load modules",
                "parent_header": [
                    "Steps to run"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "``` \nsbt compile\n```\nOnce finished, the compiled classes will be saved to ```target/scala-2.11/classes``` directory.\n",
                "type": "Text_excerpt",
                "original_header": "Compile project",
                "parent_header": [
                    "Steps to run"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nsbt package\n```\nOnce finished, the compiled jar file named ```big-data-pl_2.11-1.0.jar``` will be saved to ```target/scala-2.11/``` directory.\n",
                "type": "Text_excerpt",
                "original_header": "Package into a jar file",
                "parent_header": [
                    "Steps to run"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-shell --name \"<your job name>\"\" --master yarn --deploy-mode client --verbose --driver-memory 5G --executor-memory 2G --num-executors 20\n```\n",
                "type": "Text_excerpt",
                "original_header": "Running spark-shell in with configurable memory and executors",
                "parent_header": [
                    "Steps to run"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"TEST\" --class test.Test --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 10 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nA random job just to test is spark-submit is working.\n",
                "type": "Text_excerpt",
                "original_header": "1. Running test job",
                "parent_header": [
                    "Examples"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_ETL\" --class etl.TransformGithubRaw --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Raw``` data path in HDFS (as listed above) and stores cleaned data into the ```Cleaned``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "ETL",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_PROFILE\" --class profile.ProfileGithub --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Cleaned``` data path in HDFS (as listed above) and stores tables generated with profiling info into the ```Profiling``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "Profiling",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_ANALYZE\" --class analysis.AnalyzeGithub --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Cleaned``` data path in HDFS (as listed above) and stores the computed metrics data into the ```Analytics``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "Analysis",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        }
    ],
    "faq": [
        {
            "result": {
                "value": "```\nspark2-submit --name \"<your job name>\" --class <your main class> --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 10 target/scala-2.11/<your JAR>.jar\n```\n",
                "type": "Text_excerpt",
                "original_header": "Submitting jobs to cluster",
                "parent_header": [
                    "Steps to run"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"TEST\" --class test.Test --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 10 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nA random job just to test is spark-submit is working.\n",
                "type": "Text_excerpt",
                "original_header": "1. Running test job",
                "parent_header": [
                    "Examples"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_ETL\" --class etl.TransformGithubRaw --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Raw``` data path in HDFS (as listed above) and stores cleaned data into the ```Cleaned``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "ETL",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_PROFILE\" --class profile.ProfileGithub --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Cleaned``` data path in HDFS (as listed above) and stores tables generated with profiling info into the ```Profiling``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "Profiling",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_ANALYZE\" --class analysis.AnalyzeGithub --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Cleaned``` data path in HDFS (as listed above) and stores the computed metrics data into the ```Analytics``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "Analysis",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        }
    ],
    "usage": [
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_ETL\" --class etl.TransformGithubRaw --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Raw``` data path in HDFS (as listed above) and stores cleaned data into the ```Cleaned``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "ETL",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_PROFILE\" --class profile.ProfileGithub --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Cleaned``` data path in HDFS (as listed above) and stores tables generated with profiling info into the ```Profiling``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "Profiling",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "value": "```\nspark2-submit --name \"GH_ANALYZE\" --class analysis.AnalyzeGithub --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Cleaned``` data path in HDFS (as listed above) and stores the computed metrics data into the ```Analytics``` data path.\n",
                "type": "Text_excerpt",
                "original_header": "Analysis",
                "parent_header": [
                    "Examples",
                    "3. Run jobs for StackOverflow data"
                ]
            },
            "confidence": 1,
            "technique": "header_analysis",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        }
    ],
    "installation": [
        {
            "result": {
                "type": "Text_excerpt",
                "value": "Size: 102 GB \\\nSource: https://ghtorrent.org/ \n",
                "original_header": "1. Github"
            },
            "confidence": 0.9745959204199135,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        },
        {
            "result": {
                "type": "Text_excerpt",
                "value": "```\n.\n\u251c\u2500\u2500 build.sbt\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 github_final_metrics.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 stackoverflow_final_metrics.csv\n\u251c\u2500\u2500 project\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 build.properties\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 target\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 config-classes\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 scala-2.12\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 streams\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 screenshots\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 main\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 scala\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 app_code\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 analysis_github.scala\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 analysis_stackoverflow.scala\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 data_ingest\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 ingest.txt\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 etl_code\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 etl_github.scala\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 etl_stackoverflow.scala\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 profiling_code\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 profile_github.scala\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile_stackoverflow.scala\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 test_code\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 test.scala\n\u2514\u2500\u2500 target\n    \u251c\u2500\u2500 scala-2.11\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 big-data-pl_2.11-1.0.jar\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 classes\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 update\n    \u2514\u2500\u2500 streams\n```\nThe BASH2* folder contains the final computed metrics each for GitHub and StackOverflow.\nThe compiled BASH3* file can be found at location BASH4*.\n \n",
                "original_header": "File Structure"
            },
            "confidence": 0.9874598937690077,
            "technique": "supervised_classification",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        }
    ],
    "full_title": [
        {
            "result": {
                "type": "String",
                "value": " Big Data Application Project"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        }
    ],
    "related_papers": [
        {
            "result": {
                "type": "Url",
                "value": "https://arxiv.org/pdf/2006.01351.pdf\">\n    <img align=\"right\" src=\"https://img.shields.io/badge/arXiv-2006.01351-blue\" class=\"build-status\" alt=\"build status\">\n</a>\n<span style=\"float:right; padding-right:10px;\">&nbsp;</span>\n<a href=\"https://public.tableau.com/profile/samarth.tambad#!/vizhome/ProgrammingLanguagesAnalysis/Dashboard1\">\n    <img align=\"right\" src=\"https://img.shields.io/badge/visualization-Tableu-blue\" class=\"build-status\" alt=\"build status\">\n</a>\n</h1>\n\nAnalysis of the community friendliness of a programming language from Github and StackOverflow data\n\n## Data Sources\nThe following data was downloaded into HDFS as part of ingestion process:\n\n#### 1. Github\nSize: 102 GB \\\nSource: https://ghtorrent.org/\n##### Data Locations on HDFS\nStarting from root directory of user svt258 \\\n```Raw```: project/data/raw/data \\\n```Cleaned```: project/data/cleaned \\\n```Profiling```: project/data/stats \\\n```Analytics```: project/data/analysis\n\n#### 2. StackOverflow\nSize: 120 GB \\\nSource: https://archive.org/details/stackexchange\n##### Data Locations on HDFS\nStarting from root directory of user rhn235 \\\n```Raw```: project/data/raw/data \\\n```Cleaned```: project/data/cleaned \\\n```Profiling```: project/data/stats \\\n```Analytics```: project/data/analysis\n\n## File Structure\n```\n.\n\u251c\u2500\u2500 build.sbt\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 github_final_metrics.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 stackoverflow_final_metrics.csv\n\u251c\u2500\u2500 project\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 build.properties\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 target\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 config-classes\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 scala-2.12\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 streams\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 screenshots\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 main\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 scala\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 app_code\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 analysis_github.scala\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 analysis_stackoverflow.scala\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 data_ingest\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 ingest.txt\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 etl_code\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 etl_github.scala\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 etl_stackoverflow.scala\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 profiling_code\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 profile_github.scala\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile_stackoverflow.scala\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 test_code\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 test.scala\n\u2514\u2500\u2500 target\n    \u251c\u2500\u2500 scala-2.11\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 big-data-pl_2.11-1.0.jar\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 classes\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 update\n    \u2514\u2500\u2500 streams\n```\nThe ```data``` folder contains the final computed metrics each for GitHub and StackOverflow.\nThe compiled ```jar``` file can be found at location ```target/scala-2.11/```.\n\n## Steps to run\nStarting at the root of the project folder, perform the following steps to run the application.\n\n### Load modules\n```\nmodule load spark/2.4.0\nmodule load git/1.8.3.1\nmodule load sbt/1.2.8\nmodule load scala/2.11.8\n```\n\n### Compile project\n``` \nsbt compile\n```\nOnce finished, the compiled classes will be saved to ```target/scala-2.11/classes``` directory.\n\n### Package into a jar file\n```\nsbt package\n```\nOnce finished, the compiled jar file named ```big-data-pl_2.11-1.0.jar``` will be saved to ```target/scala-2.11/``` directory.\n\n### Submitting jobs to cluster\n```\nspark2-submit --name \"<your job name>\" --class <your main class> --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 10 target/scala-2.11/<your JAR>.jar\n```\n\n### Running spark-shell in with configurable memory and executors\n```\nspark2-shell --name \"<your job name>\"\" --master yarn --deploy-mode client --verbose --driver-memory 5G --executor-memory 2G --num-executors 20\n```\n\n# Examples\n\n## 1. Running test job\n```\nspark2-submit --name \"TEST\" --class test.Test --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 10 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nA random job just to test is spark-submit is working.\n\n## 2. Run jobs for Github data\n#### ETL\n```\nspark2-submit --name \"GH_ETL\" --class etl.TransformGithubRaw --master yarn --deploy-mode cluster --verbose --driver-memory 5G --executor-memory 2G --num-executors 20 target/scala-2.11/big-data-pl_2.11-1.0.jar\n```\nReads data from the ```Raw``` data path in HDFS (as listed above"
            },
            "confidence": 1,
            "technique": "regular_expression",
            "source": "https://raw.githubusercontent.com/samarthtambad/big-data-pl/master/README.md"
        }
    ]
}